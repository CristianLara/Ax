"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[6801],{5804:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=r(4848),t=r(8453);const i={id:"trial-evaluation",title:"Trial Evaluation"},o=void 0,s={id:"trial-evaluation",title:"Trial Evaluation",description:"There are 3 paradigms for evaluating trials in Ax. Note:",source:"@site/../docs/trial-evaluation.md",sourceDirName:".",slug:"/trial-evaluation",permalink:"/Ax/docs/next/trial-evaluation",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"Cristian Lara",lastUpdatedAt:1731805385e3,frontMatter:{id:"trial-evaluation",title:"Trial Evaluation"},sidebar:"docs",previous:{title:"Core",permalink:"/Ax/docs/next/core"},next:{title:"Data",permalink:"/Ax/docs/next/data"}},l={},c=[{value:"[RECOMMENDED] Service API",id:"recommended-service-api",level:2},{value:"Evaluating Trial Parameters",id:"evaluating-trial-parameters",level:3},{value:"Loop API",id:"loop-api",level:2},{value:"Developer API",id:"developer-api",level:2},{value:"Custom Metrics",id:"custom-metrics",level:3},{value:"Adding Your Own Runner",id:"adding-your-own-runner",level:3}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["There are 3 paradigms for evaluating ",(0,a.jsx)(n.a,{href:"/Ax/docs/next/glossary#trial",children:"trials"})," in Ax. Note:\nensure that you are using the\n",(0,a.jsx)(n.a,{href:"/Ax/docs/next/core#trial-vs-batched-trial",children:"appropriate type of trials"})," for your\nexperiment, before proceeding to trial evaluation."]}),"\n",(0,a.jsx)(n.h2,{id:"recommended-service-api",children:"[RECOMMENDED] Service API"}),"\n",(0,a.jsxs)(n.p,{children:["The Service API ",(0,a.jsx)(n.a,{href:"/api/service.html#module-ax.service.ax_client",children:(0,a.jsx)(n.code,{children:"AxClient"})}),"\nexposes\n",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.ax_client.AxClient.get_next_trial",children:(0,a.jsx)(n.code,{children:"get_next_trial"})}),",\nas well as\n",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.ax_client.AxClient.complete_trial",children:(0,a.jsx)(n.code,{children:"complete_trial"})}),".\nThe user is responsible for evaluating the trial parameters and passing the\nresults to\n",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.ax_client.AxClient.complete_trial",children:(0,a.jsx)(n.code,{children:"complete_trial"})}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"...\nfor i in range(25):\n    parameters, trial_index = ax_client.get_next_trial()\n    raw_data = evaluate_trial(parameters)\n    ax_client.complete_trial(trial_index=trial_index, raw_data=raw_data)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"evaluating-trial-parameters",children:"Evaluating Trial Parameters"}),"\n",(0,a.jsxs)(n.p,{children:["In the Service API, the\n",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.ax_client.AxClient.complete_trial",children:(0,a.jsx)(n.code,{children:"complete_trial"})}),"\nmethod requires ",(0,a.jsx)(n.code,{children:"raw_data"})," evaluated from the parameters suggested by\n",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.ax_client.AxClient.get_next_trial",children:(0,a.jsx)(n.code,{children:"get_next_trial"})}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The data can be in the form of:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["A dictionary of metric names to tuples of (mean and ",(0,a.jsx)(n.a,{href:"/Ax/docs/next/glossary#sem",children:"SEM"}),")"]}),"\n",(0,a.jsx)(n.li,{children:"A single (mean, SEM) tuple"}),"\n",(0,a.jsx)(n.li,{children:"A single mean"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:'In the second case, Ax will assume that the mean and the SEM are for the\nexperiment objective (if the evaluations are noiseless, simply provide an SEM of\n0.0). In the third case, Ax will assume that observations are corrupted by\nGaussian noise with zero mean and unknown SEM, and infer the SEM from the data\n(this is equivalent to specifying an SEM of None). Note that if the observation\nnoise is non-zero (either provided or inferred), the "best arm" suggested by Ax\nmay not always be the one whose evaluation returned the best observed value (as\nthe "best arm" is selected based on the model-predicted mean).'}),"\n",(0,a.jsxs)(n.p,{children:["For example, this evaluation function computes mean and SEM for\n",(0,a.jsx)(n.a,{href:"https://www.sfu.ca/~ssurjano/hart6.html",children:"Hartmann6"})," function and for the\nL2-norm. We return ",(0,a.jsx)(n.code,{children:"0.0"})," for SEM since the observations are noiseless:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from ax.utils.measurement.synthetic_functions import hartmann6\ndef hartmann_evaluation_function(parameterization):\n    x = np.array([parameterization.get(f"x{i+1}") for i in range(6)])\n    # Standard error is 0 since we are computing a synthetic function.\n    return {"hartmann6": (hartmann6(x), 0.0), "l2norm": (np.sqrt((x ** 2).sum()), 0.0)}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["This function computes just the objective mean and SEM, assuming the\n",(0,a.jsx)(n.a,{href:"https://www.sfu.ca/~ssurjano/branin.html",children:"Branin"})," function is the objective of\nthe experiment:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from ax.utils.measurement.synthetic_functions import branin\ndef branin_evaluation_function(parameterization):\n    # Standard error is 0 since we are computing a synthetic function.\n    return (branin(parameterization.get("x1"), parameterization.get("x2")), 0.0)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Alternatively, if the SEM is unknown, we could use the following form:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'lambda parameterization: branin(parameterization.get("x1"), parameterization.get("x2"))\n'})}),"\n",(0,a.jsxs)(n.p,{children:["This is equivalent to returning ",(0,a.jsx)(n.code,{children:"None"})," for the SEM:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from ax.utils.measurement.synthetic_functions import branin\ndef branin_evaluation_function_unknown_sem(parameterization):\n    return (branin(parameterization.get("x1"), parameterization.get("x2")), None)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"loop-api",children:"Loop API"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"/api/service.html#ax.service.managed_loop.optimize",children:(0,a.jsx)(n.code,{children:"optimize"})})," function\nrequires an ",(0,a.jsx)(n.code,{children:"evaluation_function"}),", which accepts parameters and returns raw data\nin the format described above. It can also accept a ",(0,a.jsx)(n.code,{children:"weight"})," parameter, a\nnullable ",(0,a.jsx)(n.code,{children:"float"})," representing the fraction of available data on which the\nparameterization should be evaluated. For example, this could be a downsampling\nrate in case of hyperparameter optimization (what portion of data the ML model\nshould be trained on for evaluation) or the percentage of users exposed to a\ngiven configuration in A/B testing. This weight is not used in unweighted\nexperiments and defaults to ",(0,a.jsx)(n.code,{children:"None"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"developer-api",children:"Developer API"}),"\n",(0,a.jsxs)(n.p,{children:["The Developer API is supported by the\n",(0,a.jsx)(n.a,{href:"/api/core.html#module-ax.core.experiment",children:(0,a.jsx)(n.code,{children:"Experiment"})})," class. In this\nparadigm, the user specifies:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner",children:(0,a.jsx)(n.code,{children:"Runner"})}),": Defines how to deploy the\nexperiment."]}),"\n",(0,a.jsxs)(n.li,{children:["List of ",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.metric.Metric",children:(0,a.jsx)(n.code,{children:"Metrics"})}),": Each defines how\nto compute/fetch data for a given objective or outcome."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The experiment requires a ",(0,a.jsx)(n.code,{children:"generator_run"})," to create a new trial or batch trial.\nA generator run can be generated by a model. The trial then has its own ",(0,a.jsx)(n.code,{children:"run"}),"\nand ",(0,a.jsx)(n.code,{children:"mark_complete"})," methods."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"...\nsobol = Models.SOBOL(exp.search_space)\nfor i in range(5):\n    trial = exp.new_trial(generator_run=sobol.gen(1))\n    trial.run()\n    trial.mark_completed()\n\nfor i in range(15):\n    gpei = Models.BOTORCH_MODULAR(experiment=exp, data=exp.fetch_data())\n    generator_run = gpei.gen(1)\n    trial = exp.new_trial(generator_run=generator_run)\n    trial.run()\n    trial.mark_completed()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"custom-metrics",children:"Custom Metrics"}),"\n",(0,a.jsxs)(n.p,{children:["Similar to a trial evaluation in the Service API, a custom metric computes a\nmean and SEM for each arm of a trial. However, the metric's ",(0,a.jsx)(n.code,{children:"fetch_trial_data"}),"\nmethod will be called automatically by the experiment's\n",(0,a.jsx)(n.a,{href:"/api/core.html#ax.core.base_trial.BaseTrial.fetch_data",children:(0,a.jsx)(n.code,{children:"fetch_data"})})," method.\nIf there are multiple objectives or outcomes that need to be optimized for, each\nneeds its own metric."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class MyMetric(Metric):\n    def fetch_trial_data(self, trial):\n        records = []\n        for arm_name, arm in trial.arms_by_name.items():\n            params = arm.parameters\n            records.append({\n                "arm_name": arm_name,\n                "metric_name": self.name,\n                "mean": self.foo(params["x1"], params["x2"]),\n                "sem": 0.0,\n                "trial_index": trial.index,\n            })\n        return Data(df=pd.DataFrame.from_records(records))\n'})}),"\n",(0,a.jsx)(n.h3,{id:"adding-your-own-runner",children:"Adding Your Own Runner"}),"\n",(0,a.jsxs)(n.p,{children:["In order to control how the experiment is deployed, you can add your own runner.\nTo do so, subclass ",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner",children:(0,a.jsx)(n.code,{children:"Runner"})})," and\nimplement the ",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner.run",children:(0,a.jsx)(n.code,{children:"run"})})," method and\n",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner.staging_required",children:(0,a.jsx)(n.code,{children:"staging_required"})}),"\nproperty."]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner.run",children:(0,a.jsx)(n.code,{children:"run"})})," method accepts a\n",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.trial.Trial",children:(0,a.jsx)(n.code,{children:"Trial"})})," and returns a JSON-serializable\ndictionary of any necessary tracking info to fetch data later from this external\nsystem. A unique identifier or name for this trial in the external system should\nbe stored in this dictionary with the key ",(0,a.jsx)(n.code,{children:'"name"'}),", and this can later be\naccessed via ",(0,a.jsx)(n.code,{children:"trial.deployed_name"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["The\n",(0,a.jsx)(n.a,{href:"../api/core.html#ax.core.runner.Runner.staging_required",children:(0,a.jsx)(n.code,{children:"staging_required"})}),"\nindicates whether the trial requires an intermediate staging period before\nevaluation begins. This property returns False by default."]}),"\n",(0,a.jsx)(n.p,{children:"An example implementation is given below:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from foo_system import deploy_to_foo\nfrom ax import Runner\n\nclass FooRunner(Runner):\n    def __init__(self, foo_param):\n        self.foo_param = foo_param\n\n    def run(self, trial):\n        name_to_params = {\n            arm.name: arm.parameters for arm in trial.arms\n        }\n        run_metadata = deploy_to_foo(self.foo_param, name_to_params)\n        return run_metadata\n\n    @property\n    def staging_required(self):\n        return False\n"})}),"\n",(0,a.jsx)(n.p,{children:"This is then invoked by calling:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'exp = Experiment(...)\nexp.runner = FooRunner(foo_param="foo")\ntrial = exp.new_batch_trial()\n\n# This calls runner\'s run method and stores metadata output\n# in the trial.run_metadata field\ntrial.run()\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var a=r(6540);const t={},i=a.createContext(t);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);