"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[427],{6007:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var i=t(4848),a=t(8453),o=t(8987);t(1023),t(290);const r={title:"Multi-Task Modeling",sidebar_label:"Multi-Task Modeling"},s="Multi-task Bayesian Optimization",l={id:"tutorials/multi_task/index",title:"Multi-Task Modeling",description:"<LinkButtons",source:"@site/../docs/tutorials/multi_task/index.mdx",sourceDirName:"tutorials/multi_task",slug:"/tutorials/multi_task/",permalink:"/Ax/docs/tutorials/multi_task/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Multi-Task Modeling",sidebar_label:"Multi-Task Modeling"},sidebar:"tutorials",previous:{title:"Hyperparameter Optimization on SLURM via SubmitIt",permalink:"/Ax/docs/tutorials/submitit/"},next:{title:"Multi-Objective Optimization",permalink:"/Ax/docs/tutorials/multiobjective_optimization/"}},c={},p=[{value:"1. Define Metric classes",id:"1-define-metric-classes",level:2},{value:"2. Create experiment",id:"2-create-experiment",level:2},{value:"3. Vizualize the simulator bias",id:"3-vizualize-the-simulator-bias",level:2},{value:"4. The Bayesian optimization loop",id:"4-the-bayesian-optimization-loop",level:2},{value:"4a. Optimization with online observations only",id:"4a-optimization-with-online-observations-only",level:4},{value:"4b. Multi-task Bayesian optimization",id:"4b-multi-task-bayesian-optimization",level:4},{value:"4c. Run both loops",id:"4c-run-both-loops",level:4},{value:"References",id:"references",level:4}];function m(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.A,{githubUrl:"",colabUrl:""}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multi-task-bayesian-optimization",children:"Multi-task Bayesian Optimization"})}),"\n",(0,i.jsx)(n.p,{children:"This tutorial uses synthetic functions to illustrate Bayesian optimization using a\nmulti-task Gaussian Process in Ax. A typical use case is optimizing an\nexpensive-to-evaluate (online) system with supporting (offline) simulations of that\nsystem."}),"\n",(0,i.jsx)(n.p,{children:"Bayesian optimization with a multi-task kernel (Multi-task Bayesian optimization) is\ndescribed by Swersky et al. (2013). Letham and Bakshy (2019) describe using multi-task\nBayesian optimization to tune a ranking system with a mix of online and offline\n(simulator) experiments."}),"\n",(0,i.jsxs)(n.p,{children:["This tutorial produces the results of Online Appendix 2 from\n",(0,i.jsx)(n.a,{href:"https://arxiv.org/pdf/1904.01049.pdf",children:"that paper"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The synthetic problem used here is to maximize the Hartmann 6 function, a classic\noptimization test problem in 6 dimensions. The objective is treated as unknown and are\nmodeled with separate GPs. The objective is noisy."}),"\n",(0,i.jsx)(n.p,{children:"Throughout the optimization we can make nosiy observations directly of the objective (an\nonline observation), and we can make noisy observations of a biased version of the\nobjective (offline observations). Bias is simulated by passing the function values\nthrough a piecewise linear function. Offline observations are much less time-consuming\nthan online observations, so we wish to use them to improve our ability to optimize the\nonline objective."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import os\nimport time\n\nfrom copy import deepcopy\nfrom typing import Optional\n\nimport numpy as np\n\nimport torch\n\nfrom ax.core.data import Data\nfrom ax.core.experiment import Experiment\nfrom ax.core.generator_run import GeneratorRun\nfrom ax.core.multi_type_experiment import MultiTypeExperiment\nfrom ax.core.objective import Objective\nfrom ax.core.observation import ObservationFeatures, observations_from_data\nfrom ax.core.optimization_config import OptimizationConfig\nfrom ax.core.parameter import ParameterType, RangeParameter\nfrom ax.core.search_space import SearchSpace\nfrom ax.metrics.hartmann6 import Hartmann6Metric\nfrom ax.modelbridge.factory import get_sobol\nfrom ax.modelbridge.registry import Models, MT_MTGP_trans, ST_MTGP_trans\nfrom ax.modelbridge.torch import TorchModelBridge\nfrom ax.modelbridge.transforms.convert_metric_names import tconfig_from_mt_experiment\nfrom ax.plot.diagnostic import interact_batch_comparison\nfrom ax.runners.synthetic import SyntheticRunner\nfrom ax.utils.common.typeutils import checked_cast\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\ninit_notebook_plotting()\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'SMOKE_TEST = os.environ.get("SMOKE_TEST")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"1-define-metric-classes",children:"1. Define Metric classes"}),"\n",(0,i.jsx)(n.p,{children:"For this example, the online system is optimizing a Hartmann6 function. The Metric\nobjects for these are directly imported above. We create analagous offline versions of\nthis metrics which are identical but have a transform applied (a piecewise linear\nfunction). We construct Metric objects for each of them."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Create metric with artificial offline bias, for the objective\n# by passing the true values through a piecewise linear function.\n\n\nclass OfflineHartmann6Metric(Hartmann6Metric):\n    def f(self, x: np.ndarray) -> float:\n        raw_res = super().f(x)\n        m = -0.35\n        if raw_res < m:\n            return (1.5 * (raw_res - m)) + m\n        else:\n            return (6.0 * (raw_res - m)) + m\n"})}),"\n",(0,i.jsx)(n.h2,{id:"2-create-experiment",children:"2. Create experiment"}),"\n",(0,i.jsx)(n.p,{children:"A MultiTypeExperiment is used for managing online and offline trials together. It is\nconstructed in several steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Create the search space"})," - This is done in the usual way."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:"Specify optimization config"})," - Also done in the usual way."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:"Initialize Experiment"}),' - In addition to the search_space and\noptimization_config, specify that "online" is the default trial_type. This is the\nmain trial type for which we\'re optimizing. Optimization metrics are defined to be\nfor this type and new trials assume this trial type by default.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:"Establish offline trial_type"}),' - Register the "offline" trial type and specify\nhow to deploy trials of this type.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:"Add offline metrics"}),' - Create the offline metrics and add them to the\nexperiment. When adding the metrics, we need to specify the trial type ("offline")\nand online metric name it is associated with so the model can link them.']}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Finally, because this is a synthetic benchmark problem where the true function values\nare known, we will also register metrics with the true (noiseless) function values for\nplotting below."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def get_experiment(include_true_metric=True):\n    noise_sd = 0.1  # Observations will have this much Normal noise added to them\n\n    # 1. Create simple search space for [0,1]^d, d=6\n    param_names = [f"x{i}" for i in range(6)]\n    parameters = [\n        RangeParameter(\n            name=param_names[i],\n            parameter_type=ParameterType.FLOAT,\n            lower=0.0,\n            upper=1.0,\n        )\n        for i in range(6)\n    ]\n    search_space = SearchSpace(parameters=parameters)\n\n    # 2. Specify optimization config\n    online_objective = Hartmann6Metric(\n        "objective", param_names=param_names, noise_sd=noise_sd\n    )\n    opt_config = OptimizationConfig(\n        objective=Objective(online_objective, minimize=True)\n    )\n\n    # 3. Init experiment\n    exp = MultiTypeExperiment(\n        name="mt_exp",\n        search_space=search_space,\n        default_trial_type="online",\n        default_runner=SyntheticRunner(),\n        optimization_config=opt_config,\n    )\n\n    # 4. Establish offline trial_type, and how those trials are deployed\n    exp.add_trial_type("offline", SyntheticRunner())\n\n    # 5. Add offline metrics that provide biased estimates of the online metrics\n    offline_objective = OfflineHartmann6Metric(\n        "offline_objective", param_names=param_names, noise_sd=noise_sd\n    )\n    # Associate each offline metric with corresponding online metric\n    exp.add_tracking_metric(\n        metric=offline_objective, trial_type="offline", canonical_name="objective"\n    )\n\n    return exp\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3-vizualize-the-simulator-bias",children:"3. Vizualize the simulator bias"}),"\n",(0,i.jsx)(n.p,{children:"These figures compare the online measurements to the offline measurements on a random\nset of points, for the objective metric. You can see the offline measurements are biased\nbut highly correlated. This produces Fig. S3 from the paper."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Generate 50 points from a Sobol sequence\nexp = get_experiment(include_true_metric=False)\ns = get_sobol(exp.search_space, scramble=False)\ngr = s.gen(50)\n# Deploy them both online and offline\nexp.new_batch_trial(trial_type="online", generator_run=gr).run()\nexp.new_batch_trial(trial_type="offline", generator_run=gr).run()\n# Fetch data\ndata = exp.fetch_data()\nobservations = observations_from_data(exp, data)\n# Plot the arms in batch 0 (online) vs. batch 1 (offline)\nrender(interact_batch_comparison(observations, exp, 1, 0))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"4-the-bayesian-optimization-loop",children:"4. The Bayesian optimization loop"}),"\n",(0,i.jsx)(n.p,{children:"Here we construct a Bayesian optimization loop that interleaves online and offline\nbatches. The loop defined here is described in Algorithm 1 of the paper. We compare\nmulti-task Bayesian optimization to regular Bayesian optimization using only online\nobservations."}),"\n",(0,i.jsx)(n.p,{children:"Here we measure performance over 3 repetitions of the loop. Each one takes 1-2 hours so\nthe whole benchmark run will take several hours to complete."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Settings for the optimization benchmark.\n\n# Number of repeated experiments, each with independent observation noise.\n# This should be changed to 50 to reproduce the results from the paper.\nif SMOKE_TEST:\n    n_batches = 1\n    n_init_online = 2\n    n_init_offline = 2\n    n_opt_online = 2\n    n_opt_offline = 2\nelse:\n    n_batches = 3  # Number of optimized BO batches\n    n_init_online = 5  # Size of the quasirandom initialization run online\n    n_init_offline = 20  # Size of the quasirandom initialization run offline\n    n_opt_online = 5  # Batch size for BO selected points to be run online\n    n_opt_offline = 20  # Batch size for BO selected to be run offline\n"})}),"\n",(0,i.jsx)(n.h4,{id:"4a-optimization-with-online-observations-only",children:"4a. Optimization with online observations only"}),"\n",(0,i.jsxs)(n.p,{children:["For the online-only case, we run ",(0,i.jsx)(n.code,{children:"n_init_online"})," sobol points followed by ",(0,i.jsx)(n.code,{children:"n_batches"}),"\nbatches of ",(0,i.jsx)(n.code,{children:"n_opt_online"})," points selected by the GP. This is a normal Bayesian\noptimization loop."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# This function runs a Bayesian optimization loop, making online observations only.\ndef run_online_only_bo():\n    t1 = time.time()\n    ### Do BO with online only\n    ## Quasi-random initialization\n    exp_online = get_experiment()\n    m = get_sobol(exp_online.search_space, scramble=False)\n    gr = m.gen(n=n_init_online)\n    exp_online.new_batch_trial(trial_type="online", generator_run=gr).run()\n    ## Do BO\n    for b in range(n_batches):\n        print("Online-only batch", b, time.time() - t1)\n        # Fit the GP\n        m = Models.BOTORCH_MODULAR(\n            experiment=exp_online,\n            data=exp_online.fetch_data(),\n            search_space=exp_online.search_space,\n        )\n        # Generate the new batch\n        gr = m.gen(\n            n=n_opt_online,\n            search_space=exp_online.search_space,\n            optimization_config=exp_online.optimization_config,\n        )\n        exp_online.new_batch_trial(trial_type="online", generator_run=gr).run()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"4b-multi-task-bayesian-optimization",children:"4b. Multi-task Bayesian optimization"}),"\n",(0,i.jsx)(n.p,{children:"Here we incorporate offline observations to accelerate the optimization, while using the\nsame total number of online observations as in the loop above. The strategy here is that\noutlined in Algorithm 1 of the paper."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Initialization"})," - Run ",(0,i.jsx)(n.code,{children:"n_init_online"})," Sobol points online, and\n",(0,i.jsx)(n.code,{children:"n_init_offline"})," Sobol points offline."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Fit model "})," - Fit an MTGP to both online and offline observations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Generate candidates "})," - Generate ",(0,i.jsx)(n.code,{children:"n_opt_offline"})," candidates using NEI."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Launch offline batch "})," - Run the ",(0,i.jsx)(n.code,{children:"n_opt_offline"})," candidates offline and\nobserve their offline metrics."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Update model "})," - Update the MTGP with the new offline observations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Select points for online batch "})," - Select the best (maximum utility)\n",(0,i.jsx)(n.code,{children:"n_opt_online"})," of the NEI candidates, after incorporating their offline observations,\nand run them online."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)("b",{children:" Update model and repeat "})," - Update the model with the online observations, and\nrepeat from step 3 for the next batch."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def get_MTGP(\n    experiment: Experiment,\n    data: Data,\n    search_space: Optional[SearchSpace] = None,\n    trial_index: Optional[int] = None,\n    device: torch.device = torch.device("cpu"),\n    dtype: torch.dtype = torch.double,\n) -> TorchModelBridge:\n    """Instantiates a Multi-task Gaussian Process (MTGP) model that generates\n    points with EI.\n\n    If the input experiment is a MultiTypeExperiment then a\n    Multi-type Multi-task GP model will be instantiated.\n    Otherwise, the model will be a Single-type Multi-task GP.\n    """\n\n    if isinstance(experiment, MultiTypeExperiment):\n        trial_index_to_type = {\n            t.index: t.trial_type for t in experiment.trials.values()\n        }\n        transforms = MT_MTGP_trans\n        transform_configs = {\n            "TrialAsTask": {"trial_level_map": {"trial_type": trial_index_to_type}},\n            "ConvertMetricNames": tconfig_from_mt_experiment(experiment),\n        }\n    else:\n        # Set transforms for a Single-type MTGP model.\n        transforms = ST_MTGP_trans\n        transform_configs = None\n\n    # Choose the status quo features for the experiment from the selected trial.\n    # If trial_index is None, we will look for a status quo from the last\n    # experiment trial to use as a status quo for the experiment.\n    if trial_index is None:\n        trial_index = len(experiment.trials) - 1\n    elif trial_index >= len(experiment.trials):\n        raise ValueError("trial_index is bigger than the number of experiment trials")\n\n    status_quo = experiment.trials[trial_index].status_quo\n    if status_quo is None:\n        status_quo_features = None\n    else:\n        status_quo_features = ObservationFeatures(\n            parameters=status_quo.parameters,\n            trial_index=trial_index,  # pyre-ignore[6]\n        )\n\n    \n    return checked_cast(\n        TorchModelBridge,\n        Models.ST_MTGP(\n            experiment=experiment,\n            search_space=search_space or experiment.search_space,\n            data=data,\n            transforms=transforms,\n            transform_configs=transform_configs,\n            torch_dtype=dtype,\n            torch_device=device,\n            status_quo_features=status_quo_features,\n        ),\n    )\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Online batches are constructed by selecting the maximum utility points from the offline\n# batch, after updating the model with the offline results. This function selects the max utility points according\n# to the MTGP predictions.\ndef max_utility_from_GP(n, m, experiment, search_space, gr):\n    obsf = []\n    for arm in gr.arms:\n        params = deepcopy(arm.parameters)\n        params["trial_type"] = "online"\n        obsf.append(ObservationFeatures(parameters=params))\n    # Make predictions\n    f, cov = m.predict(obsf)\n    # Compute expected utility\n    u = -np.array(f["objective"])\n    best_arm_indx = np.flip(np.argsort(u))[:n]\n    gr_new = GeneratorRun(\n        arms=[gr.arms[i] for i in best_arm_indx],\n        weights=[1.0] * n,\n    )\n    return gr_new\n\n\n# This function runs a multi-task Bayesian optimization loop, as outlined in Algorithm 1 and above.\ndef run_mtbo():\n    t1 = time.time()\n    online_trials = []\n    ## 1. Quasi-random initialization, online and offline\n    exp_multitask = get_experiment()\n    # Online points\n    m = get_sobol(exp_multitask.search_space, scramble=False)\n    gr = m.gen(\n        n=n_init_online,\n    )\n    tr = exp_multitask.new_batch_trial(trial_type="online", generator_run=gr)\n    tr.run()\n    online_trials.append(tr.index)\n    # Offline points\n    m = get_sobol(exp_multitask.search_space, scramble=False)\n    gr = m.gen(\n        n=n_init_offline,\n    )\n    exp_multitask.new_batch_trial(trial_type="offline", generator_run=gr).run()\n    ## Do BO\n    for b in range(n_batches):\n        print("Multi-task batch", b, time.time() - t1)\n        # (2 / 7). Fit the MTGP\n        m = get_MTGP(\n            experiment=exp_multitask,\n            data=exp_multitask.fetch_data(),\n            search_space=exp_multitask.search_space,\n        )\n\n        # 3. Finding the best points for the online task\n        gr = m.gen(\n            n=n_opt_offline,\n            optimization_config=exp_multitask.optimization_config,\n            fixed_features=ObservationFeatures(\n                parameters={}, trial_index=online_trials[-1]\n            ),\n        )\n\n        # 4. But launch them offline\n        exp_multitask.new_batch_trial(trial_type="offline", generator_run=gr).run()\n\n        # 5. Update the model\n        m = get_MTGP(\n            experiment=exp_multitask,\n            data=exp_multitask.fetch_data(),\n            search_space=exp_multitask.search_space,\n        )\n\n        # 6. Select max-utility points from the offline batch to generate an online batch\n        gr = max_utility_from_GP(\n            n=n_opt_online,\n            m=m,\n            experiment=exp_multitask,\n            search_space=exp_multitask.search_space,\n            gr=gr,\n        )\n        tr = exp_multitask.new_batch_trial(trial_type="online", generator_run=gr)\n        tr.run()\n        online_trials.append(tr.index)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"4c-run-both-loops",children:"4c. Run both loops"}),"\n",(0,i.jsx)(n.p,{children:"Run both Bayesian optimization loops and aggregate results."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'runners = {\n    "GP, online only": run_online_only_bo,\n    "MTGP": run_mtbo,\n}\nfor k, r in runners.items():\n    r()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.p,{children:["Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via\nonline-offline experimentation. ",(0,i.jsx)(n.em,{children:"arXiv preprint arXiv:1603.09326"}),", 2019."]}),"\n",(0,i.jsxs)(n.p,{children:["Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In\n",(0,i.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"})," 26, NIPS, pages 2004\u20132012, 2013."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},1023:(e,n,t)=>{t.d(n,{A:()=>d});t(6540);var i,a=new Uint8Array(16);function o(){if(!i&&!(i="undefined"!=typeof crypto&&crypto.getRandomValues&&crypto.getRandomValues.bind(crypto)||"undefined"!=typeof msCrypto&&"function"==typeof msCrypto.getRandomValues&&msCrypto.getRandomValues.bind(msCrypto)))throw new Error("crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported");return i(a)}const r=/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;const s=function(e){return"string"==typeof e&&r.test(e)};for(var l=[],c=0;c<256;++c)l.push((c+256).toString(16).substr(1));const p=function(e){var n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,t=(l[e[n+0]]+l[e[n+1]]+l[e[n+2]]+l[e[n+3]]+"-"+l[e[n+4]]+l[e[n+5]]+"-"+l[e[n+6]]+l[e[n+7]]+"-"+l[e[n+8]]+l[e[n+9]]+"-"+l[e[n+10]]+l[e[n+11]]+l[e[n+12]]+l[e[n+13]]+l[e[n+14]]+l[e[n+15]]).toLowerCase();if(!s(t))throw TypeError("Stringified UUID is invalid");return t};const m=function(e,n,t){var i=(e=e||{}).random||(e.rng||o)();if(i[6]=15&i[6]|64,i[8]=63&i[8]|128,n){t=t||0;for(var a=0;a<16;++a)n[t+a]=i[a];return n}return p(i)};var h=t(4848);const d=function(e){return(0,h.jsxs)("div",{style:{backgroundColor:"lightgray",marginBottom:"var(--ifm-leading)",borderRadius:"var(--ifm-global-radius)",boxShadow:"var(--ifm-global-shadow-lw)",overflow:"hidden",padding:"10px",font:"var(--ifm-code-font-size) / var(--ifm-pre-line-height) var(--ifm-font-family-monospace)"},children:[(0,h.jsx)("span",{style:{color:"red"},children:"Out: "}),(0,h.jsx)("pre",{style:{margin:"0px",backgroundColor:"inherit"},children:e.children.split("\n").map((function(e){return(0,h.jsx)("p",{style:{marginBottom:"0px"},children:e},m())}))})]})}},8987:(e,n,t)=>{t.d(n,{A:()=>o});t(6540);var i=t(8774),a=t(4848);const o=function(e){var n=e.githubUrl,t=e.colabUrl;return(0,a.jsxs)("div",{className:"link-buttons",children:[(0,a.jsx)(i.A,{to:n,children:"Open in GitHub"}),(0,a.jsx)("div",{}),(0,a.jsx)(i.A,{to:t,children:"Run in Google Colab"})]})}},290:(e,n,t)=>{t(6540);var i=t(3259),a=t.n(i),o=(t(2303),t(4848));a()({loader:function(){return t.e(236).then(t.bind(t,1236))},loading:function(e){return e.timedOut?(0,o.jsx)("blockquote",{children:"Error: Loading Plotly timed out."}):(0,o.jsx)("div",{children:"loading..."})},timeout:1e4})},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);