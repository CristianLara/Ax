"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[535],{6116:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var n=i(4848),o=i(8453),a=i(8987);i(1023),i(290);const r={title:"Fully Bayesian, High-Dimensional, Multi-Objective Optimization",sidebar_label:"Fully Bayesian, High-Dimensional, Multi-Objective Optimization"},s="Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO",l={id:"tutorials/saasbo_nehvi/index",title:"Fully Bayesian, High-Dimensional, Multi-Objective Optimization",description:"<LinkButtons",source:"@site/../docs/tutorials/saasbo_nehvi/index.mdx",sourceDirName:"tutorials/saasbo_nehvi",slug:"/tutorials/saasbo_nehvi/",permalink:"/Ax/docs/tutorials/saasbo_nehvi/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Fully Bayesian, High-Dimensional, Multi-Objective Optimization",sidebar_label:"Fully Bayesian, High-Dimensional, Multi-Objective Optimization"},sidebar:"tutorials",previous:{title:"High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces (SAASBO)",permalink:"/Ax/docs/tutorials/saasbo/"},next:{title:"Sparsity Exploration Bayesian Optimization (SEBO)",permalink:"/Ax/docs/tutorials/sebo/"}},c={},p=[{value:"This Tutorial",id:"this-tutorial",level:3},{value:"Problem Statement",id:"problem-statement",level:3},{value:"Fully Bayesian Inference",id:"fully-bayesian-inference",level:3},{value:"SAAS Priors (SAASBO)",id:"saas-priors-saasbo",level:3},{value:"qNEHVI",id:"qnehvi",level:3},{value:"Further Information",id:"further-information",level:3},{value:"Setup",id:"setup",level:2},{value:"Load our sample 2-objective problem",id:"load-our-sample-2-objective-problem",level:3},{value:"Define experiment configurations",id:"define-experiment-configurations",level:2},{value:"Search Space",id:"search-space",level:3},{value:"MultiObjectiveOptimizationConfig",id:"multiobjectiveoptimizationconfig",level:3},{value:"Define experiment creation utilities",id:"define-experiment-creation-utilities",level:2},{value:"qNEHVI + SAASBO",id:"qnehvi--saasbo",level:2},{value:"Plot empirical data",id:"plot-empirical-data",level:2},{value:"Plot observed hypervolume, with color representing the iteration that a point was generated on.",id:"plot-observed-hypervolume-with-color-representing-the-iteration-that-a-point-was-generated-on",level:4},{value:"Plot the results",id:"plot-the-results",level:4},{value:"Inspect Model fits",id:"inspect-model-fits",level:2}];function d(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.A,{githubUrl:"",colabUrl:""}),"\n",(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"fully-bayesian-multi-objective-optimization-using-qnehvi--saasbo",children:"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO"})}),"\n",(0,n.jsx)(t.h3,{id:"this-tutorial",children:"This Tutorial"}),"\n",(0,n.jsx)(t.p,{children:"This tutorial will show how to use qNEHVI with fully bayesian inference for\nmulti-objective optimization."}),"\n",(0,n.jsxs)(t.p,{children:["Multi-objective optimization (MOO) covers the case where we care about multiple outcomes\nin our experiment but we do not know before hand a specific weighting of those\nobjectives (covered by ",(0,n.jsx)(t.code,{children:"ScalarizedObjective"}),") or a specific constraint on one objective\n(covered by ",(0,n.jsx)(t.code,{children:"OutcomeConstraint"}),"s) that will produce the best result."]}),"\n",(0,n.jsx)(t.p,{children:"The solution in this case is to find a whole Pareto frontier, a surface in outcome-space\ncontaining points that can't be improved on in every outcome. This shows us the\ntradeoffs between objectives that we can choose to make."}),"\n",(0,n.jsx)(t.h3,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,n.jsx)(t.p,{children:"Optimize a list of M objective functions\n$ \\bigl(f^{(1)}( x),..., f^{(M)}( x) \\bigr)$ over a bounded search space\n$\\mathcal X \\subset \\mathbb R^d$."}),"\n",(0,n.jsx)(t.p,{children:"We assume $f^{(i)}$ are expensive-to-evaluate black-box functions with no known\nanalytical expression, and no observed gradients. For instance, a machine learning model\nwhere we're interested in maximizing accuracy and minimizing inference time, with\n$\\mathcal X$ the set of possible configuration spaces"}),"\n",(0,n.jsx)(t.h3,{id:"fully-bayesian-inference",children:"Fully Bayesian Inference"}),"\n",(0,n.jsxs)(t.p,{children:["Previous work, has shown that using a fully Bayesian treatment of GP model\nhyperparameters $\\boldsymbol \\theta$ can lead to improved closed loop Bayesian\noptimization performance [1]. Snoek et al [1] propose to use an integrated\nacquisition function $\\alpha_{MCMC}$ where the base acquisition function\n$\\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D)$ is integrated over the the\nposterior distribution over the hyperparameters\n$p({\\boldsymbol{\\theta}} | \\mathcal{D})$, where\n$ \\mathcal{D} = {{\\mathbf{x}}",(0,n.jsx)(t.em,{children:"i, y_i}"}),"{i=1}^n$:"]}),"\n",(0,n.jsx)(t.p,{children:"$\\alpha_{MCMC}(\\mathbf{x}, \\mathcal D) = \\int \\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D) p(\\boldsymbol \\theta | \\mathcal D) d\\boldsymbol \\theta$"}),"\n",(0,n.jsx)(t.p,{children:"Since $p({\\boldsymbol{\\theta}} | \\mathcal{D})$ typically cannot be expressed in\nclosed-form, Markov Chain Monte-Carlo (MCMC) methods are used to draw samples from\n$p({\\boldsymbol{\\theta}} | \\mathcal{D})$. In this tutorial we use the NUTS sampler\nfrom the pyro package for automatic, robust fully Bayesian inference."}),"\n",(0,n.jsx)(t.p,{children:"[1] J. Snoek, H. Larochelle, R. P. Adams, Practical Bayesian Optimization of Machine\nLearning Algorithms. Advances in Neural Information Processing Systems 26, 2012."}),"\n",(0,n.jsx)(t.h3,{id:"saas-priors-saasbo",children:"SAAS Priors (SAASBO)"}),"\n",(0,n.jsx)(t.p,{children:"Recently Eriksson et al [2] propose using sparse axis-aligned subspace priors for\nBayesian optimization over high-dimensional search spaces. Specifically, the authors\npropose using a hierarchical sparsity prior consisting of a global shrinkage parameter\nwith a Half-Cauchy prior $\\tau \\sim \\mathcal{HC}(\\beta)$, and ARD lengthscales\n$\\rho_d \\sim \\mathcal{HC}(\\tau)$ for $d=1, ..., D$. See [2] for details."}),"\n",(0,n.jsx)(t.p,{children:"[2] D. Eriksson, M. Jankowiak. High-Dimensional Bayesian Optimization with Sparse\nAxis-Aligned Subspaces. Proceedings of the Thirty-Seventh Conference on Uncertainty in\nArtificial Intelligence, 2021."}),"\n",(0,n.jsx)(t.h3,{id:"qnehvi",children:"qNEHVI"}),"\n",(0,n.jsx)(t.p,{children:"In this tutorial, we use qNEHVI [3] as our acquisition function for multi-objective\noptimization. We integrate qNEHVI over the posterior distribution of the GP\nhyperparameters as proposed in [4]."}),"\n",(0,n.jsx)(t.p,{children:"[3] S. Daulton, M. Balandat, E. Bakshy. Parallel Bayesian Optimization of Multiple\nNoisy Objectives with Expected Hypervolume Improvement. Arxiv, 2021."}),"\n",(0,n.jsx)(t.p,{children:"[4] D. Eriksson, P. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu, S. Zhao, A.\nAly, G. Venkatesh, M. Balandat. Latency-Aware Neural Architecture Search with\nMulti-Objective Bayesian Optimization. ICML AutoML Workshop, 2021."}),"\n",(0,n.jsx)(t.h3,{id:"further-information",children:"Further Information"}),"\n",(0,n.jsxs)(t.p,{children:["For a deeper explanation of multi-objective optimization, please refer to the dedicated\nmulti-objective optimization tutorial:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/multiobjective_optimization.html",children:"https://ax.dev/tutorials/multiobjective_optimization.html"}),"."]}),"\n",(0,n.jsx)(t.h2,{id:"setup",children:"Setup"}),"\n",(0,n.jsx)(t.p,{children:"In this tutorial, we use Ax Developer API. Additional resources:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["To learn more about the developer API, refer to the dedicated tutorial:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/gpei_hartmann_developer.html",children:"https://ax.dev/tutorials/gpei_hartmann_developer.html"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:["To set up a ",(0,n.jsx)(t.code,{children:"GenerationStrategy"})," with multi-objective SAASBO (and use it in Ax Service\nAPI), follow the generation strategy tutorial:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/generation_strategy.html",children:"https://ax.dev/tutorials/generation_strategy.html"})," and use ",(0,n.jsx)(t.code,{children:"Models.SAASBO"})," for the\nBayesian optimization generation step."]}),"\n",(0,n.jsxs)(t.li,{children:["To learn about multi-objective optimization in Ax Service API:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API",children:"https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API"}),"."]}),"\n"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"import os\n\nimport matplotlib\n\nimport numpy as np\n\nimport pandas as pd\nimport torch\nfrom ax.core.data import Data\nfrom ax.core.experiment import Experiment\nfrom ax.core.metric import Metric\nfrom ax.core.objective import MultiObjective, Objective\nfrom ax.core.optimization_config import (\n    MultiObjectiveOptimizationConfig,\n    ObjectiveThreshold,\n)\nfrom ax.core.parameter import ParameterType, RangeParameter\nfrom ax.core.search_space import SearchSpace\nfrom ax.metrics.noisy_function import GenericNoisyFunctionMetric\nfrom ax.modelbridge.cross_validation import compute_diagnostics, cross_validate\n\n# Analysis utilities, including a method to evaluate hypervolumes\nfrom ax.modelbridge.modelbridge_utils import observed_hypervolume\n\n# Model registry for creating multi-objective optimization models.\nfrom ax.modelbridge.registry import Models\nfrom ax.models.torch.botorch_modular.surrogate import Surrogate\nfrom ax.plot.contour import plot_contour\nfrom ax.plot.diagnostic import tile_cross_validation\nfrom ax.plot.pareto_frontier import plot_pareto_frontier\nfrom ax.plot.pareto_utils import compute_posterior_pareto_frontier\nfrom ax.runners.synthetic import SyntheticRunner\nfrom ax.service.utils.report_utils import exp_to_df\n\n# Plotting imports and initialization\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\nfrom botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP\nfrom botorch.test_functions.multi_objective import DTLZ2\nfrom botorch.utils.multi_objective.box_decompositions.dominated import (\n    DominatedPartitioning,\n)\nfrom matplotlib import pyplot as plt\nfrom matplotlib.cm import ScalarMappable\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"init_notebook_plotting()\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'SMOKE_TEST = os.environ.get("SMOKE_TEST")\n'})}),"\n",(0,n.jsx)(t.h3,{id:"load-our-sample-2-objective-problem",children:"Load our sample 2-objective problem"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'d = 10\ntkwargs = {\n    "dtype": torch.double,\n    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),\n}\nproblem = DTLZ2(num_objectives=2, dim=d, negate=True).to(**tkwargs)\n'})}),"\n",(0,n.jsx)(t.h2,{id:"define-experiment-configurations",children:"Define experiment configurations"}),"\n",(0,n.jsx)(t.h3,{id:"search-space",children:"Search Space"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'search_space = SearchSpace(\n    parameters=[\n        RangeParameter(\n            name=f"x{i}", lower=0, upper=1, parameter_type=ParameterType.FLOAT\n        )\n        for i in range(d)\n    ],\n)\n'})}),"\n",(0,n.jsx)(t.h3,{id:"multiobjectiveoptimizationconfig",children:"MultiObjectiveOptimizationConfig"}),"\n",(0,n.jsxs)(t.p,{children:["To optimize multiple objective we must create a ",(0,n.jsx)(t.code,{children:"MultiObjective"})," containing the metrics\nwe'll optimize and ",(0,n.jsx)(t.code,{children:"MultiObjectiveOptimizationConfig"})," (which contains\n",(0,n.jsx)(t.code,{children:"ObjectiveThreshold"}),"s) instead of our more typical ",(0,n.jsx)(t.code,{children:"Objective"})," and ",(0,n.jsx)(t.code,{children:"OptimizationConfig"}),".\nAdditional resources:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["To set up a custom metric for your problem, refer to the dedicated section of the\nDeveloper API tutorial:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics",children:"https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics"}),"."]}),"\n",(0,n.jsxs)(t.li,{children:["To avoid needing to setup up custom metrics by using multi-objective optimization in\nAx Service API:\n",(0,n.jsx)(t.a,{href:"https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API",children:"https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["We define ",(0,n.jsx)(t.code,{children:"GenericNoisyFunctionMetric"}),"s to wrap our synthetic Branin-Currin problem's\noutputs."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'param_names = [f"x{i}" for i in range(d)]\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'def f1(x) -> float:\n    x_sorted = [x[p_name] for p_name in param_names]\n    return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0.0, 1.0))[0])\n\n\ndef f2(x) -> float:\n    x_sorted = [x[p_name] for p_name in param_names]\n    return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0.0, 1.0))[1])\n\n\nmetric_a = GenericNoisyFunctionMetric("a", f=f1, noise_sd=0.0, lower_is_better=False)\nmetric_b = GenericNoisyFunctionMetric("b", f=f2, noise_sd=0.0, lower_is_better=False)\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"mo = MultiObjective(\n    objectives=[Objective(metric=metric_a), Objective(metric=metric_b)],\n)\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"objective_thresholds = [\n    ObjectiveThreshold(metric=metric, bound=val, relative=False)\n    for metric, val in zip(mo.metrics, problem.ref_point)\n]\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"optimization_config = MultiObjectiveOptimizationConfig(\n    objective=mo,\n    objective_thresholds=objective_thresholds,\n)\n"})}),"\n",(0,n.jsx)(t.h2,{id:"define-experiment-creation-utilities",children:"Define experiment creation utilities"}),"\n",(0,n.jsx)(t.p,{children:"These construct our experiment, then initialize with Sobol points before we fit a\nGaussian Process model to those initial points."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"N_INIT = 2 * (d + 1)\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'def build_experiment():\n    experiment = Experiment(\n        name="pareto_experiment",\n        search_space=search_space,\n        optimization_config=optimization_config,\n        runner=SyntheticRunner(),\n    )\n    return experiment\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"def initialize_experiment(experiment):\n    sobol = Models.SOBOL(search_space=experiment.search_space)\n    experiment.new_batch_trial(sobol.gen(N_INIT)).run()\n    return experiment.fetch_data()\n"})}),"\n",(0,n.jsx)(t.h2,{id:"qnehvi--saasbo",children:"qNEHVI + SAASBO"}),"\n",(0,n.jsx)(t.p,{children:"Noisy expected hypervolume improvement + fully Bayesian inference with SAAS priors."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"experiment = build_experiment()\ndata = initialize_experiment(experiment)\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"BATCH_SIZE = 4\n\nif SMOKE_TEST:\n    N_BATCH = 1\n    num_samples = 128\n    warmup_steps = 256\nelse:\n    N_BATCH = 10\n    BATCH_SIZE = 4\n    num_samples = 256\n    warmup_steps = 512\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'hv_list = []\nmodel = None\nfor i in range(N_BATCH):\n    model = Models.BOTORCH_MODULAR(\n        experiment=experiment,\n        data=data,\n        surrogate=Surrogate(\n            botorch_model_class=SaasFullyBayesianSingleTaskGP,\n            mll_options={\n                "num_samples": num_samples,  # Increasing this may result in better model fits\n                "warmup_steps": warmup_steps,  # Increasing this may result in better model fits\n            },\n        )\n        )\n    generator_run = model.gen(BATCH_SIZE)\n    trial = experiment.new_batch_trial(generator_run=generator_run)\n    trial.run()\n    data = Data.from_multiple_data([data, trial.fetch_data()])\n\n    exp_df = exp_to_df(experiment)\n    outcomes = torch.tensor(exp_df[["a", "b"]].values, **tkwargs)\n    partitioning = DominatedPartitioning(ref_point=problem.ref_point, Y=outcomes)\n    try:\n        hv = partitioning.compute_hypervolume().item()\n    except:\n        hv = 0\n        print("Failed to compute hv")\n    hv_list.append(hv)\n    print(f"Iteration: {i}, HV: {hv}")\n\ndf = exp_to_df(experiment).sort_values(by=["trial_index"])\noutcomes = df[["a", "b"]].values\n'})}),"\n",(0,n.jsx)(t.h2,{id:"plot-empirical-data",children:"Plot empirical data"}),"\n",(0,n.jsx)(t.h4,{id:"plot-observed-hypervolume-with-color-representing-the-iteration-that-a-point-was-generated-on",children:"Plot observed hypervolume, with color representing the iteration that a point was generated on."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'%matplotlib inline\nmatplotlib.rcParams.update({"font.size": 16})\n\n\nfig, axes = plt.subplots(1, 1, figsize=(8, 6))\nalgos = ["qNEHVI"]\ntrain_obj = outcomes\ncm = matplotlib.colormaps["viridis"]\n\nn_results = N_INIT + N_BATCH * BATCH_SIZE\n\nbatch_number = df.trial_index.values\nsc = axes.scatter(train_obj[:, 0], train_obj[:, 1], c=batch_number, alpha=0.8)\naxes.set_title(algos[0])\naxes.set_xlabel("Objective 1")\naxes.set_ylabel("Objective 2")\nnorm = plt.Normalize(batch_number.min(), batch_number.max())\nsm = ScalarMappable(norm=norm, cmap=cm)\nsm.set_array([])\nfig.subplots_adjust(right=0.9)\ncbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\ncbar = fig.colorbar(sm, cax=cbar_ax)\ncbar.ax.set_title("Iteration")\n'})}),"\n",(0,n.jsx)(t.h1,{id:"hypervolume-statistics",children:"Hypervolume statistics"}),"\n",(0,n.jsx)(t.p,{children:"The hypervolume of the space dominated by points that dominate the reference point."}),"\n",(0,n.jsx)(t.h4,{id:"plot-the-results",children:"Plot the results"}),"\n",(0,n.jsx)(t.p,{children:"The plot below shows a common metric of multi-objective optimization performance when\nthe true Pareto frontier is known: the log difference between the hypervolume of the\ntrue Pareto front and the hypervolume of the approximate Pareto front identified by\nqNEHVI."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'iters = np.arange(1, N_BATCH + 1)\nlog_hv_difference = np.log10(problem.max_hv - np.asarray(hv_list))[: N_BATCH + 1]\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax.plot(iters, log_hv_difference, label="qNEHVI+SAASBO", linewidth=1.5)\nax.set(xlabel="Batch Iterations", ylabel="Log Hypervolume Difference")\nax.legend(loc="lower right")\n'})}),"\n",(0,n.jsx)(t.h2,{id:"inspect-model-fits",children:"Inspect Model fits"}),"\n",(0,n.jsx)(t.p,{children:"Here, we examine the GP model fits using the fully bayesian inference with SAAS priors.\nWe plot the leave-one-out cross-validation below. Note: model hyperparameters are not\nre-sampled on each fold to reduce the runtime."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"saas_model = Models.SAASBO(experiment=experiment, data=data)\ncv = cross_validate(model)\nrender(tile_cross_validation(cv))\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'# compute  out-of-sample log likelihood\ncompute_diagnostics(cv)["Log likelihood"]\n'})}),"\n",(0,n.jsx)(t.p,{children:"Finally, we examine the GP model fits using MAP estimation for comparison. The fully\nbayesian model has a higher log-likelihood than the MAP model."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"map_model = Models.BOTORCH_MODULAR(experiment=experiment, data=data)\nmap_cv = cross_validate(map_model)\nrender(tile_cross_validation(map_cv))\n"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'# compute out-of-sample log likelihood\ncompute_diagnostics(map_cv)["Log likelihood"]\n'})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python"})})]})}function m(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},1023:(e,t,i)=>{i.d(t,{A:()=>h});i(6540);var n,o=new Uint8Array(16);function a(){if(!n&&!(n="undefined"!=typeof crypto&&crypto.getRandomValues&&crypto.getRandomValues.bind(crypto)||"undefined"!=typeof msCrypto&&"function"==typeof msCrypto.getRandomValues&&msCrypto.getRandomValues.bind(msCrypto)))throw new Error("crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported");return n(o)}const r=/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;const s=function(e){return"string"==typeof e&&r.test(e)};for(var l=[],c=0;c<256;++c)l.push((c+256).toString(16).substr(1));const p=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,i=(l[e[t+0]]+l[e[t+1]]+l[e[t+2]]+l[e[t+3]]+"-"+l[e[t+4]]+l[e[t+5]]+"-"+l[e[t+6]]+l[e[t+7]]+"-"+l[e[t+8]]+l[e[t+9]]+"-"+l[e[t+10]]+l[e[t+11]]+l[e[t+12]]+l[e[t+13]]+l[e[t+14]]+l[e[t+15]]).toLowerCase();if(!s(i))throw TypeError("Stringified UUID is invalid");return i};const d=function(e,t,i){var n=(e=e||{}).random||(e.rng||a)();if(n[6]=15&n[6]|64,n[8]=63&n[8]|128,t){i=i||0;for(var o=0;o<16;++o)t[i+o]=n[o];return t}return p(n)};var m=i(4848);const h=function(e){return(0,m.jsxs)("div",{style:{backgroundColor:"lightgray",marginBottom:"var(--ifm-leading)",borderRadius:"var(--ifm-global-radius)",boxShadow:"var(--ifm-global-shadow-lw)",overflow:"hidden",padding:"10px",font:"var(--ifm-code-font-size) / var(--ifm-pre-line-height) var(--ifm-font-family-monospace)"},children:[(0,m.jsx)("span",{style:{color:"red"},children:"Out: "}),(0,m.jsx)("pre",{style:{margin:"0px",backgroundColor:"inherit"},children:e.children.split("\n").map((function(e){return(0,m.jsx)("p",{style:{marginBottom:"0px"},children:e},d())}))})]})}},8987:(e,t,i)=>{i.d(t,{A:()=>a});i(6540);var n=i(8774),o=i(4848);const a=function(e){var t=e.githubUrl,i=e.colabUrl;return(0,o.jsxs)("div",{className:"link-buttons",children:[(0,o.jsx)(n.A,{to:t,children:"Open in GitHub"}),(0,o.jsx)("div",{}),(0,o.jsx)(n.A,{to:i,children:"Run in Google Colab"})]})}},290:(e,t,i)=>{i(6540);var n=i(3259),o=i.n(n),a=(i(2303),i(4848));o()({loader:function(){return i.e(236).then(i.bind(i,1236))},loading:function(e){return e.timedOut?(0,a.jsx)("blockquote",{children:"Error: Loading Plotly timed out."}):(0,a.jsx)("div",{children:"loading..."})},timeout:1e4})},8453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>s});var n=i(6540);const o={},a=n.createContext(o);function r(e){const t=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);