"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[125],{5679:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>h});var a=n(4848),r=n(8453),i=n(8987);n(1023),n(290);const o={title:"Bandit Optimization",sidebar_label:"Bandit Optimization"},s="Factorial design with empirical Bayes and Thompson Sampling",l={id:"tutorials/factorial/index",title:"Bandit Optimization",description:"<LinkButtons",source:"@site/../docs/tutorials/factorial/index.mdx",sourceDirName:"tutorials/factorial",slug:"/tutorials/factorial/",permalink:"/Ax/docs/tutorials/factorial/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Bandit Optimization",sidebar_label:"Bandit Optimization"},sidebar:"tutorials",previous:{title:"Global Stopping (Experiment-Level Early Stopping)",permalink:"/Ax/docs/tutorials/gss/"},next:{title:"Human-in-the-Loop Optimization",permalink:"/Ax/docs/tutorials/human_in_the_loop/"}},c={},h=[{value:"1. Define the search space",id:"1-define-the-search-space",level:2},{value:"2. Define a custom metric",id:"2-define-a-custom-metric",level:2},{value:"3. Define the experiment",id:"3-define-the-experiment",level:2},{value:"4. Run an exploratory batch",id:"4-run-an-exploratory-batch",level:2},{value:"5. Iterate using Thompson Sampling",id:"5-iterate-using-thompson-sampling",level:2},{value:"Plot 1: Predicted outcomes for each arm in initial trial",id:"plot-1-predicted-outcomes-for-each-arm-in-initial-trial",level:2},{value:"Plot 2: Predicted outcomes for arms in last trial",id:"plot-2-predicted-outcomes-for-arms-in-last-trial",level:2},{value:"Plot 3: Rollout Process",id:"plot-3-rollout-process",level:2},{value:"Plot 4: Marginal Effects",id:"plot-4-marginal-effects",level:2}];function d(e){const t={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.A,{githubUrl:"",colabUrl:""}),"\n",(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"factorial-design-with-empirical-bayes-and-thompson-sampling",children:"Factorial design with empirical Bayes and Thompson Sampling"})}),"\n",(0,a.jsx)(t.p,{children:"This tutorial illustrates how to run a factorial experiment. In such an experiment, each\nparameter (factor) can be assigned one of multiple discrete values (levels). A\nfull-factorial experiment design explores all possible combinations of factors and\nlevels."}),"\n",(0,a.jsx)(t.p,{children:"For instance, consider a banner with a title and an image. We are considering two\ndifferent titles and three different images. A full-factorial experiment will compare\nall 2*3=6 possible combinations of title and image, to see which version of the banner\nperforms the best."}),"\n",(0,a.jsx)(t.p,{children:"In this example, we first run an exploratory batch to collect data on all possible\ncombinations. Then we use empirical Bayes to model the data and shrink noisy estimates\ntoward the mean. Next, we use Thompson Sampling to suggest a set of arms (combinations\nof factors and levels) on which to collect more data. We repeat the process until we\nhave identified the best performing combination(s)."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"import numpy as np\nimport pandas as pd\nimport sklearn as skl\nfrom typing import Dict, Optional, Tuple, Union\nfrom ax import (\n    Arm,\n    ChoiceParameter,\n    Models,\n    ParameterType,\n    SearchSpace,\n    Experiment,\n    OptimizationConfig,\n    Objective,\n)\nfrom ax.plot.scatter import plot_fitted\nfrom ax.utils.notebook.plotting import render, init_notebook_plotting\nfrom ax.utils.stats.statstools import agresti_coull_sem\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"init_notebook_plotting()\n"})}),"\n",(0,a.jsx)(t.h2,{id:"1-define-the-search-space",children:"1. Define the search space"}),"\n",(0,a.jsx)(t.p,{children:"First, we define our search space. A factorial search space contains a ChoiceParameter\nfor each factor, where the values of the parameter are its levels."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'search_space = SearchSpace(\n    parameters=[\n        ChoiceParameter(\n            name="factor1",\n            parameter_type=ParameterType.STRING,\n            values=["level11", "level12", "level13"],\n        ),\n        ChoiceParameter(\n            name="factor2",\n            parameter_type=ParameterType.STRING,\n            values=["level21", "level22"],\n        ),\n        ChoiceParameter(\n            name="factor3",\n            parameter_type=ParameterType.STRING,\n            values=["level31", "level32", "level33", "level34"],\n        ),\n    ]\n)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"2-define-a-custom-metric",children:"2. Define a custom metric"}),"\n",(0,a.jsx)(t.p,{children:"Second, we define a custom metric, which is responsible for computing the mean and\nstandard error of a given arm."}),"\n",(0,a.jsx)(t.p,{children:"In this example, each possible parameter value is given a coefficient. The higher the\nlevel, the higher the coefficient, and the higher the coefficients, the greater the\nmean."}),"\n",(0,a.jsx)(t.p,{children:"The standard error of each arm is determined by the weight passed into the evaluation\nfunction, which represents the size of the population on which this arm was evaluated.\nThe higher the weight, the greater the sample size, and thus the lower the standard\nerror."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from ax import Data, Metric\nfrom ax.utils.common.result import Ok\nimport pandas as pd\nfrom random import random\n\n\none_hot_encoder = skl.preprocessing.OneHotEncoder(\n    categories=[par.values for par in search_space.parameters.values()],\n)\n\n\nclass FactorialMetric(Metric):\n    def fetch_trial_data(self, trial):\n        records = []\n        for arm_name, arm in trial.arms_by_name.items():\n            params = arm.parameters\n            batch_size = 10000\n            noise_level = 0.0\n            weight = trial.normalized_arm_weights().get(arm, 1.0)\n            coefficients = np.array([0.1, 0.2, 0.3, 0.1, 0.2, 0.1, 0.2, 0.3, 0.4])\n            features = np.array(list(params.values())).reshape(1, -1)\n            encoded_features = one_hot_encoder.fit_transform(features)\n            z = (\n                coefficients @ encoded_features.T\n                + np.sqrt(noise_level) * np.random.randn()\n            )\n            p = np.exp(z) / (1 + np.exp(z))\n            plays = np.random.binomial(batch_size, weight)\n            successes = np.random.binomial(plays, p)\n            records.append(\n                {\n                    "arm_name": arm_name,\n                    "metric_name": self.name,\n                    "trial_index": trial.index,\n                    "mean": float(successes) / plays,\n                    "sem": agresti_coull_sem(successes, plays),\n                }\n            )\n        return Ok(value=Data(df=pd.DataFrame.from_records(records)))\n'})}),"\n",(0,a.jsx)(t.h2,{id:"3-define-the-experiment",children:"3. Define the experiment"}),"\n",(0,a.jsx)(t.p,{children:"We now set up our experiment and define the status quo arm, in which each parameter is\nassigned to the lowest level."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from ax import Runner\n\n\nclass MyRunner(Runner):\n    def run(self, trial):\n        trial_metadata = {"name": str(trial.index)}\n        return trial_metadata\n\n\nexp = Experiment(\n    name="my_factorial_closed_loop_experiment",\n    search_space=search_space,\n    optimization_config=OptimizationConfig(\n        objective=Objective(metric=FactorialMetric(name="success_metric"), minimize=False)\n    ),\n    runner=MyRunner(),\n)\nexp.status_quo = Arm(\n    parameters={"factor1": "level11", "factor2": "level21", "factor3": "level31"}\n)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"4-run-an-exploratory-batch",children:"4. Run an exploratory batch"}),"\n",(0,a.jsx)(t.p,{children:"We then generate an a set of arms that covers the full space of the factorial design,\nincluding the status quo. There are three parameters, with two, three, and four values,\nrespectively, so there are 24 possible arms."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"factorial = Models.FACTORIAL(search_space=exp.search_space)\nfactorial_run = factorial.gen(\n    n=-1\n)  # Number of arms to generate is derived from the search space.\nprint(len(factorial_run.arms))\n"})}),"\n",(0,a.jsx)(t.p,{children:"Now we create a trial including all of these arms, so that we can collect data and\nevaluate the performance of each."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"trial = exp.new_batch_trial(optimize_for_power=True).add_generator_run(\n    factorial_run, multiplier=1\n)\n"})}),"\n",(0,a.jsxs)(t.p,{children:["By default, the weight of each arm in ",(0,a.jsx)(t.code,{children:"factorial_run"})," will be 1. However, to optimize\nfor power on the contrasts of ",(0,a.jsx)(t.code,{children:"k"})," groups against the status quo, the status quo should\nbe ",(0,a.jsx)(t.code,{children:"sqrt(k)"})," larger than any of the treatment groups. Since we have 24 different arms in\nour search space, the status quo should be roughly five times larger. That larger weight\nis automatically set by Ax under the hood if ",(0,a.jsx)(t.code,{children:"optimize_for_power"})," kwarg is set to True\non new batched trial creation."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"trial._status_quo_weight_override\n"})}),"\n",(0,a.jsx)(t.h2,{id:"5-iterate-using-thompson-sampling",children:"5. Iterate using Thompson Sampling"}),"\n",(0,a.jsx)(t.p,{children:"Next, we run multiple trials (iterations of the experiment) to hone in on the optimal\narm(s)."}),"\n",(0,a.jsxs)(t.p,{children:["In each iteration, we first collect data about all arms in that trial by calling\n",(0,a.jsx)(t.code,{children:"trial.run()"})," and ",(0,a.jsx)(t.code,{children:"trial.mark_complete()"}),". Then we run Thompson Sampling, which assigns\na weight to each arm that is proportional to the probability of that arm being the best.\nArms whose weight exceed ",(0,a.jsx)(t.code,{children:"min_weight"})," are added to the next trial, so that we can gather\nmore data on their performance."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'models = []\nfor i in range(4):\n    print(f"Running trial {i+1}...")\n    trial.run()\n    trial.mark_completed()\n    thompson = Models.THOMPSON(experiment=exp, data=trial.fetch_data(), min_weight=0.01)\n    models.append(thompson)\n    thompson_run = thompson.gen(n=-1)\n    trial = exp.new_batch_trial(optimize_for_power=True).add_generator_run(thompson_run)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"plot-1-predicted-outcomes-for-each-arm-in-initial-trial",children:"Plot 1: Predicted outcomes for each arm in initial trial"}),"\n",(0,a.jsx)(t.p,{children:"The plot below shows the mean and standard error for each arm in the first trial. We can\nsee that the standard error for the status quo is the smallest, since this arm was\nassigned 5x weight."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'render(plot_fitted(models[0], metric="success_metric", rel=False))\n'})}),"\n",(0,a.jsx)(t.h2,{id:"plot-2-predicted-outcomes-for-arms-in-last-trial",children:"Plot 2: Predicted outcomes for arms in last trial"}),"\n",(0,a.jsx)(t.p,{children:"The following plot below shows the mean and standard error for each arm that made it to\nthe last trial (as well as the status quo, which appears throughout)."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'render(plot_fitted(models[-1], metric="success_metric", rel=False))\n'})}),"\n",(0,a.jsx)(t.p,{children:"As expected given our evaluation function, arms with higher levels perform better and\nare given higher weight. Below we see the arms that made it to the final trial."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'results = pd.DataFrame(\n    [\n        {"values": ",".join(arm.parameters.values()), "weight": weight}\n        for arm, weight in trial.normalized_arm_weights().items()\n    ]\n)\nprint(results)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"plot-3-rollout-process",children:"Plot 3: Rollout Process"}),"\n",(0,a.jsx)(t.p,{children:"We can also visualize the progression of the experience in the following rollout chart.\nEach bar represents a trial, and the width of the bands within a bar are proportional to\nthe weight of the arms in that trial."}),"\n",(0,a.jsx)(t.p,{children:"In the first trial, all arms appear with equal weight, except for the status quo. By the\nlast trial, we have narrowed our focus to only four arms, with arm 0_22 (the arm with\nthe highest levels) having the greatest weight."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from ax.plot.bandit_rollout import plot_bandit_rollout\nfrom ax.utils.notebook.plotting import render\n\nrender(plot_bandit_rollout(exp))\n"})}),"\n",(0,a.jsx)(t.h2,{id:"plot-4-marginal-effects",children:"Plot 4: Marginal Effects"}),"\n",(0,a.jsxs)(t.p,{children:["Finally, we can examine which parameter values had the greatest effect on the overall\narm value. As we see in the diagram below, arms whose parameters were assigned the lower\nlevel values (such as ",(0,a.jsx)(t.code,{children:"levell1"}),", ",(0,a.jsx)(t.code,{children:"levell2"}),", ",(0,a.jsx)(t.code,{children:"level31"})," and ",(0,a.jsx)(t.code,{children:"level32"}),") performed worse\nthan average, whereas arms with higher levels performed better than average."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from ax.plot.marginal_effects import plot_marginal_effects\n\nrender(plot_marginal_effects(models[0], "success_metric"))\n'})})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},1023:(e,t,n)=>{n.d(t,{A:()=>m});n(6540);var a,r=new Uint8Array(16);function i(){if(!a&&!(a="undefined"!=typeof crypto&&crypto.getRandomValues&&crypto.getRandomValues.bind(crypto)||"undefined"!=typeof msCrypto&&"function"==typeof msCrypto.getRandomValues&&msCrypto.getRandomValues.bind(msCrypto)))throw new Error("crypto.getRandomValues() not supported. See https://github.com/uuidjs/uuid#getrandomvalues-not-supported");return a(r)}const o=/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i;const s=function(e){return"string"==typeof e&&o.test(e)};for(var l=[],c=0;c<256;++c)l.push((c+256).toString(16).substr(1));const h=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,n=(l[e[t+0]]+l[e[t+1]]+l[e[t+2]]+l[e[t+3]]+"-"+l[e[t+4]]+l[e[t+5]]+"-"+l[e[t+6]]+l[e[t+7]]+"-"+l[e[t+8]]+l[e[t+9]]+"-"+l[e[t+10]]+l[e[t+11]]+l[e[t+12]]+l[e[t+13]]+l[e[t+14]]+l[e[t+15]]).toLowerCase();if(!s(n))throw TypeError("Stringified UUID is invalid");return n};const d=function(e,t,n){var a=(e=e||{}).random||(e.rng||i)();if(a[6]=15&a[6]|64,a[8]=63&a[8]|128,t){n=n||0;for(var r=0;r<16;++r)t[n+r]=a[r];return t}return h(a)};var p=n(4848);const m=function(e){return(0,p.jsxs)("div",{style:{backgroundColor:"lightgray",marginBottom:"var(--ifm-leading)",borderRadius:"var(--ifm-global-radius)",boxShadow:"var(--ifm-global-shadow-lw)",overflow:"hidden",padding:"10px",font:"var(--ifm-code-font-size) / var(--ifm-pre-line-height) var(--ifm-font-family-monospace)"},children:[(0,p.jsx)("span",{style:{color:"red"},children:"Out: "}),(0,p.jsx)("pre",{style:{margin:"0px",backgroundColor:"inherit"},children:e.children.split("\n").map((function(e){return(0,p.jsx)("p",{style:{marginBottom:"0px"},children:e},d())}))})]})}},8987:(e,t,n)=>{n.d(t,{A:()=>i});n(6540);var a=n(8774),r=n(4848);const i=function(e){var t=e.githubUrl,n=e.colabUrl;return(0,r.jsxs)("div",{className:"link-buttons",children:[(0,r.jsx)(a.A,{to:t,children:"Open in GitHub"}),(0,r.jsx)("div",{}),(0,r.jsx)(a.A,{to:n,children:"Run in Google Colab"})]})}},290:(e,t,n)=>{n(6540);var a=n(3259),r=n.n(a),i=(n(2303),n(4848));r()({loader:function(){return n.e(236).then(n.bind(n,1236))},loading:function(e){return e.timedOut?(0,i.jsx)("blockquote",{children:"Error: Loading Plotly timed out."}):(0,i.jsx)("div",{children:"loading..."})},timeout:1e4})},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>s});var a=n(6540);const r={},i=a.createContext(r);function o(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);