---
title: Welcome to Ax Tutorials
sidebar_label: Overview
---

Here you can learn about the structure and applications of Ax from examples.

**Our 3 API tutorials:** [Loop](gpei_hartmann_loop.html), [Service](gpei_hartmann_service.html), and [Developer](gpei_hartmann_developer.html) — are a good place to start. Each tutorial showcases optimization on a constrained Hartmann6 problem, with the Loop API being the simplest to use and the Developer API being the most customizable.

**NOTE: We recommend the [Service API](gpei_hartmann_service.html) for the vast majority of use cases.** This API provides an ideal balance of flexibility and simplicity for most users, and we are in the process of consolidating Ax usage around it more formally.

**Further, we explore the different components available in Ax in more detail.** {' '} The components explored below serve to set up an experiment, visualize its results, configure an optimization algorithm, run an entire experiment in a managed closed loop, and combine BoTorch components in Ax in a modular way.

*   [Visualizations](visualizations.html) illustrates the different plots available to view and understand your results.

*   [GenerationStrategy](generation_strategy.html) steps through setting up a way to specify the optimization algorithm (or multiple). A `GenerationStrategy` is an important component of Service API and the `Scheduler`.

*   [Scheduler](scheduler.html) demonstrates an example of a managed and configurable closed-loop optimization, conducted in an asyncronous fashion. `Scheduler` is a manager abstraction in Ax that deploys trials, polls them, and uses their results to produce more trials.

*   [Modular `BoTorchModel`](modular_botax.html) walks though a new beta-feature — an improved interface between Ax and{' '} [BoTorch](https://botorch.org/) — which allows for combining arbitrary BoTorch components like `AcquisitionFunction`, `Model`, `AcquisitionObjective` etc. into a single{' '} `Model` in Ax.

**Our other Bayesian Optimization tutorials include:**

*   [Hyperparameter Optimization for PyTorch](tune_cnn_service.html) provides an example of hyperparameter optimization with Ax and integration with an external ML library.

*   [Hyperparameter Optimization on SLURM via SubmitIt](submitit.html) shows how to use the AxClient to schedule jobs and tune hyperparameters on a Slurm cluster.

*   [Multi-Task Modeling](multi_task.html) illustrates multi-task Bayesian Optimization on a constrained synthetic Hartmann6 problem.

*   [Multi-Objective Optimization](multiobjective_optimization.html) demonstrates Multi-Objective Bayesian Optimization on a synthetic Branin-Currin test function.

*   [Trial-Level Early Stopping](early_stopping/early_stopping.html) shows how to use trial-level early stopping on an ML training job to save resources and iterate faster.

{/* *   [Benchmarking Suite](benchmarking_suite_example.html) demonstrates how to use the Ax benchmarking suite to compare Bayesian Optimization algorithm performances and generate a comparative report with visualizations. */}

For experiments done in a real-life setting, refer to our field experiments tutorials:

*   [Bandit Optimization](factorial.html) shows how Thompson Sampling can be used to intelligently reallocate resources to well-performing configurations in real-time.

*   [Human-in-the-Loop Optimization](human_in_the_loop/human_in_the_loop.html) walks through manually influencing the course of optimization in real-time.
