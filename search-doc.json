{"searchDocs":[{"title":"Overview","type":0,"sectionRef":"#","url":"/Ax/docs/algo-overview","content":"Overview Ax supports: Bandit optimization Empirical Bayes with Thompson sampling Bayesian optimization","keywords":"","version":"Next"},{"title":"Bayesian Optimization","type":0,"sectionRef":"#","url":"/Ax/docs/bayesopt","content":"","keywords":"","version":"Next"},{"title":"How does it work?​","type":1,"pageTitle":"Bayesian Optimization","url":"/Ax/docs/bayesopt#how-does-it-work","content":" Parameter tuning is often done with simple strategies like grid search. However, grid search scales very poorly with the number of parameters (the dimensionality of the parameter space) and generally does not work well for more than a couple of continuous parameters. Alternative global optimization techniques like DIRECT or genetic algorithms are more flexible, but also typically require more evaluations than is feasible, especially in the presence of uncertainty.  Bayesian optimization starts by building a smooth surrogate model of the outcomes using Gaussian processes (GPs) based on the (possibly noisy) observations available from previous rounds of experimentation. See below for more details on how the GP model works. This surrogate model can be used to make predictions at unobserved parameterizations and quantify the uncertainty around them. The predictions and the uncertainty estimates are combined to derive an acquisition function, which quantifies the value of observing a particular parameterization. We optimize the acquisition function to find the best configuration to observe, and then after observing the outcomes at that configuration a new surrogate model is fitted and the process is repeated until convergence. The entire process is adaptive in the sense that the predictions and uncertainty estimates are updated as new observations are made.  The strategy of relying on successive surrogate models to update knowledge of the objective allows BO to strike a balance between the conflicting goals of exploration (trying out parameterizations with high uncertainty in their outcomes) and exploitation (converging on configurations that are likely to be good). As a result, BO is able to find better configurations with fewer evaluations than is generally possible with grid search or other global optimization techniques. This makes it a good choice for applications where a limited number of function evaluations can be made.    Figure 1 shows a 1D example, where a surrogate model is fitted to five noisy observations using GPs to predict the objective (solid line) and place uncertainty estimates (proportional to the width of the shaded bands) over the entire x-axis, which represents the range of possible parameter values. The model is able to predict the outcome of configurations that have not yet been tested. As intuitively expected, the uncertainty bands are tight in regions that are well-explored and become wider as we move away from them.  ","version":"Next","tagName":"h2"},{"title":"Tradeoff between parallelism and total number of trials​","type":1,"pageTitle":"Bayesian Optimization","url":"/Ax/docs/bayesopt#tradeoff-between-parallelism-and-total-number-of-trials","content":" In Bayesian Optimization (any optimization, really), we have the choice between performing evaluations of our function in a sequential fashion (i.e. only generate a new candidate point to evaluate after the previous candidate has been evaluated), or in a parallel fashion (where we evaluate multiple candidates concurrently). The sequential approach will (in expectation) produce better optimization results, since at any point during the optimization the ML model that drives it uses strictly more information than the parallel approach. However, if function evaluations take a long time and end-to-end optimization time is important, then the parallel approach becomes attractive. The difference between the performance of a sequential (aka 'fully adaptive') algorithm and that of a (partially) parallelized algorithm is referred to as the 'adaptivity gap'.  To balance end-to-end optimization time with finding the optimal solution in fewer trials, we opt for a ‘staggered’ approach by allowing a limited number of trials to be evaluated in parallel. By default, in simplified Ax APIs (e.g., in Service API) the allowed parallelism for the Bayesian phase of the optimization is 3. Service API tutorial has more information on how to handle and change allowed parallelism for that API.  For cases where its not too computationally expensive to run many trials (and therefore sample efficiency is less of a concern), higher parallelism can significantly speed up the end-to-end optimization time. By default, we recommend keeping the ratio of allowed parallelism to total trials relatively small (&lt;10%) in order to not hurt optimization performance too much, but the reasonable ratio can differ depending on the specific setup.  ","version":"Next","tagName":"h2"},{"title":"Acquisition functions​","type":1,"pageTitle":"Bayesian Optimization","url":"/Ax/docs/bayesopt#acquisition-functions","content":" BoTorch — Ax's optimization engine — supports some of the most commonly used acquisition functions in BO like expected improvement (EI), probability of improvement, and upper confidence bound. Expected improvement is a popular acquisition function owing to its good practical performance and an analytic form that is easy to compute. As the name suggests it rewards evaluation of the objective $f$ based on the expected improvement relative to the current best. If $f^* = \\max_i y_i$ is the current best observed outcome and our goal is to maximize $f$, then EI is defined as  $$ \\text{EI}(x) = \\mathbb{E}\\bigl[\\max(f(x) - f^*, 0)\\bigr] $$  The parameterization with the highest EI is selected and evaluated in the next step. Using an acquisition function like EI to sample new points initially promotes quick exploration because its values, like the uncertainty estimates, are higher in unexplored regions. Once the parameter space is adequately explored, EI naturally narrows in on locations where there is a high likelihood of a good objective value.  The above definition of the EI function assumes that the objective function is observed free of noise. In many types of experiments, such as those found in A/B testing and reinforcement learning, the observations are typically noisy. For these cases, BoTorch implements an efficient variant of EI, called Noisy EI, which allow for optimization of highly noisy outcomes, along with any number of constraints (i.e., ensuring that auxiliary outcomes do not increase or decrease too much). Figure 2 shows how an EI acquisition function can be used in a noisy setting to seamlessly transition from exploration to optimization in BO. For more on Noisy EI, see our blog post.    ","version":"Next","tagName":"h2"},{"title":"A closer look at Gaussian processes​","type":1,"pageTitle":"Bayesian Optimization","url":"/Ax/docs/bayesopt#a-closer-look-at-gaussian-processes","content":" How exactly do we model the true objective $f$ for making predictions about yet-to-be-explored regions using only a few noisy observations? GPs are a simple and powerful way of imposing assumptions over functions in the form of a probability distribution. The family of functions is characterized by,  A mean function that is the average of all functions, and,A covariance or kernel function that provides an overall template for the look and feel of the individual functions (such as their shape or smoothness) and how much they can vary around the mean function.  In most applications of BO, a radial basis function (RBF) or Matern kernel is used because they allow us the flexibility to fit a wide variety of functions in high dimensions. By default, BoTorch uses the Matern 5/2 kernel, which tends to allow for less smooth surfaces compared to the RBF. For more mathematical details and intuitions about GPs and the different kernels check out this tutorial.  In GP regression, the true objective is specified by a GP prior distribution with mean zero and a kernel function. Given a set of noisy observations from initial experimental evaluations, a Bayesian update gives the posterior distribution which is itself a GP with an updated mean and kernel function. The mean function of the posterior distribution gives the best prediction at any point conditional on the available observations, and the kernel function helps to quantify the uncertainty in the predictions in terms of posterior predictive intervals. Figure 3 shows three draws from the posterior GP as well as the predictions and posterior predictive intervals.    The kernel function has several hyperparameters that determine how smooth the GP posterior will be. For the predictions and uncertainty estimates to be practically useful, we have to make sure that the kernel is adapted to the observations. This is done by fitting the kernel hyperparameters to the data, usually by maximizing the marginal likelihood of the data, or with MCMC.  For detailed information about Ax's underlying Bayesian optimization engine, BoTorch, see the BoTorch documentation. ","version":"Next","tagName":"h2"},{"title":"Bandit Optimization","type":0,"sectionRef":"#","url":"/Ax/docs/banditopt","content":"","keywords":"","version":"Next"},{"title":"How does it work?​","type":1,"pageTitle":"Bandit Optimization","url":"/Ax/docs/banditopt#how-does-it-work","content":" Ax relies on the simple and effective Thompson sampling algorithm for performing bandit optimization. There is a clear intuition to this method: select a parameterization (referred to from now on as an &quot;arm&quot;) with a probability proportional to that arm being the best. This algorithm is easy to implement and has strong guarantees of converging to an arm that is close to the best — all without any human intervention. To understand how this works, we describe an advertising optimization problem in which we want to choose arms which maximize the click-through rate (CTR) and the rewards are binary: either clicks (successes) or views without clicks (failures).  As we run the experiment, we develop more precise estimates of the performance of each arm. More precisely, in each iteration, we draw samples from the distribution of plausible effects for each arm, and we record the largest sampled value. We repeat this process many times, until settling on a final distribution of maximal arms, which determines how we assign users to arms going forward. This process rapidly narrows down our set of arms to only the very best performers.  The following figure is an example of how assignment probabilities for an experiment with 10 arms may evolve over 20 iterations of batch-based Thompson sampling:    The process starts by distributing users equally among all of the arms. Bandit optimization then produces updated assignment probabilities (represented here by the height of the colored bars in each column) based on the average CTR observed up until that point. Since the true CTR is highest for the second arm, followed by the first arm, in this simulated example those arms are subsequently given larger allocations over 20 rounds of optimization.  Early in the process, the uncertainty in our estimates of CTR means that the bandit optimization spreads samples somewhat evenly amongst arms. This helps us obtain better estimates for all of the arms and allows us to start focusing in on those which perform well. The following figure animates this evolution. The small blue x indicates the observed CTRs within each round, while the solid round symbol (and gray error bars) indicate our aggregated estimates across all rounds. Arms 3 through 8 are sampled just often enough to get a rough estimate that their CTRs are low, and the algorithm then focuses further exploration on the first two arms to better identify which is the best. This example can be viewed as a discretized version of the animated example of Bayesian optimization.    ","version":"Next","tagName":"h2"},{"title":"How well does it work?​","type":1,"pageTitle":"Bandit Optimization","url":"/Ax/docs/banditopt#how-well-does-it-work","content":" We want a bandit algorithm to maximize the total rewards over time or equivalently, to minimize the regret, which is defined as the cumulative difference between the highest possible reward and the actual reward at a point in time. In our running example, regret is the number of clicks we &quot;left on the table&quot; through our choice of allocation procedure. We can imagine two extremes:  Pure exploration, in which we just always allocate users evenly across all conditions. This is the standard approach to A/B tests.Pure exploitation, in which we simply allocate all users to the arm we think is most likely to be best.  Both of these extremes will do a poor job of minimizing our regret, so our aim is to balance them.  The following figure compares the cumulative regret of three different approaches to bandit optimization for 200 rounds of experimentation on our running example:  Thompson sampling: the primary approach used by Ax, described aboveGreedy: select the arm with the current best rewardEpsilon-greedy: randomly picks an arm $e$ percent of the time, picks the current best arm $100-e$ percent of the time    The regret of the purely greedy approach is the highest amongst the three approaches. A little bit of exploration, as in the epsilon-greedy approach with $e = 10$, leads to much less regret over time. Thompson sampling best balances the tradeoff between exploration and exploitation, and thus outperforms the other two approaches.  As it turns out, we can do even better by applying a simple model.  ","version":"Next","tagName":"h2"},{"title":"Empirical Bayes​","type":1,"pageTitle":"Bandit Optimization","url":"/Ax/docs/banditopt#empirical-bayes","content":" In short, our empirical Bayes model consists of taking noisy estimates from a bunch of arms and &quot;shrinking&quot; the outlying ones a bit towards the overall central tendency across all arms.  The specific method we use is James-Stein estimation. This method is linear, which means that if multiple arms have estimates with similar levels of precision, they will be moved towards the middle of the effect distribution proportionally to their distance from the middle. Doing this turns out to be optimal in the case of a Gaussian distribution of effects, but will improve accuracy even if that isn't the case (so long as there are at least three means).  The diagram below illustrates how the estimates of two different experiments change as a result of applying the empirical Bayes estimator.    The experiment on the left has large effects relative to estimation variability, and so shrinkage (visualized here as distance from the dashed $y=x$ line), is very small. On the right side, however, we can see an experiment where shrinkage makes a significant difference. Effects far from the center of the distribution result in fairly substantial shrinkage, reducing the range of effects by nearly half. While effect estimates in the middle were largely unchanged, the largest observed effects went from around 17% before shrinkage to around 8% afterwards.  The vast majority of experimental groups are estimated more accurately using empirical Bayes. The arms which tend to have increases in error are those with the largest effects. Understating the effects of such arms is usually not a very big deal when making launch decisions, however, as one is usually most interested in which arm is the best rather than exactly how good it is.  Using Empirical Bayes does better at allocating users to the best arm than does using the raw effect estimates. It does this by concentrating exploration early in the experiment. In particular, it concentrates that exploration on the set of arms that look good, rather than over-exploiting the single best performing arm. By spreading exploration out a little bit more when effect estimates are noisy (and playing the best arm a little less), it is able to identify the best arm with more confidence later in the experiment.  See more details in our paper. ","version":"Next","tagName":"h2"},{"title":"Core","type":0,"sectionRef":"#","url":"/Ax/docs/core","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#overview","content":" In Ax, an experiment keeps track of the whole optimization process. It contains a search space, optimization config, metadata, information on what metrics to track and how to run iterations, etc. An experiment is composed of a sequence of trials each of which has a set of parameterizations (or arms) to be evaluated. A trial is added to the experiment when a new set of arms is proposed by the optimization algorithm. The trial is then evaluated to compute the values of each metric for each arm, which are fed into the algorithms to create a new trial. Most applications have one arm per trial, which is the default implementation.  The core constructs that define the experiment are detailed below.  ","version":"Next","tagName":"h3"},{"title":"Trial VS. Batch Trial​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#trial-vs-batch-trial","content":" An experiment consists of trials, which can be one of two types: regular trial or batch trial. A regular trial contains a single arm and relevant metadata. A batch trial contains multiple arms, relevant metadata, and optionally a set of arm weights, which are a measure of how much of the total resources allocated to evaluating a batch should go towards evaluating the specific arm.  A batch trial is not just a trial with many arms! It is a trial for which it is important that the arms are evaluated simultaneously and together. For instance, a batch trial would be appropriate in an A/B test where the evaluation results are subject to nonstationarity and require multiple arms to be deployed (and gathered data for) at the same time. For cases where multiple arms are evaluated separately and independently of each other, use multiple trials with a single arm each, which will allow Ax to keep track of their deployment and results appropriately.  ","version":"Next","tagName":"h3"},{"title":"Search Space and Parameters​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#search-space-and-parameters","content":" A search space is composed of a set of parameters to be tuned in the experiment, and optionally a set of parameter constraints that define restrictions across these parameters (e.g. p_a &lt;= p_b). Each parameter has a name, a type (int, float, bool, or string), and a domain, which is a representation of the possible values the parameter can take. The search space is used by the optimization algorithms to know which arms are valid to suggest.  Ax supports three types of parameters:  Range parameters: must be of type int or float, and the domain is represented by a lower and upper bound. If the parameter is specified as an int, newly generated points are rounded to the nearest integer by default.  from ax import RangeParameter, ParameterType float_range_param = RangeParameter(name=&quot;x1&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0) int_range_param = RangeParameter(name=&quot;x2&quot;, parameter_type=ParameterType.INT, lower=0, upper=10)   Choice parameters: domain is a set of values  from ax import ChoiceParameter, ParameterType choice_param = ChoiceParameter(name=&quot;y&quot;, parameter_type=ParameterType.STRING, values=[&quot;foo&quot;, &quot;bar&quot;])   Fixed parameters: domain is a single value  from ax import FixedParameter, ParameterType fixed_param = FixedParameter(name=&quot;z&quot;, parameter_type=ParameterType.BOOL, value=True)   Ax supports three types of parameter constraints, each of which can only be used on int or float parameters:  Linear constraints: w * v &lt;= b where w is the vector of parameter weights, v is a vector of parameter values, * is the dot product, and b is the specified bound. Linear constraints are specified with the bound and a dictionary that maps parameter name to the weight  from ax import ParameterConstraint param_a = RangeParameter(name=&quot;a&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0) param_b = RangeParameter(name=&quot;b&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0) # 1.0*a + 0.5*b &lt;= 1.0 con_1 = ParameterConstraint(constraint_dict={&quot;a&quot;: 1.0, &quot;b&quot;: 0.5}, bound=1.0)   Order constraints: specifies that one parameter must be smaller than the other  from ax import OrderConstraint # a &lt;= b con_2 = OrderConstraint(lower_parameter=param_a, upper_parameter=param_b)   Sum constraints: specifies that the sum of the parameters must be greater or less than a bound  from ax import SumConstraint # a + b &gt;= 0.5 con_3 = SumConstraint(parameters=[param_a, param_b], is_upper_bound=False, bound=0.5)   Given parameters and (optionally) parameter constraints, you can construct a search space:  from ax import SearchSpace SearchSpace(parameters=[param_a, param_b], parameter_constraints=[con_1, con_2, con_3])   ","version":"Next","tagName":"h3"},{"title":"Optimization Config​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#optimization-config","content":" An optimization config is composed of an objective metric to be minimized or maximized, and optionally a set of outcome constraints that place restrictions on how other metrics can be moved by the experiment. Note that you cannot constrain the objective metric.  from ax import Metric from ax import Objective objective = Objective(metric=Metric(name=&quot;m1&quot;), minimize=True)   There is no minimum or maximum number of outcome constraints, but an individual metric can have at most two constraints — which is how we represent metrics with both upper and lower bounds.  Outcome constraints may be of the form metric &gt;= bound or metric &lt;= bound. The bound can be expressed as an absolute measurement, or relative to the status quo (if applicable), in which case the bound is the acceptable percent change from the status quo's value.  from ax import Metric from ax import OutcomeConstraint from ax import ComparisonOp # m2 cannot regress the status quo by more than 5% oc = OutcomeConstraint(metric=Metric(name=&quot;m2&quot;), op = ComparisonOp.GEQ, bound=-5.0, relative=True)   Finally, create the optimization config to attach to the experiment.  from ax import OptimizationConfig opt_config = OptimizationConfig(objective=objective, outcome_constraints=[oc])   ","version":"Next","tagName":"h3"},{"title":"Arm​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#arm","content":" An arm in Ax is a set of parameters and their values with a name attached to it. In the case of hyperparameter optimization, an arm corresponds to a hyperparameter configuration explored in the course of a given optimization.  An arm is defined by specifying the value for each parameter, and optionally giving it a name:  from ax import Arm Arm(parameters={&quot;x&quot;: 0, &quot;y&quot;: &quot;Foo&quot;, z: True}) # Names are automatically assigned by the experiment # but can also be specified by the user Arm(parameters={&quot;x&quot;: 0, &quot;y&quot;: &quot;Foo&quot;, z: True}, name=&quot;arm1&quot;)   Arms are typically attached to trials, as discussed in the Experiment Lifecycle section below.  ","version":"Next","tagName":"h3"},{"title":"Status Quo​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#status-quo","content":" An experiment can optionally contain a status quo arm, which represents the “control” parameterization. This allows viewing results and doing optimization using relativized outcomes, meaning all metrics will be presented as percentage deltas against the status quo.  If the status quo is specified on the experiment, it will be automatically added to every trial that is created.  ","version":"Next","tagName":"h3"},{"title":"Experiment Lifecycle​","type":1,"pageTitle":"Core","url":"/Ax/docs/core#experiment-lifecycle","content":" An experiment consists of a sequence of trials, each of which evaluates one or more arms. For more details on the implementing the evaluation, see the trial evaluation and metric references.  Based on the evaluation results, the optimization algorithm suggest one or more arms to evaluate. You then create a new trial containing these suggested arms, evaluate this trial, and repeat.  You can directly add arm(s) to a new trial, or you can add a generator run –– output of the optimization algorithm:  # If only one arm should be evaluated experiment.new_trial().add_arm(Arm(...)) # If multiple arms should be evaluated experiment.new_batch_trial().add_arms_and_weights(arms=[Arm(...), Arm(...)]) # To evaluate the arms suggested by a GeneratorRun experiment.new_batch_trial().add_generator_run(generator_run=GeneratorRun(...))   A trial goes through multiple phases during the experimentation cycle, tracked by its TrialStatus field. These stages are:  CANDIDATE - Trial has just been created and can still be modified before deployment.STAGED - Relevant for external systems, where the trial configuration has been deployed but not begun the evaluation stage.RUNNING - Trial is in the process of being evaluated.COMPLETED - Trial completed evaluation successfully.FAILED - Trial incurred a failure while being evaluated.ABANDONED - User manually stopped the trial for some specified reason.  When a trial is first created, its status is &quot;candidate&quot;. If applicable, we can call trial.mark_staged to move the trial into &quot;staged&quot; mode. We then call trial.runto run the trial, which moves it into the &quot;running&quot; stage. We can then calltrial.mark_completed, trial.mark_failed, or trial.mark_abandoned to end the trial.  If the trial's runner has &quot;staging_required&quot; = True, then trial.run will first mark the trial as &quot;staged&quot;, and we can later calltrial.mark_running explicitly to move the trial to &quot;running&quot;. ","version":"Next","tagName":"h2"},{"title":"APIs","type":0,"sectionRef":"#","url":"/Ax/docs/api","content":"APIs The modular design of Ax enables three different usage modes, with different balances of structure to flexibility and reproducibility. Navigate to the&quot;Tutorials&quot; page for an in-depth walk-through of each API and usage mode. NOTE: We recommend the Service API for the vast majority of use cases. This API provides an ideal balance of flexibility and simplicity for most users, and we are in the process of consolidating Ax usage around it more formally. From most lightweight to fullest functionality, our APIs are: Loop API (tutorial) is intended for synchronous optimization loops, where trials can be evaluated right away. With this API, optimization can be executed in a single call and experiment introspection is available once optimization is complete. Use this API only for the simplest use cases where running a single trial is fast and only one trial should be running at a time.[RECOMMENDED] Service API(tutorial) can be used as a lightweight service for parameter-tuning applications where trials might be evaluated in parallel and data is available asynchronously (e.g. hyperparameter or simulation optimization). It requires little to no knowledge of Ax data structures and easily integrates with various schedulers. In this mode, Ax suggests one-arm trials to be evaluated by the client application, and expects them to be completed withmetric data when available. This is our most popular API and a good place to start as a new user. Use it to leverage nearly full hyperparameter optimization functionality of Ax without the need to learn its architecture and how things work under the hood. In both the Loop and the Service API, it is possible to configure the optimization algorithm via an Ax GenerationStrategy(tutorial), so use of Developer API is not required to control the optimization algorithm in Ax. Developer API (tutorial) is for ad-hoc use by data scientists, machine learning engineers, and researchers. The developer API allows for a great deal of customization and introspection, and is recommended for those who plan to use Ax to optimize A/B tests. Using the developer API requires some knowledge of Ax architecture. Use this API if you are looking to perform field experiments with BatchTrial-s, customize or contribute to Ax, or leverage advanced functionality that is not exposed in other APIs. While not an API, the Scheduler(tutorial) is an important and distinct use-case of the Ax Developer API. With the Scheduler, it's possible to run a configurable, managed closed-loop optimization where trials are deployed and polled in an async fashion and no human intervention/oversight is required until the experiment is complete. Use the Scheduler when you are looking to configure and start a full experiment that will need to interact with an external system to evaluate trials. Here is a comparison of the three APIs in the simple case of evaluating the unconstrained synthetic Branin function: LoopServiceDeveloperScheduler from ax import optimize from ax.utils.measurement.synthetic_functions import branin best_parameters, values, experiment, model = optimize( parameters=[ { &quot;name&quot;: &quot;x1&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [-5.0, 10.0], }, { &quot;name&quot;: &quot;x2&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 10.0], }, ], evaluation_function=lambda p: (branin(p[&quot;x1&quot;], p[&quot;x2&quot;]), 0.0), minimize=True, ) ","keywords":"","version":"Next"},{"title":"Glossary","type":0,"sectionRef":"#","url":"/Ax/docs/glossary","content":"","keywords":"","version":"Next"},{"title":"Arm​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#arm","content":" Mapping from parameters (i.e. a parameterization or parameter configuration) to parameter values. An arm provides the configuration to be tested in an Ax trial. Also known as &quot;treatment group&quot; or &quot;parameterization&quot;, the name 'arm' comes from the Multi-Armed Bandit optimization problem, in which a player facing a row of “one-armed bandit” slot machines has to choose which machines to play when and in what order. [Arm]  ","version":"Next","tagName":"h3"},{"title":"Bandit optimization​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#bandit-optimization","content":" Machine learning-driven version of A/B testing that dynamically allocates traffic to arms which are performing well, to determine the best arm among a given set.  ","version":"Next","tagName":"h3"},{"title":"Batch trial​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#batch-trial","content":" Single step in the experiment, contains multiple arms that are deployed and evaluated together. A batch trial is not just a trial with many arms; it is a trial for which it is important that the arms are evaluated simultaneously, e.g. in an A/B test where the evaluation results are subject to nonstationarity. For cases where multiple arms are evaluated separately and independently of each other, use multiple regular trials with a single arm each. [BatchTrial]  ","version":"Next","tagName":"h3"},{"title":"Bayesian optimization​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#bayesian-optimization","content":" Sequential optimization strategy for finding an optimal arm in a continuous search space.  ","version":"Next","tagName":"h3"},{"title":"Evaluation function​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#evaluation-function","content":" Function that takes a parameterization and an optional weight as input and outputs a set of metric evaluations (more details). Used in the Loop API.  ","version":"Next","tagName":"h3"},{"title":"Experiment​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#experiment","content":" Object that keeps track of the whole optimization process. Contains a search space, optimization config, and other metadata. [Experiment]  ","version":"Next","tagName":"h3"},{"title":"Generation strategy​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#generation-strategy","content":" Abstraction that allows to declaratively specify one or multiple models to use in the course of the optimization and automate transition between them (relevant tutorial). [GenerationStrategy]  ","version":"Next","tagName":"h3"},{"title":"Generator run​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#generator-run","content":" Outcome of a single run of the gen method of a model bridge, contains the generated arms, as well as possibly best arm predictions, other model predictions, fit times etc. [GeneratorRun]  ","version":"Next","tagName":"h3"},{"title":"Metric​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#metric","content":" Interface for fetching data for a specific measurement on an experiment or trial. [Metric]  ","version":"Next","tagName":"h3"},{"title":"Model​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#model","content":" Algorithm that can be used to generate new points in a search space. [Model]  ","version":"Next","tagName":"h3"},{"title":"Model bridge​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#model-bridge","content":" Adapter for interactions with a model within the Ax ecosystem. [ModelBridge]  ","version":"Next","tagName":"h3"},{"title":"Objective​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#objective","content":" The metric to be optimized, with an optimization direction (maximize/minimize). [Objective]  ","version":"Next","tagName":"h3"},{"title":"Optimization config​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#optimization-config","content":" Contains information necessary to run an optimization, i.e. objective and outcome constraints. [OptimizationConfig]  ","version":"Next","tagName":"h3"},{"title":"Outcome constraint​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#outcome-constraint","content":" Constraint on metric values, can be an order constraint or a sum constraint; violating arms will be considered infeasible. [OutcomeConstraint]  ","version":"Next","tagName":"h3"},{"title":"Parameter​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#parameter","content":" Configurable quantity that can be assigned one of multiple possible values, can be continuous (RangeParameter), discrete (ChoiceParameter) or fixed (FixedParameter). [Parameter]  ","version":"Next","tagName":"h3"},{"title":"Parameter constraint​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#parameter-constraint","content":" Places restrictions on the relationships between parameters. For example buffer_size1 &lt; buffer_size2 or buffer_size_1 + buffer_size_2 &lt; 1024. [ParameterConstraint]  ","version":"Next","tagName":"h3"},{"title":"Relative outcome constraint​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#relative-outcome-constraint","content":" Outcome constraint evaluated relative to the status quo instead of directly on the metric value. [OutcomeConstraint]  ","version":"Next","tagName":"h3"},{"title":"Runner​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#runner","content":" Dispatch abstraction that defines how a given trial is to be run (either locally or by dispatching to an external system). [Runner]  ","version":"Next","tagName":"h3"},{"title":"Scheduler​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#scheduler","content":" Configurable closed-loop optimization manager class, capable of conducting a full experiment by deploying trials, polling their results, and leveraging those results to generate and deploy more trials (relevant tutorial). [Scheduler]  ","version":"Next","tagName":"h3"},{"title":"Search space​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#search-space","content":" Continuous, discrete or mixed design space that defines the set of parameters to be tuned in the optimization, and optionally parameter constraints on these parameters. The parameters of the arms to be evaluated in the optimization are drawn from a search space. [SearchSpace]  ","version":"Next","tagName":"h3"},{"title":"SEM​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#sem","content":" Standard error of the metric's mean, 0.0 for noiseless measurements. If no value is provided, defaults to np.nan, in which case Ax infers its value using the measurements collected during experimentation.  ","version":"Next","tagName":"h3"},{"title":"Status quo​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#status-quo","content":" An arm, usually the currently deployed configuration, which provides a baseline for comparing all other arms. Also known as a control arm. [StatusQuo]  ","version":"Next","tagName":"h3"},{"title":"Trial​","type":1,"pageTitle":"Glossary","url":"/Ax/docs/glossary#trial","content":" Single step in the experiment, contains a single arm. In cases where the trial contains multiple arms that are deployed simultaneously, we refer to it as a batch trial. [Trial], [BatchTrial] ","version":"Next","tagName":"h3"},{"title":"Data","type":0,"sectionRef":"#","url":"/Ax/docs/data","content":"","keywords":"","version":"Next"},{"title":"Fetching Data​","type":1,"pageTitle":"Data","url":"/Ax/docs/data#fetching-data","content":" Metrics provide an interface for fetching data for an experiment or trial. Experiment objectives and outcome constraints are special types of metrics, and you can also attach additional metrics for tracking purposes.  Each metric is responsible for fetching its own data. Thus, all metric classes must implement the method fetch_trial_data, which accepts a Trial and returns an instance of Data, a wrapper around a Pandas DataFrame.  To fetch data for an experiment or trial, use exp.fetch_data or trial.fetch_data. These methods fetch data for all metrics on the experiment and then combine the results into a new aggregate Data instance.  Each row of the final DataFrame represents the evaluation of an arm on a metric. As such, the required columns are: arm_name, metric_name, mean, and sem. Additional optional columns are also supported: trial_index, start_time, and end_time.  arm_name\tmetric_name\tmean\tsem0_0\tmetric1\t...\t... 0_0\tmetric2\t...\t... 0_1\tmetric1\t...\t... 0_1\tmetric2\t...\t...  ","version":"Next","tagName":"h2"},{"title":"Adding Your Own Metric​","type":1,"pageTitle":"Data","url":"/Ax/docs/data#adding-your-own-metric","content":" Our base Metric class is meant to be subclassed. Subclasses must provide an implementation of fetch_trial_data.  An example of a custom metric:  import pandas as pd from ax import Metric class CustomMetric(Metric): def fetch_trial_data(self, trial, **kwargs): records = [] for arm_name, arm in trial.arms_by_name.items(): records.append({ &quot;arm_name&quot;: arm_name, &quot;metric_name&quot;: self.name, &quot;mean&quot;: 0.0, # mean value of this metric when this arm is used &quot;sem&quot;: 0.0, # standard error of the above mean &quot;trial_index&quot;: trial.index, }) return Data(df=pd.DataFrame.from_records(records))   ","version":"Next","tagName":"h2"},{"title":"Advanced Data Fetching​","type":1,"pageTitle":"Data","url":"/Ax/docs/data#advanced-data-fetching","content":" If you need to fetch data for multiple metrics or trials simultaneously, your Metric can implement the methods fetch_experiment_data, fetch_trial_data_multi, and fetch_experiment_data_multi. The default implementations of these methods use fetch_trial_data internally, but can be overridden if bulk data fetching is more appropriate for the metric type. ","version":"Next","tagName":"h2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/Ax/docs/installation","content":"","keywords":"","version":"Next"},{"title":"Requirements​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#requirements","content":" You need Python 3.10 or later to run Ax.  The required Python dependencies are:  botorchjinja2pandasscipysklearnplotly &gt;=2.2.1  ","version":"Next","tagName":"h2"},{"title":"Stable Version​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#stable-version","content":" ","version":"Next","tagName":"h2"},{"title":"Installing via pip​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#installing-via-pip","content":" We recommend installing Ax via pip (even if using Conda environment):  conda install pytorch torchvision -c pytorch # OSX only (details below) pip install ax-platform   Installation will use Python wheels from PyPI, available for OSX, Linux, and Windows.  Note: Make sure the pip being used to install ax-platform is actually the one from the newly created Conda environment. If you're using a Unix-based OS, you can use which pip to check.  Recommendation for MacOS users: PyTorch is a required dependency of BoTorch, and can be automatically installed via pip. However, we recommend you install PyTorch manually before installing Ax, using the Anaconda package manager. Installing from Anaconda will link against MKL (a library that optimizes mathematical computation for Intel processors). This will result in up to an order-of-magnitude speed-up for Bayesian optimization, whereas installing PyTorch from pip does not link against MKL.  If you need CUDA on MacOS, you will need to build PyTorch from source. Please consult the PyTorch installation instructions above.  ","version":"Next","tagName":"h3"},{"title":"Optional Dependencies​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#optional-dependencies","content":" To use Ax with a notebook environment, you will need Jupyter. Install it first:  pip install jupyter   If you want to store the experiments in MySQL, you will need SQLAlchemy:  pip install SQLAlchemy   ","version":"Next","tagName":"h3"},{"title":"Latest Version​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#latest-version","content":" ","version":"Next","tagName":"h2"},{"title":"Installing from Git​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#installing-from-git","content":" You can install the latest (bleeding edge) version from GitHub:  pip install 'git+https://github.com/facebook/Ax.git#egg=ax-platform'   See also the recommendation for installing PyTorch for MacOS users above.  At times, the bleeding edge for Ax can depend on bleeding edge versions of BoTorch (or GPyTorch). We therefore recommend installing those from Git as well:  pip install git+https://github.com/cornellius-gp/gpytorch.git pip install git+https://github.com/pytorch/botorch.git   ","version":"Next","tagName":"h3"},{"title":"Optional Dependencies​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#optional-dependencies-1","content":" To use Ax with a notebook environment, you will need Jupyter. Install it first:  pip install 'git+https://github.com/facebook/Ax.git#egg=ax-platform[notebook]'   If storing Ax experiments via SQLAlchemy in MySQL or SQLite:  pip install 'git+https://github.com/facebook/Ax.git#egg=ax-platform[mysql]'   ","version":"Next","tagName":"h3"},{"title":"Development​","type":1,"pageTitle":"Installation","url":"/Ax/docs/installation#development","content":" When contributing to Ax, we recommend cloning the repository and installing all optional dependencies:  # bleeding edge versions of GPyTorch + BoTorch are recommended pip install git+https://github.com/cornellius-gp/gpytorch.git pip install git+https://github.com/pytorch/botorch.git git clone https://github.com/facebook/ax.git --depth 1 cd ax pip install -e .[notebook,mysql,dev]   See recommendation for installing PyTorch for MacOS users above.  The above example limits the cloned directory size via the--depthargument to git clone. If you require the entire commit history you may remove this argument. ","version":"Next","tagName":"h2"},{"title":"Welcome to Ax Tutorials","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/","content":"Welcome to Ax Tutorials Here you can learn about the structure and applications of Ax from examples. Our 3 API tutorials: Loop, Service, and Developer — are a good place to start. Each tutorial showcases optimization on a constrained Hartmann6 problem, with the Loop API being the simplest to use and the Developer API being the most customizable. NOTE: We recommend the Service API for the vast majority of use cases. This API provides an ideal balance of flexibility and simplicity for most users, and we are in the process of consolidating Ax usage around it more formally. Further, we explore the different components available in Ax in more detail. The components explored below serve to set up an experiment, visualize its results, configure an optimization algorithm, run an entire experiment in a managed closed loop, and combine BoTorch components in Ax in a modular way. Visualizations illustrates the different plots available to view and understand your results. GenerationStrategy steps through setting up a way to specify the optimization algorithm (or multiple). A GenerationStrategy is an important component of Service API and the Scheduler. Scheduler demonstrates an example of a managed and configurable closed-loop optimization, conducted in an asyncronous fashion. Scheduler is a manager abstraction in Ax that deploys trials, polls them, and uses their results to produce more trials. Modular BoTorchModel walks though a new beta-feature — an improved interface between Ax and BoTorch — which allows for combining arbitrary BoTorch components like AcquisitionFunction, Model, AcquisitionObjective etc. into a single Model in Ax. Our other Bayesian Optimization tutorials include: Hyperparameter Optimization for PyTorch provides an example of hyperparameter optimization with Ax and integration with an external ML library. Hyperparameter Optimization on SLURM via SubmitIt shows how to use the AxClient to schedule jobs and tune hyperparameters on a Slurm cluster. Multi-Task Modeling illustrates multi-task Bayesian Optimization on a constrained synthetic Hartmann6 problem. Multi-Objective Optimization demonstrates Multi-Objective Bayesian Optimization on a synthetic Branin-Currin test function. Trial-Level Early Stopping shows how to use trial-level early stopping on an ML training job to save resources and iterate faster. For experiments done in a real-life setting, refer to our field experiments tutorials: Bandit Optimization shows how Thompson Sampling can be used to intelligently reallocate resources to well-performing configurations in real-time. Human-in-the-Loop Optimization walks through manually influencing the course of optimization in real-time.","keywords":"","version":"Next"},{"title":"Storage","type":0,"sectionRef":"#","url":"/Ax/docs/storage","content":"","keywords":"","version":"Next"},{"title":"JSON​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#json","content":" ","version":"Next","tagName":"h2"},{"title":"Saving​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#saving","content":" To save an experiment to JSON, specify the filepath:  from ax import Experiment from ax.storage.json_store.save import save_experiment experiment = Experiment(...) filepath = &quot;experiments/experiment.json&quot; save_experiment(experiment, filepath)   The experiment (including attached data) will be serialized and saved to the specified file.  ","version":"Next","tagName":"h3"},{"title":"Updating​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#updating","content":" To update a JSON-backed experiment, re-save to the same file.  ","version":"Next","tagName":"h3"},{"title":"Loading​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#loading","content":" To load an experiment from JSON, specify the filepath again:  from ax.storage.json_store.load import load_experiment experiment = load_experiment(filepath)   ","version":"Next","tagName":"h3"},{"title":"Customizing​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#customizing","content":" If you add a custom Metric or Runner and want to ensure it is saved to JSON properly, create a RegistryBundle, which bundles together encoding and decoding logic for use in the various save/load functions as follows:  from ax import Experiment, Metric, Runner, SearchSpace from ax.storage.json_store.load import load_experiment from ax.storage.json_store.save import save_experiment from ax.storage.registry_bundle import RegistryBundle # Minimal custom runner/metric. class MyRunner(Runner): def run(): pass class MyMetric(Metric): pass # Minimal experiment must have a search space, plus our custom classes. experiment = Experiment( search_space=SearchSpace(parameters=[]), runner=MyRunner(), tracking_metrics=[MyMetric(name=&quot;my_metric&quot;)] ) # A RegistryBundle allows Ax to encode/decode the custom classes. bundle = RegistryBundle( runner_clss={MyRunner: None} metric_clss={MyMetric: None}, ) filepath = &quot;experiments/experiment.json&quot; save_experiment(experiment=experiment, filepath=filepath, encoder_registry=bundle.encoder_registry) loaded_experiment=load_experiment(filepath=filepath, decoder_registry=bundle.decoder_registry)   ","version":"Next","tagName":"h3"},{"title":"SQL​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#sql","content":" ","version":"Next","tagName":"h2"},{"title":"Saving​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#saving-1","content":" To save an experiment to SQL, first initialize a session by passing a URL pointing to your database. Such a URL is typically composed of a dialect (e.g. sqlite, mysql, postgresql), optional driver (DBAPI used to connect to the database; e.g. psycopg2 for postgresql), username, password, hostname, and database name. A more detailed explanation how to generate a URL can be found in the SQLAlchemy docs.  from ax.storage.sqa_store.db import init_engine_and_session_factory # url is of the form &quot;dialect+driver://username:password@host:port/database&quot; init_engine_and_session_factory(url=&quot;postgresql+psycopg2://[USERNAME]:[PASSWORD]@localhost:[PORT]/[DATABASE]&quot;)   Then create all tables:  from ax.storage.sqa_store.db import get_engine, create_all_tables engine = get_engine() create_all_tables(engine)   Then save your experiment:  from ax import Experiment from ax.storage.sqa_store.save import save_experiment experiment = Experiment(...) save_experiment(experiment)   The experiment (including attached data) will be saved to the corresponding tables.  Alternatively, you can pass a creator function instead of a url to init_engine_and_session_factory:  from ax import Experiment from ax.storage.sqa_store.db import init_engine_and_session_factory from ax.storage.sqa_store.save import save_experiment init_engine_and_session_factory(creator=creator) experiment = Experiment(...) save_experiment(experiment)   ","version":"Next","tagName":"h3"},{"title":"Updating​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#updating-1","content":" To update a SQL-backed experiment, call save_experiment(experiment) again. Ax will determine what updates to perform.  ","version":"Next","tagName":"h3"},{"title":"Loading​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#loading-1","content":" To load an experiment from SQL, specify the name:  from ax import Experiment from ax.storage.sqa_store.db import init_engine_and_session_factory from ax.storage.sqa_store.load import load_experiment init_engine_and_session_factory(url=dialect+driver://username:password@host:port/database) experiment = load_experiment(experiment_name)   ","version":"Next","tagName":"h3"},{"title":"Customizing​","type":1,"pageTitle":"Storage","url":"/Ax/docs/storage#customizing-1","content":" Adding a new metric or runner:  If you add a custom Metric or Runner and want to ensure it is saved to SQL properly, create a RegistryBundle, which bundles together encoding and decoding logic for use in the various save/load functions as follows:  from ax import Experiment, RangeParameter, ParameterType from ax.storage.sqa_store.load import load_experiment from ax.storage.sqa_store.save import save_experiment from ax.storage.sqa_store.sqa_config import SQAConfig # Minimal custom runner/metric. class MyRunner(Runner): def run(): pass class MyMetric(Metric): pass # Minimal experiment for SQA must have a name and a nonempty SearchSpace, plus our custom classes. experiment = Experiment( name=&quot;my_experiment&quot;, search_space=SearchSpace( parameters=[ RangeParameter( lower=0, upper=1, name=&quot;my_parameter&quot;, parameter_type=ParameterType.FLOAT ) ] ), runner=MyRunner(), tracking_metrics=[MyMetric(name=&quot;my_metric&quot;)], ) # The RegistryBundle contains our custom classes. bundle = RegistryBundle( metric_clss={MyMetric: None}, runner_clss={MyRunner: None} ) # Abstract this into a SQAConfig as follows, to make loading/saving a bit simpler. sqa_config = SQAConfig( json_encoder_registry=bundle.encoder_registry, json_decoder_registry=bundle.decoder_registry, metric_registry=bundle.metric_registry, runner_registry=bundle.runner_registry, ) save_experiment(experiment, config=sqa_config) loaded_experiment = load_experiment(experiment_name=&quot;my_experiment&quot;, config=sqa_config)   Specifying experiment types:  If you choose to add types to your experiments, create an Enum mapping experiment types to integer representations, pass this Enum to a custom instance of SQAConfig, and then pass the config to sqa_store.save:  from ax import Experiment from ax.storage.sqa_store.save import save_experiment from ax.storage.sqa_store.sqa_config import SQAConfig from enum import Enum class ExperimentType(Enum): DEFAULT: 0 config = SQAConfig(experiment_type_enum=ExperimentType) save_experiment(experiment, config=config)   Specifying generator run types:  If you choose to add types to your generator runs (beyond the existing status_quo type), create an enum mapping generator run types to integer representations, pass this enum to a custom instance of SQAConfig, and then pass the config to sqa_store.save:  from ax import Experiment from ax.storage.sqa_store.save import save_experiment from ax.storage.sqa_store.sqa_config import SQAConfig from enum import Enum class GeneratorRunType(Enum): DEFAULT: 0 STATUS_QUO: 1 config = SQAConfig(generator_run_type_enum=GeneratorRunType) save_experiment(experiment, config=config)  ","version":"Next","tagName":"h3"},{"title":"Trial Evaluation","type":0,"sectionRef":"#","url":"/Ax/docs/trial-evaluation","content":"","keywords":"","version":"Next"},{"title":"[RECOMMENDED] Service API​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#recommended-service-api","content":" The Service API AxClientexposesget_next_trial, as well ascomplete_trial. The user is responsible for evaluating the trial parameters and passing the results tocomplete_trial.  ... for i in range(25): parameters, trial_index = ax_client.get_next_trial() raw_data = evaluate_trial(parameters) ax_client.complete_trial(trial_index=trial_index, raw_data=raw_data)   ","version":"Next","tagName":"h2"},{"title":"Evaluating Trial Parameters​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#evaluating-trial-parameters","content":" In the Service API, thecomplete_trialmethod requires raw_data evaluated from the parameters suggested byget_next_trial.  The data can be in the form of:  A dictionary of metric names to tuples of (mean and SEM)A single (mean, SEM) tupleA single mean  In the second case, Ax will assume that the mean and the SEM are for the experiment objective (if the evaluations are noiseless, simply provide an SEM of 0.0). In the third case, Ax will assume that observations are corrupted by Gaussian noise with zero mean and unknown SEM, and infer the SEM from the data (this is equivalent to specifying an SEM of None). Note that if the observation noise is non-zero (either provided or inferred), the &quot;best arm&quot; suggested by Ax may not always be the one whose evaluation returned the best observed value (as the &quot;best arm&quot; is selected based on the model-predicted mean).  For example, this evaluation function computes mean and SEM forHartmann6 function and for the L2-norm. We return 0.0 for SEM since the observations are noiseless:  from ax.utils.measurement.synthetic_functions import hartmann6 def hartmann_evaluation_function(parameterization): x = np.array([parameterization.get(f&quot;x{i+1}&quot;) for i in range(6)]) # Standard error is 0 since we are computing a synthetic function. return {&quot;hartmann6&quot;: (hartmann6(x), 0.0), &quot;l2norm&quot;: (np.sqrt((x ** 2).sum()), 0.0)}   This function computes just the objective mean and SEM, assuming theBranin function is the objective of the experiment:  from ax.utils.measurement.synthetic_functions import branin def branin_evaluation_function(parameterization): # Standard error is 0 since we are computing a synthetic function. return (branin(parameterization.get(&quot;x1&quot;), parameterization.get(&quot;x2&quot;)), 0.0)   Alternatively, if the SEM is unknown, we could use the following form:  lambda parameterization: branin(parameterization.get(&quot;x1&quot;), parameterization.get(&quot;x2&quot;))   This is equivalent to returning None for the SEM:  from ax.utils.measurement.synthetic_functions import branin def branin_evaluation_function_unknown_sem(parameterization): return (branin(parameterization.get(&quot;x1&quot;), parameterization.get(&quot;x2&quot;)), None)   ","version":"Next","tagName":"h3"},{"title":"Loop API​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#loop-api","content":" The optimize function requires an evaluation_function, which accepts parameters and returns raw data in the format described above. It can also accept a weight parameter, a nullable float representing the fraction of available data on which the parameterization should be evaluated. For example, this could be a downsampling rate in case of hyperparameter optimization (what portion of data the ML model should be trained on for evaluation) or the percentage of users exposed to a given configuration in A/B testing. This weight is not used in unweighted experiments and defaults to None.  ","version":"Next","tagName":"h2"},{"title":"Developer API​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#developer-api","content":" The Developer API is supported by theExperiment class. In this paradigm, the user specifies:  Runner: Defines how to deploy the experiment.List of Metrics: Each defines how to compute/fetch data for a given objective or outcome.  The experiment requires a generator_run to create a new trial or batch trial. A generator run can be generated by a model. The trial then has its own runand mark_complete methods.  ... sobol = Models.SOBOL(exp.search_space) for i in range(5): trial = exp.new_trial(generator_run=sobol.gen(1)) trial.run() trial.mark_completed() for i in range(15): gpei = Models.BOTORCH_MODULAR(experiment=exp, data=exp.fetch_data()) generator_run = gpei.gen(1) trial = exp.new_trial(generator_run=generator_run) trial.run() trial.mark_completed()   ","version":"Next","tagName":"h2"},{"title":"Custom Metrics​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#custom-metrics","content":" Similar to a trial evaluation in the Service API, a custom metric computes a mean and SEM for each arm of a trial. However, the metric's fetch_trial_datamethod will be called automatically by the experiment'sfetch_data method. If there are multiple objectives or outcomes that need to be optimized for, each needs its own metric.  class MyMetric(Metric): def fetch_trial_data(self, trial): records = [] for arm_name, arm in trial.arms_by_name.items(): params = arm.parameters records.append({ &quot;arm_name&quot;: arm_name, &quot;metric_name&quot;: self.name, &quot;mean&quot;: self.foo(params[&quot;x1&quot;], params[&quot;x2&quot;]), &quot;sem&quot;: 0.0, &quot;trial_index&quot;: trial.index, }) return Data(df=pd.DataFrame.from_records(records))   ","version":"Next","tagName":"h3"},{"title":"Adding Your Own Runner​","type":1,"pageTitle":"Trial Evaluation","url":"/Ax/docs/trial-evaluation#adding-your-own-runner","content":" In order to control how the experiment is deployed, you can add your own runner. To do so, subclass Runner and implement the run method andstaging_requiredproperty.  The run method accepts aTrial and returns a JSON-serializable dictionary of any necessary tracking info to fetch data later from this external system. A unique identifier or name for this trial in the external system should be stored in this dictionary with the key &quot;name&quot;, and this can later be accessed via trial.deployed_name.  Thestaging_requiredindicates whether the trial requires an intermediate staging period before evaluation begins. This property returns False by default.  An example implementation is given below:  from foo_system import deploy_to_foo from ax import Runner class FooRunner(Runner): def __init__(self, foo_param): self.foo_param = foo_param def run(self, trial): name_to_params = { arm.name: arm.parameters for arm in trial.arms } run_metadata = deploy_to_foo(self.foo_param, name_to_params) return run_metadata @property def staging_required(self): return False   This is then invoked by calling:  exp = Experiment(...) exp.runner = FooRunner(foo_param=&quot;foo&quot;) trial = exp.new_batch_trial() # This calls runner's run method and stores metadata output # in the trial.run_metadata field trial.run()  ","version":"Next","tagName":"h3"},{"title":"Models","type":0,"sectionRef":"#","url":"/Ax/docs/models","content":"","keywords":"","version":"Next"},{"title":"Using models in Ax​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#using-models-in-ax","content":" In the optimization algorithms implemented by Ax, models predict the outcomes of metrics within an experiment evaluated at a parameterization, and are used to predict metrics or suggest new parameterizations for trials. Models in Ax are created using factory functions from the ax.modelbridge.factory. All of these models share a common API with predict() to make predictions at new points and gen() to generate new candidates to be tested. There are a variety of models available in the factory; here we describe the usage patterns for the primary model types and show how the various Ax utilities can be used with models.  Sobol sequence​  The get_sobol function is used to construct a model that produces a quasirandom Sobol sequence whengen is called. This code generates a scrambled Sobol sequence of 10 points:  from ax.modelbridge.factory import get_sobol m = get_sobol(search_space) gr = m.gen(n=10)   The output of gen is a GeneratorRun object that contains the generated points, along with metadata about the generation process. The generated arms can be accessed at GeneratorRun.arms.  Additional arguments can be passed to get_sobol such as scramble=False to disable scrambling, and seed to set a seed (see model API).  Sobol sequences are typically used to select initialization points, and this model does not implement predict. It can be used on search spaces with any combination of discrete and continuous parameters.  Gaussian Process with EI​  Gaussian Processes (GPs) are used for Bayesian Optimization in Ax, the get_GPEI function constructs a model that fits a GP to the data, and uses the EI acquisition function to generate new points on calls to gen. This code fits a GP and generates a batch of 5 points which maximizes EI:  from ax.modelbridge.factory import get_GPEI m = get_GPEI(experiment, data) gr = m.gen(n=5, optimization_config=optimization_config)   In contrast to get_sobol, the GP requires data and is able to make predictions. We make predictions by constructing a list of ObservationFeatures objects with the parameter values for which we want predictions:  from ax.core.observation import ObservationFeatures obs_feats = [ ObservationFeatures(parameters={'x1': 3.14, 'x2': 2.72}), ObservationFeatures(parameters={'x1': 1.41, 'x2': 1.62}), ] f, cov = m.predict(obs_feats)   The output of predict is the mean estimate of each metric and the covariance (across metrics) for each point.  All Ax models that implement predict can be used with the built-in plotting utilities, which can produce plots of model predictions on 1-d or 2-d slices of the parameter space:  from ax.plot.slice import plot_slice from ax.utils.notebook.plotting import render, init_notebook_plotting init_notebook_plotting() render(plot_slice( model=m, param_name='x1', # slice on values of 'x1' metric_name='metric_a', slice_values={'x2': 7.5}, # Fix at this value for the slice ))     from ax.plot.contour import plot_contour render(plot_contour( model=m, param_x='x1', param_y='x2', metric_name='metric_a', ))     Ax also includes utilities for cross validation to assess model predictive performance. Leave-one-out cross validation can be performed as follows:  from ax.modelbridge.cross_validation import cross_validate, compute_diagnostics cv = cross_validate(model) diagnostics = compute_diagnostics(cv)   compute_diagnostics computes a collection of diagnostics of model predictions, such as the correlation between predictions and actual values, and the p-value for a Fisher test of the model's ability to distinguish high values from low. A very useful tool for assessing model performance is to plot the cross validated predictions against the actual observed values:  from ax.plot.diagnostic import interact_cross_validation render(interact_cross_validation(cv))     If the model fits the data well, the values will lie along the diagonal. Poor GP fits tend to produce cross validation plots that are flat with high predictive uncertainty - such fits are unlikely to produce good candidates in gen.  By default, this model will apply a number of transformations to the feature space, such as one-hot encoding of ChoiceParameters and log transformation of RangeParameters which have log_scale set to True. Transforms are also applied to the observed outcomes, such as standardizing the data for each metric. See the section below on Transforms for a description of the default transforms, and how new transforms can be implemented and included.  GPs typically does a good job of modeling continuous parameters (RangeParameters). If the search space contains ChoiceParameters, they will be one-hot-encoded and the GP fit in the encoded space. A search space with a mix of continuous parameters and ChoiceParameters that take a small number of values can be modeled effectively with a GP, but model performance may be poor if there are more than about 20 parameters after one-hot encoding. Cross validation is an effective tool for determining usefulness of the GP on a particular problem.  In discrete spaces where the GP does not predict well, a multi-armed bandit approach is often preferred, and we now discuss the models suitable for that approach.  Support for mixed search spaces and categorical variables​  The most common way of dealing with categorical variables in Bayesian optimization is to one-hot encode the categories to allow fitting a GP model in a continuous space. In this setting, a categorical variable with categories [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;] is represented by three new variables (one for each category). While this is a convenient choice, it can drastically increase the dimensionality of the search space. In addition, the acquisition function is often optimized in the corresponding continuous space and the final candidate is selected by rounding back to the original space, which may result in selecting sub-optimal points according to the acquisition function.  Our new approach uses separate kernels for the categorical and ordinal (continuous/integer) variables. In particular, we use a kernel of the form: $$k(x, y) = k_\\text{cat}(x_\\text{cat}, y_\\text{cat}) \\times k_\\text{ord}(x_\\text{ord}, y_\\text{ord}) + k_\\text{cat}(x_\\text{cat}, y_\\text{cat}) + k_\\text{ord}(x_\\text{ord}, y_\\text{ord})$$ For the ordinal variables we can use a standard kernel such as Matérn-5/2, but for the categorical variables we need a way to compute distances between the different categories. A natural choice is to set the distance is 0 if two categories are equal and 1 otherwise, similar to the idea of Hamming distances. This approach can be combined with the idea automatic relevance determination (ARD) where each categorical variable has its own lengthscale. Rather than optimizing the acquisition function in a continuously relaxed space, we optimize it separately over each combination of the categorical variables. While this is likely to result in better optimization performance, it may lead to slow optimization of the acquisition function when there are many categorical variables.  Empirical Bayes and Thompson sampling​  For Bandit optimization, The get_empirical_bayes_thompson factory function returns a model that applies empirical Bayes shrinkage to a discrete set of arms, and then uses Thompson sampling to construct a policy with the weight that should be allocated to each arms. Here we apply empirical Bayes to the data and use Thompson sampling to generate a policy that is truncated at n=10 arms:  from ax.modelbridge.factory import get_empirical_bayes_thompson m = get_empirical_bayes_thompson(experiment, data) gr = m.gen(n=10, optimization_config=optimization_config)   The arms and their corresponding weights can be accessed as gr.arm_weights.  As with the GP, we can use predict to evaluate the model at points of our choosing. However, because this is a purely in-sample model, those points should correspond to arms that were in the data. The model prediction will return the estimate at that point after applying the empirical Bayes shrinkage:  f, cov = m.predict([ObservationFeatures(parameters={'x1': 3.14, 'x2': 2.72})])   We can generate a plot that shows the predictions for each arm with the shrinkage using plot_fitted, which shows model predictions on all in-sample arms:  from ax.plot.scatter import plot_fitted render(plot_fitted(m, metric=&quot;metric_a&quot;, rel=False))     Factorial designs​  The factory function get_factorial can be used to construct a factorial design on a set of ChoiceParameters.  from ax.modelbridge.factory import get_factorial m = get_factorial(search_space) gr = m.gen(n=10)   Like the Sobol sequence, the factorial model is only used to generate points and does not implement predict.  ","version":"Next","tagName":"h2"},{"title":"Deeper dive: organization of the modeling stack​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#deeper-dive-organization-of-the-modeling-stack","content":" Ax uses a bridge design to provide a unified interface for models, while still allowing for modularity in how different types of models are implemented. The modeling stack consists of two layers: the ModelBridge and the Model.  The ModelBridge is the object that is directly used in Ax: model factories return ModelBridge objects, and plotting and cross validation tools operate on a ModelBridge. The ModelBridge defines a unified API for all of the models used in Ax via methods like predict and gen. Internally, it is responsible for transforming Ax objects like Arm and Data into objects which are then consumed downstream by a Model.  Model objects are only used in Ax via a ModelBridge. Each Model object defines an API which does not use Ax objects, allowing for modularity of different model types and making it easy to implement new models. For example, the TorchModel defines an API for a model that operates on torch tensors. There is a 1-to-1 link between ModelBridge objects and Model objects. For instance, the TorchModelBridge takes in Ax objects, converts them to torch tensors, and sends them along to the TorchModel. Similar pairings exist for all of the different model types:  ModelBridge\tModel\tExample implementation\tTorchModelBridge\tTorchModel\tBotorchModel DiscreteModelBridge\tDiscreteModel\tThompsonSampler RandomModelBridge\tRandomModel\tSobolGenerator\t  This structure allows for different models like the GP in BotorchModel and the Random Forest in RandomForest to share an interface and use common plotting tools at the level of the ModelBridge, while each is implemented using its own torch or numpy structures.  The primary role of the ModelBridge is to act as a transformation layer. This includes transformations to the data, search space, and optimization config such as standardization and log transforms, as well as the final transform from Ax objects into the objects consumed by the Model. We now describe how transforms are implemented and used in the ModelBridge.  ","version":"Next","tagName":"h2"},{"title":"Transforms​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#transforms","content":" The transformations in the ModelBridge are done by chaining together a set of individual Transform objects. For continuous space models obtained via factory functions (get_sobol and get_GPEI), the following transforms will be applied by default, in this sequence:  RemoveFixed: Remove FixedParameters from the search space.OrderedChoiceEncode: ChoiceParameters with is_ordered set to True are encoded as a sequence of integers.OneHot: ChoiceParameters with is_ordered set to False are one-hot encoded.IntToFloat: Integer-valued RangeParameters are converted to have float values.Log: RangeParameters with log_scale set to True are log transformed.UnitX: All float RangeParameters are mapped to [0, 1].Derelativize: Constraints relative to status quo are converted to constraints on raw values.StandardizeY: The Y values for each metric are standardized (subtract mean, divide by standard deviation).  Each transform defines both a forward and backwards transform. Arm parameters are passed through the forward transform before being sent along to the Model. The Model works entirely in the transformed space, and when new candidates are generated, they are passed through all of the backwards transforms so the ModelBridge returns points in the original space.  New transforms can be implemented by creating a subclass of Transform, which defines the interface for all transforms. There are separate methods for transforming the search space, optimization config, observation features, and observation data. Transforms that operate on only some aspects of the problem do not need to implement all methods, for instance, Log implements only transform_observation_features (to log transform the parameters), transform_search_space (to log transform the search space bounds), and untransform_observation_features (to apply the inverse transform).  The (ordered) list of transforms to apply is an input to the ModelBridge, and so can easily be altered to add new transforms. It is important that transforms be applied in the right order. For instance, the StandardizeY and Winsorize transforms both transform the observed metric values. Applying them in the order [StandardizeY, Winsorize] could produce very different results than [Winsorize, StandardizeY]. In the former case, outliers would have already been included in the standardization (a procedure sensitive to outliers), and so the second approach that winsorizes first is preferred.  See the API reference for the full collection of implemented transforms.  ","version":"Next","tagName":"h2"},{"title":"Implementing new models​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#implementing-new-models","content":" The structure of the modeling stack makes it easy to implement new models and use them inside Ax. There are two ways this might be done.  ","version":"Next","tagName":"h2"},{"title":"Using an existing Model interface​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#using-an-existing-model-interface","content":" The easiest way to implement a new model is if it can be adapted to the one of the existing Model interfaces: (TorchModel, DiscreteModel, or RandomModel). The class definition provides the interface for each of the methods that should be implemented in order for Ax to be able to fully use the new model. Note however that not all methods must need be implemented to use some Ax functionality. For instance, an implementation of TorchModel that implements only fit and predict can be used to fit data and make plots in Ax; however, it will not be able to generate new candidates (requires implementing gen) or be used with Ax's cross validation utility (requires implementing cross_validate).  Once the new model has been implemented, it can be used in Ax with the corresponding ModelBridge from the table above. For instance, suppose a new torch-based model was implemented as a subclass of TorchModel. We can use that model in Ax like:  new_model_obj = NewModel(init_args) # An instance of the new model class m = TorchModelBridge( experiment=experiment, search_space=search_space, data=data, model=new_model_obj, transforms=[UnitX, StandardizeY], # Include the desired set of transforms )   The ModelBridge object m can then be used with plotting and cross validation utilities exactly the same way as the built-in models.  ","version":"Next","tagName":"h3"},{"title":"Creating a new Model interface​","type":1,"pageTitle":"Models","url":"/Ax/docs/models#creating-a-new-model-interface","content":" If none of the existing Model interfaces work are suitable for the new model type, then a new interface will have to be created. This involves two steps: creating the new model interface and creating the new model bridge. The new model bridge must be a subclass of ModelBridge that implements ModelBridge._fit, ModelBridge._predict, ModelBridge._gen, and ModelBridge._cross_validate. The implementation of each of these methods will transform the Ax objects in the inputs into objects required for the interface with the new model type. The model bridge will then call out to the new model interface to do the actual modeling work. All of the ModelBridge/Model pairs in the table above provide examples of how this interface can be defined. The main key is that the inputs on the ModelBridge side are fixed, but those inputs can then be transformed in whatever way is desired for the downstream Model interface to be that which is most convenient for implementing the model.  &lt;script type=&quot;text/javascript&quot; src=&quot;assets/slice.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;assets/contour.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;assets/cv.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;assets/fitted.js&quot;&gt;&lt;/script&gt;  ","version":"Next","tagName":"h3"},{"title":"Using external methods for candidate generation in Ax","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/external_generation_node/","content":"","keywords":"","version":"Next"},{"title":"Construct the GenerationStrategy​","type":1,"pageTitle":"Using external methods for candidate generation in Ax","url":"/Ax/docs/tutorials/external_generation_node/#construct-the-generationstrategy","content":" We will use Sobol for the first 5 trials and defer to random forest for the rest.  generation_strategy = GenerationStrategy( name=&quot;Sobol+RandomForest&quot;, nodes=[ GenerationNode( node_name=&quot;Sobol&quot;, model_specs=[ModelSpec(Models.SOBOL)], transition_criteria=[ MaxTrials( # This specifies the maximum number of trials to generate from this node, # and the next node in the strategy. threshold=5, block_transition_if_unmet=True, transition_to=&quot;RandomForest&quot; ) ], ), RandomForestGenerationNode(num_samples=128, regressor_options={}), ], )   ","version":"Next","tagName":"h2"},{"title":"Run a simple experiment using AxClient​","type":1,"pageTitle":"Using external methods for candidate generation in Ax","url":"/Ax/docs/tutorials/external_generation_node/#run-a-simple-experiment-using-axclient","content":" More details on how to use AxClient can be found in thetutorial.  ax_client = AxClient(generation_strategy=generation_strategy) ax_client.create_experiment( name=&quot;hartmann_test_experiment&quot;, parameters=[ { &quot;name&quot;: f&quot;x{i}&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], &quot;value_type&quot;: &quot;float&quot;, # Optional, defaults to inference from type of &quot;bounds&quot;. } for i in range(1, 7) ], objectives={&quot;hartmann6&quot;: ObjectiveProperties(minimize=True)}, ) def evaluate(parameterization: TParameterization) -&gt; Dict[str, Tuple[float, float]]: x = np.array([parameterization.get(f&quot;x{i+1}&quot;) for i in range(6)]) return {&quot;hartmann6&quot;: (checked_cast(float, hartmann6(x)), 0.0)}   Out: [INFO 09-30 17:09:47] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-30 17:09:47] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).  ","version":"Next","tagName":"h2"},{"title":"Run the optimization loop​","type":1,"pageTitle":"Using external methods for candidate generation in Ax","url":"/Ax/docs/tutorials/external_generation_node/#run-the-optimization-loop","content":" for i in range(15): parameterization, trial_index = ax_client.get_next_trial() ax_client.complete_trial( trial_index=trial_index, raw_data=evaluate(parameterization) )   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 0 with parameters {'x1': 0.384079, 'x2': 0.15172, 'x3': 0.372233, 'x4': 0.318388, 'x5': 0.153991, 'x6': 0.273027} using model Sobol.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 0 with data: {'hartmann6': (-0.607185, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 1 with parameters {'x1': 0.856816, 'x2': 0.722, 'x3': 0.97936, 'x4': 0.54725, 'x5': 0.905573, 'x6': 0.654784} using model Sobol.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 1 with data: {'hartmann6': (-0.000897, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 2 with parameters {'x1': 0.665045, 'x2': 0.473207, 'x3': 0.065621, 'x4': 0.062136, 'x5': 0.610325, 'x6': 0.80947} using model Sobol.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 2 with data: {'hartmann6': (-0.106968, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 3 with parameters {'x1': 0.075361, 'x2': 0.90295, 'x3': 0.707786, 'x4': 0.822042, 'x5': 0.361424, 'x6': 0.176813} using model Sobol.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 3 with data: {'hartmann6': (-0.211837, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 4 with parameters {'x1': 0.229187, 'x2': 0.291547, 'x3': 0.752234, 'x4': 0.993827, 'x5': 0.40329, 'x6': 0.490673} using model Sobol.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 4 with data: {'hartmann6': (-0.032992, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs) [INFO 09-30 17:09:47] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.08833, 'x2': 0.184134, 'x3': 0.647417, 'x4': 0.407827, 'x5': 0.518476, 'x6': 0.723195} using model RandomForest.  Out: [INFO 09-30 17:09:47] ax.service.ax_client: Completed trial 5 with data: {'hartmann6': (-1.332971, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs) [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.009592, 'x2': 0.177133, 'x3': 0.753087, 'x4': 0.161454, 'x5': 0.646799, 'x6': 0.545674} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 6 with data: {'hartmann6': (-0.59257, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.950225, 'x2': 0.834228, 'x3': 0.589171, 'x4': 0.148809, 'x5': 0.093415, 'x6': 0.662494} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 7 with data: {'hartmann6': (-0.058106, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.543456, 'x2': 0.070669, 'x3': 0.989356, 'x4': 0.990062, 'x5': 0.483633, 'x6': 0.705604} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 8 with data: {'hartmann6': (-0.026201, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.763684, 'x2': 0.333733, 'x3': 0.587216, 'x4': 0.164702, 'x5': 0.695464, 'x6': 0.072302} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 9 with data: {'hartmann6': (-0.011451, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 10 with parameters {'x1': 0.717886, 'x2': 0.012338, 'x3': 0.617406, 'x4': 0.035417, 'x5': 0.382238, 'x6': 0.295558} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 10 with data: {'hartmann6': (-0.208702, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 11 with parameters {'x1': 0.960026, 'x2': 0.623983, 'x3': 0.349629, 'x4': 0.762285, 'x5': 0.121308, 'x6': 0.174155} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 11 with data: {'hartmann6': (-0.007708, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 12 with parameters {'x1': 0.184547, 'x2': 0.340669, 'x3': 0.974017, 'x4': 0.473782, 'x5': 0.53758, 'x6': 0.970643} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 12 with data: {'hartmann6': (-0.357963, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 13 with parameters {'x1': 0.821476, 'x2': 0.118512, 'x3': 0.342596, 'x4': 0.752414, 'x5': 0.857898, 'x6': 0.620802} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 13 with data: {'hartmann6': (-0.001255, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return fit_method(estimator, *args, **kwargs)  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Generated new trial 14 with parameters {'x1': 0.194574, 'x2': 0.171522, 'x3': 0.645535, 'x4': 0.87357, 'x5': 0.486316, 'x6': 0.888665} using model RandomForest.  Out: [INFO 09-30 17:09:48] ax.service.ax_client: Completed trial 14 with data: {'hartmann6': (-0.150202, 0.0)}.  ","version":"Next","tagName":"h3"},{"title":"View the trials generated during optimization​","type":1,"pageTitle":"Using external methods for candidate generation in Ax","url":"/Ax/docs/tutorials/external_generation_node/#view-the-trials-generated-during-optimization","content":" exp_df = exp_to_df(ax_client.experiment) exp_df   Out: [WARNING 09-30 17:09:48] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\thartmann6\tx1\tx2\tx3\tx4\tx5\tx60\t0\t0_0\tCOMPLETED\tSobol\t-0.607185\t0.384079\t0.15172\t0.372233\t0.318388\t0.153991\t0.273027 1\t1\t1_0\tCOMPLETED\tSobol\t-0.000897\t0.856816\t0.722\t0.97936\t0.54725\t0.905573\t0.654784 2\t2\t2_0\tCOMPLETED\tSobol\t-0.106968\t0.665045\t0.473207\t0.065621\t0.062136\t0.610325\t0.80947 3\t3\t3_0\tCOMPLETED\tSobol\t-0.211837\t0.075361\t0.90295\t0.707786\t0.822042\t0.361424\t0.176813 4\t4\t4_0\tCOMPLETED\tSobol\t-0.032992\t0.229187\t0.291547\t0.752234\t0.993827\t0.40329\t0.490673 5\t5\t5_0\tCOMPLETED\tRandomForest\t-1.33297\t0.08833\t0.184134\t0.647417\t0.407827\t0.518476\t0.723195 6\t6\t6_0\tCOMPLETED\tRandomForest\t-0.59257\t0.009592\t0.177133\t0.753087\t0.161454\t0.646799\t0.545674 7\t7\t7_0\tCOMPLETED\tRandomForest\t-0.058106\t0.950225\t0.834228\t0.589171\t0.148809\t0.093415\t0.662494 8\t8\t8_0\tCOMPLETED\tRandomForest\t-0.026201\t0.543456\t0.070669\t0.989356\t0.990062\t0.483633\t0.705604 9\t9\t9_0\tCOMPLETED\tRandomForest\t-0.011451\t0.763684\t0.333733\t0.587216\t0.164702\t0.695464\t0.072302 10\t10\t10_0\tCOMPLETED\tRandomForest\t-0.208702\t0.717886\t0.012338\t0.617406\t0.035417\t0.382238\t0.295558 11\t11\t11_0\tCOMPLETED\tRandomForest\t-0.007708\t0.960026\t0.623983\t0.349629\t0.762285\t0.121308\t0.174155 12\t12\t12_0\tCOMPLETED\tRandomForest\t-0.357963\t0.184547\t0.340669\t0.974017\t0.473782\t0.53758\t0.970643 13\t13\t13_0\tCOMPLETED\tRandomForest\t-0.001255\t0.821476\t0.118512\t0.342596\t0.752414\t0.857898\t0.620802 14\t14\t14_0\tCOMPLETED\tRandomForest\t-0.150202\t0.194574\t0.171522\t0.645535\t0.87357\t0.486316\t0.888665  plot_objective_value_vs_trial_index( exp_df=exp_df, metric_colname=&quot;hartmann6&quot;, minimize=True, title=&quot;Hartmann6 Objective Value vs. Trial Index&quot;, )   Out: /Users/cristianlara/Projects/Ax-1.0/ax/plot/trace.py:864: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.  loading... ","version":"Next","tagName":"h3"},{"title":"Factorial design with empirical Bayes and Thompson Sampling","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/factorial/","content":"","keywords":"","version":"Next"},{"title":"1. Define the search space​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#1-define-the-search-space","content":" First, we define our search space. A factorial search space contains a ChoiceParameter for each factor, where the values of the parameter are its levels.  search_space = SearchSpace( parameters=[ ChoiceParameter( name=&quot;factor1&quot;, parameter_type=ParameterType.STRING, values=[&quot;level11&quot;, &quot;level12&quot;, &quot;level13&quot;], ), ChoiceParameter( name=&quot;factor2&quot;, parameter_type=ParameterType.STRING, values=[&quot;level21&quot;, &quot;level22&quot;], ), ChoiceParameter( name=&quot;factor3&quot;, parameter_type=ParameterType.STRING, values=[&quot;level31&quot;, &quot;level32&quot;, &quot;level33&quot;, &quot;level34&quot;], ), ] )   Out: /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:3: AxParameterWarning: is_ordered is not specified for ChoiceParameter &quot;factor1&quot;. Defaulting to False since the parameter is a string with more than 2 choices.. To override this behavior (or avoid this warning), specify is_ordered during ChoiceParameter construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied is_ordered has no effect in this particular case. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:3: AxParameterWarning: sort_values is not specified for ChoiceParameter &quot;factor1&quot;. Defaulting to False for parameters of ParameterType STRING. To override this behavior (or avoid this warning), specify sort_values during ChoiceParameter construction. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:8: AxParameterWarning: is_ordered is not specified for ChoiceParameter &quot;factor2&quot;. Defaulting to True since there are exactly two choices.. To override this behavior (or avoid this warning), specify is_ordered during ChoiceParameter construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied is_ordered has no effect in this particular case. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:8: AxParameterWarning: sort_values is not specified for ChoiceParameter &quot;factor2&quot;. Defaulting to False for parameters of ParameterType STRING. To override this behavior (or avoid this warning), specify sort_values during ChoiceParameter construction. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:13: AxParameterWarning: is_ordered is not specified for ChoiceParameter &quot;factor3&quot;. Defaulting to False since the parameter is a string with more than 2 choices.. To override this behavior (or avoid this warning), specify is_ordered during ChoiceParameter construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied is_ordered has no effect in this particular case. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/3645676115.py:13: AxParameterWarning: sort_values is not specified for ChoiceParameter &quot;factor3&quot;. Defaulting to False for parameters of ParameterType STRING. To override this behavior (or avoid this warning), specify sort_values during ChoiceParameter construction.  ","version":"Next","tagName":"h2"},{"title":"2. Define a custom metric​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#2-define-a-custom-metric","content":" Second, we define a custom metric, which is responsible for computing the mean and standard error of a given arm.  In this example, each possible parameter value is given a coefficient. The higher the level, the higher the coefficient, and the higher the coefficients, the greater the mean.  The standard error of each arm is determined by the weight passed into the evaluation function, which represents the size of the population on which this arm was evaluated. The higher the weight, the greater the sample size, and thus the lower the standard error.  from ax import Data, Metric from ax.utils.common.result import Ok import pandas as pd from random import random one_hot_encoder = skl.preprocessing.OneHotEncoder( categories=[par.values for par in search_space.parameters.values()], ) class FactorialMetric(Metric): def fetch_trial_data(self, trial): records = [] for arm_name, arm in trial.arms_by_name.items(): params = arm.parameters batch_size = 10000 noise_level = 0.0 weight = trial.normalized_arm_weights().get(arm, 1.0) coefficients = np.array([0.1, 0.2, 0.3, 0.1, 0.2, 0.1, 0.2, 0.3, 0.4]) features = np.array(list(params.values())).reshape(1, -1) encoded_features = one_hot_encoder.fit_transform(features) z = ( coefficients @ encoded_features.T + np.sqrt(noise_level) * np.random.randn() ) p = np.exp(z) / (1 + np.exp(z)) plays = np.random.binomial(batch_size, weight) successes = np.random.binomial(plays, p) records.append( { &quot;arm_name&quot;: arm_name, &quot;metric_name&quot;: self.name, &quot;trial_index&quot;: trial.index, &quot;mean&quot;: float(successes) / plays, &quot;sem&quot;: agresti_coull_sem(successes, plays), } ) return Ok(value=Data(df=pd.DataFrame.from_records(records)))   ","version":"Next","tagName":"h2"},{"title":"3. Define the experiment​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#3-define-the-experiment","content":" We now set up our experiment and define the status quo arm, in which each parameter is assigned to the lowest level.  from ax import Runner class MyRunner(Runner): def run(self, trial): trial_metadata = {&quot;name&quot;: str(trial.index)} return trial_metadata exp = Experiment( name=&quot;my_factorial_closed_loop_experiment&quot;, search_space=search_space, optimization_config=OptimizationConfig( objective=Objective(metric=FactorialMetric(name=&quot;success_metric&quot;), minimize=False) ), runner=MyRunner(), ) exp.status_quo = Arm( parameters={&quot;factor1&quot;: &quot;level11&quot;, &quot;factor2&quot;: &quot;level21&quot;, &quot;factor3&quot;: &quot;level31&quot;} )   ","version":"Next","tagName":"h2"},{"title":"4. Run an exploratory batch​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#4-run-an-exploratory-batch","content":" We then generate an a set of arms that covers the full space of the factorial design, including the status quo. There are three parameters, with two, three, and four values, respectively, so there are 24 possible arms.  factorial = Models.FACTORIAL(search_space=exp.search_space) factorial_run = factorial.gen( n=-1 ) # Number of arms to generate is derived from the search space. print(len(factorial_run.arms))   Out: 24  Now we create a trial including all of these arms, so that we can collect data and evaluate the performance of each.  trial = exp.new_batch_trial(optimize_for_power=True).add_generator_run( factorial_run, multiplier=1 )   By default, the weight of each arm in factorial_run will be 1. However, to optimize for power on the contrasts of k groups against the status quo, the status quo should be sqrt(k) larger than any of the treatment groups. Since we have 24 different arms in our search space, the status quo should be roughly five times larger. That larger weight is automatically set by Ax under the hood if optimize_for_power kwarg is set to True on new batched trial creation.  trial._status_quo_weight_override   Out: 4.795831523312719  ","version":"Next","tagName":"h2"},{"title":"5. Iterate using Thompson Sampling​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#5-iterate-using-thompson-sampling","content":" Next, we run multiple trials (iterations of the experiment) to hone in on the optimal arm(s).  In each iteration, we first collect data about all arms in that trial by callingtrial.run() and trial.mark_complete(). Then we run Thompson Sampling, which assigns a weight to each arm that is proportional to the probability of that arm being the best. Arms whose weight exceed min_weight are added to the next trial, so that we can gather more data on their performance.  models = [] for i in range(4): print(f&quot;Running trial {i+1}...&quot;) trial.run() trial.mark_completed() thompson = Models.THOMPSON(experiment=exp, data=trial.fetch_data(), min_weight=0.01) models.append(thompson) thompson_run = thompson.gen(n=-1) trial = exp.new_batch_trial(optimize_for_power=True).add_generator_run(thompson_run)   Out: Running trial 1... Running trial 2... Running trial 3... Running trial 4...  Out: /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.) /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_81700/2149564789.py:35: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)  ","version":"Next","tagName":"h2"},{"title":"Plot 1: Predicted outcomes for each arm in initial trial​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#plot-1-predicted-outcomes-for-each-arm-in-initial-trial","content":" The plot below shows the mean and standard error for each arm in the first trial. We can see that the standard error for the status quo is the smallest, since this arm was assigned 5x weight.  render(plot_fitted(models[0], metric=&quot;success_metric&quot;, rel=False))   loading...  ","version":"Next","tagName":"h2"},{"title":"Plot 2: Predicted outcomes for arms in last trial​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#plot-2-predicted-outcomes-for-arms-in-last-trial","content":" The following plot below shows the mean and standard error for each arm that made it to the last trial (as well as the status quo, which appears throughout).  render(plot_fitted(models[-1], metric=&quot;success_metric&quot;, rel=False))   loading...  As expected given our evaluation function, arms with higher levels perform better and are given higher weight. Below we see the arms that made it to the final trial.  results = pd.DataFrame( [ {&quot;values&quot;: &quot;,&quot;.join(arm.parameters.values()), &quot;weight&quot;: weight} for arm, weight in trial.normalized_arm_weights().items() ] ) print(results)   Out: values weight 0 level13,level22,level34 0.459022 1 level13,level22,level33 0.126764 2 level11,level21,level31 0.414214  ","version":"Next","tagName":"h2"},{"title":"Plot 3: Rollout Process​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#plot-3-rollout-process","content":" We can also visualize the progression of the experience in the following rollout chart. Each bar represents a trial, and the width of the bands within a bar are proportional to the weight of the arms in that trial.  In the first trial, all arms appear with equal weight, except for the status quo. By the last trial, we have narrowed our focus to only four arms, with arm 0_22 (the arm with the highest levels) having the greatest weight.  from ax.plot.bandit_rollout import plot_bandit_rollout from ax.utils.notebook.plotting import render render(plot_bandit_rollout(exp))   loading...  ","version":"Next","tagName":"h2"},{"title":"Plot 4: Marginal Effects​","type":1,"pageTitle":"Factorial design with empirical Bayes and Thompson Sampling","url":"/Ax/docs/tutorials/factorial/#plot-4-marginal-effects","content":" Finally, we can examine which parameter values had the greatest effect on the overall arm value. As we see in the diagram below, arms whose parameters were assigned the lower level values (such as levell1, levell2, level31 and level32) performed worse than average, whereas arms with higher levels performed better than average.  from ax.plot.marginal_effects import plot_marginal_effects render(plot_marginal_effects(models[0], &quot;success_metric&quot;))   loading... ","version":"Next","tagName":"h2"},{"title":"Generation Strategy (GS) Tutorial","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/generation_strategy/","content":"","keywords":"","version":"Next"},{"title":"1. Quick-start examples​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#1-quick-start-examples","content":" ","version":"Next","tagName":"h2"},{"title":"1A. Manually configured generation strategy​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#1a-manually-configured-generation-strategy","content":" Below is a typical generation strategy used for most single-objective optimization cases in Ax:  gs = GenerationStrategy( steps=[ # 1. Initialization step (does not require pre-existing data and is well-suited for # initial sampling of the search space) GenerationStep( model=Models.SOBOL, num_trials=5, # How many trials should be produced from this generation step min_trials_observed=3, # How many trials need to be completed to move to next model max_parallelism=5, # Max parallelism for this step model_kwargs={&quot;seed&quot;: 999}, # Any kwargs you want passed into the model model_gen_kwargs={}, # Any kwargs you want passed to `modelbridge.gen` ), # 2. Bayesian optimization step (requires data obtained from previous phase and learns # from all data available at the time of each new candidate generation call) GenerationStep( model=Models.BOTORCH_MODULAR, num_trials=-1, # No limitation on how many trials should be produced from this step max_parallelism=3, # Parallelism limit for this step, often lower than for Sobol # More on parallelism vs. required samples in BayesOpt: # https://ax.dev/docs/bayesopt.html#tradeoff-between-parallelism-and-total-number-of-trials ), ] )   ","version":"Next","tagName":"h3"},{"title":"1B. Auto-selected generation strategy​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#1b-auto-selected-generation-strategy","content":" Ax provides achoose_generation_strategyutility, which can auto-select a suitable generation strategy given a search space and an array of other optional settings. The utility is fairly simple at the moment, but additional development (support for multi-objective optimization, multi-fidelity optimization, Bayesian optimization with categorical kernels etc.) is coming soon.  gs = choose_generation_strategy( # Required arguments: search_space=get_branin_search_space(), # Ax `SearchSpace` # Some optional arguments (shown with their defaults), see API docs for more settings: # https://ax.dev/api/modelbridge.html#module-ax.modelbridge.dispatch_utils use_batch_trials=False, # Whether this GS will be used to generate 1-arm `Trial`-s or `BatchTrials` no_bayesian_optimization=False, # Use quasi-random candidate generation without BayesOpt max_parallelism_override=None, # Integer, to which to set the `max_parallelism` setting of all steps in this GS ) gs   Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:02:13] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  Out: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials])  ","version":"Next","tagName":"h3"},{"title":"1C. Candidate generation from a generation strategy​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#1c-candidate-generation-from-a-generation-strategy","content":" While often used through Service or Loop API or other higher-order abstractions like the Ax Scheduler (where the generation strategy is used to fit models and produce candidates from them under-the-hood), it's also possible to use the GS directly, in place of a ModelBridge instance. The interface of GenerationStrategy.gen is the same as ModelBridge.gen.  experiment = get_branin_experiment()   Out: [INFO 09-29 17:02:13] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Note that it's important to specify pending observations to the call to gen to avoid getting the same points re-suggested. Without pending_observations argument, Ax models are not aware of points that should be excluded from generation. Points are considered &quot;pending&quot; when they belong to STAGED, RUNNING, or ABANDONED trials (with the latter included so model does not re-suggest points that are considered &quot;bad&quot; and should not be re-suggested).  If the call to get_pending_obervation_features becomes slow in your setup (since it performs data-fetching etc.), you can opt forget_pending_observation_features_based_on_trial_status (also fromax.modelbridge.modelbridge_utils), but note the limitations of that utility (detailed in its docstring).  generator_run = gs.gen( experiment=experiment, # Ax `Experiment`, for which to generate new candidates data=None, # Ax `Data` to use for model training, optional. n=1, # Number of candidate arms to produce pending_observations=get_pending_observation_features( experiment ), # Points that should not be re-generated # Any other kwargs specified will be passed through to `ModelBridge.gen` along with `GenerationStep.model_gen_kwargs` ) generator_run   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e))  Out: GeneratorRun(1 arms, total weight 1.0)  Then we can add the newly producedGeneratorRun to the experiment as aTrial (or BatchTrial if n &gt; 1):  trial = experiment.new_trial(generator_run) trial   Out: Trial(experiment_name='branin_test_experiment', index=0, status=TrialStatus.CANDIDATE, arm=Arm(name='0_0', parameters={'x1': 0.4080374538898468, 'x2': 2.102140188217163}))  Important notes on GenerationStrategy.gen:  if data argument above is not specified, GS will pull experiment data from cache viaexperiment.lookup_data,without specifying pending_observations, the GS (and any model in Ax) could produce the same candidate over and over, as without that argument the model is not 'aware' that the candidate is part of a RUNNING or ABANDONED trial and should not be re-suggested again.  In cases where get_pending_observation_features is too slow and the experiment consists of 1-arm Trial-s only, it's possible to useget_pending_observation_features_based_on_trial_status instead (found in the same file).  Note that when using the Ax Service API, one of the arguments to AxClient ischoose_generation_strategy_kwargs; specifying that argument is a convenient way to influence the choice of generation strategy in AxClient without manually specifying a full GenerationStrategy.    ","version":"Next","tagName":"h3"},{"title":"2. GenerationStep as a building block of generation strategy​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#2-generationstep-as-a-building-block-of-generation-strategy","content":" ","version":"Next","tagName":"h2"},{"title":"2A. Describing a model to use in a given GenerationStep​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#2a-describing-a-model-to-use-in-a-given-generationstep","content":" There are two ways of specifying a model for a generation step: via an entry in aModels enum or via a 'factory function' –– a callable model constructor (e.g.get_GPEIand other factory functions in the same file). Note that using the latter path, a factory function, will prohibit GenerationStrategy storage and is generally discouraged.  ","version":"Next","tagName":"h3"},{"title":"2B. Other GenerationStep settings​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#2b-other-generationstep-settings","content":" All of the available settings are described in the documentation:  print(GenerationStep.__doc__)   Out: One step in the generation strategy, corresponds to a single model. Describes the model, how many trials will be generated with this model, what minimum number of observations is required to proceed to the next model, etc. NOTE: Model can be specified either from the model registry (ax.modelbridge.registry.Models or using a callable model constructor. Only models from the registry can be saved, and thus optimization can only be resumed if interrupted when using models from the registry. Args: model: A member of Models enum or a callable returning an instance of ModelBridge with an instantiated underlying Model. Refer to ax/modelbridge/factory.py for examples of such callables. num_trials: How many trials to generate with the model from this step. If set to -1, trials will continue to be generated from this model as long as generation_strategy.gen is called (available only for the last of the generation steps). min_trials_observed: How many trials must be completed before the generation strategy can proceed to the next step. Defaults to 0. If num_trials of a given step have been generated but min_trials_ observed have not been completed, a call to generation_strategy.gen will fail with a DataRequiredError. max_parallelism: How many trials generated in the course of this step are allowed to be run (i.e. have trial.status of RUNNING) simultaneously. If max_parallelism trials from this step are already running, a call to generation_strategy.gen will fail with a MaxParallelismReached Exception, indicating that more trials need to be completed before generating and running next trials. use_update: DEPRECATED. enforce_num_trials: Whether to enforce that only num_trials are generated from the given step. If False and num_trials have been generated, but min_trials_observed have not been completed, generation_strategy.gen will continue generating trials from the current step, exceeding num_ trials for it. Allows to avoid DataRequiredError, but delays proceeding to next generation step. model_kwargs: Dictionary of kwargs to pass into the model constructor on instantiation. E.g. if model is Models.SOBOL, kwargs will be applied as Models.SOBOL(**model_kwargs); if model is get_sobol, get_sobol( **model_kwargs). NOTE: if generation strategy is interrupted and resumed from a stored snapshot and its last used model has state saved on its generator runs, model_kwargs is updated with the state dict of the model, retrieved from the last generator run of this generation strategy. model_gen_kwargs: Each call to generation_strategy.gen performs a call to the step's model's gen under the hood; model_gen_kwargs will be passed to the model's gen like so: model.gen(**model_gen_kwargs). completion_criteria: List of TransitionCriterion. All is_met must evaluate True for the GenerationStrategy to move on to the next Step index: Index of this generation step, for use internally in Generation Strategy. Do not assign as it will be reassigned when instantiating GenerationStrategy with a list of its steps. should_deduplicate: Whether to deduplicate the parameters of proposed arms against those of previous arms via rejection sampling. If this is True, the generation strategy will discard generator runs produced from the generation step that has should_deduplicate=True if they contain arms already present on the experiment and replace them with new generator runs. If no generator run with entirely unique arms could be produced in 5 attempts, a GenerationStrategyRepeatedPoints error will be raised, as we assume that the optimization converged when the model can no longer suggest unique arms. model_name: Optional name of the model. If not specified, defaults to the model key of the model spec. Note for developers: by &quot;model&quot; here we really mean an Ax ModelBridge object, which contains an Ax Model under the hood. We call it &quot;model&quot; here to simplify and focus on explaining the logic of GenerationStep and GenerationStrategy.   ","version":"Next","tagName":"h3"},{"title":"2C. Chaining GenerationStep-s together​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#2c-chaining-generationstep-s-together","content":" A GenerationStrategy moves from one step to another when:  N=num_trials generator runs were produced and attached as trials to the experiment ANDM=min_trials_observed have been completed and have data.  Caveat: enforce_num_trials setting:  If enforce_num_trials=True for a given generation step, if 1) is reached but 2) is not yet reached, the generation strategy will raise a DataRequiredError, indicating that more trials need to be completed before the next step.If enforce_num_trials=False, the GS will continue producing generator runs from the current step until 2) is reached.  ","version":"Next","tagName":"h2"},{"title":"2D. max_parallelism enforcement​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#2d-max_parallelism-enforcement","content":" Generation strategy can restrict the number of trials that can be ran simultaneously (to encourage sequential optimization, which benefits Bayesian optimization performance). When the parallelism limit is reached, a call to GenerationStrategy.gen will result in a MaxParallelismReachedException.  The correct way to handle this exception:  Make sure that GenerationStep.max_parallelism is configured correctly for all steps in your generation strategy (to disable it completely, configureGenerationStep.max_parallelism=None),When encountering the exception, wait to produce more generator runs until more trial evluations complete and log the trial completion via trial.mark_completed.    ","version":"Next","tagName":"h2"},{"title":"3. SQL and JSON storage of a generation strategy​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3-sql-and-json-storage-of-a-generation-strategy","content":" When used through Service API or Scheduler, generation strategy will be automatically stored to SQL or JSON via specifying DBSettings to either AxClient or Scheduler(details in respective tutorials in the &quot;Tutorials&quot; page). Generation strategy can also be stored to SQL or JSON individually, as shown below.  More detail on SQL and JSON storage in Ax generally can befound in &quot;Building Blocks of Ax&quot; tutorial.  ","version":"Next","tagName":"h2"},{"title":"3A. SQL storage​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3a-sql-storage","content":" For SQL storage setup in Ax, read through the&quot;Storage&quot; documentation page.  Note that unlike an Ax experiment, a generation strategy does not have a name or another unique identifier. Therefore, a generation strategy is stored in association with experiment and can be retrieved by the associated experiment's name.  from ax.storage.sqa_store.db import ( create_all_tables, get_engine, init_engine_and_session_factory, ) from ax.storage.sqa_store.load import ( load_experiment, load_generation_strategy_by_experiment_name, ) from ax.storage.sqa_store.save import save_experiment, save_generation_strategy init_engine_and_session_factory(url=&quot;sqlite:///foo2.db&quot;) engine = get_engine() create_all_tables(engine) save_experiment(experiment) save_generation_strategy(gs) experiment = load_experiment(experiment_name=experiment.name) gs = load_generation_strategy_by_experiment_name( experiment_name=experiment.name, experiment=experiment, # Can optionally specify experiment object to avoid loading it from database twice ) gs   Out: [INFO 09-29 17:02:14] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:14] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials])  ","version":"Next","tagName":"h3"},{"title":"3B. JSON storage​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3b-json-storage","content":" from ax.storage.json_store.decoder import object_from_json from ax.storage.json_store.encoder import object_to_json gs_json = object_to_json(gs) # Can be written to a file or string via `json.dump` etc. gs = object_from_json( gs_json ) # Decoded back from JSON (can be loaded from file, string via `json.load` etc.) gs   Out: [INFO 09-29 17:02:14] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials])    ","version":"Next","tagName":"h3"},{"title":"3. Advanced considerations​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3-advanced-considerations","content":" Below is a list of important &quot;gotchas&quot; of using generation strategy (especially outside of the higher-level APIs like the Service API or the Scheduler):  ","version":"Next","tagName":"h2"},{"title":"3A. GenerationStrategy.gen produces GeneratorRun-s, not trials​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3a-generationstrategygen-produces-generatorrun-s-not-trials","content":" Since GenerationStrategy.gen mimics ModelBridge.gen and allows for human-in-the-loop usage mode, a call to gen produces a GeneratorRun, which can then be added (or altered before addition or not added at all) to a Trial or BatchTrial on a given experiment. So it's important to add the generator run to a trial, since otherwise it will not be attached to the experiment on its own.  generator_run = gs.gen( experiment=experiment, n=1, pending_observations=get_pending_observation_features(experiment), ) experiment.new_trial(generator_run)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e))  Out: Trial(experiment_name='branin_test_experiment', index=1, status=TrialStatus.CANDIDATE, arm=Arm(name='1_0', parameters={'x1': 8.644633921794593, 'x2': 13.181734532117844}))  ","version":"Next","tagName":"h3"},{"title":"3B. model_kwargs elements that do not define serialization logic in Ax​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3b-model_kwargs-elements-that-do-not-define-serialization-logic-in-ax","content":" Note that passing objects that are not yet serializable in Ax (e.g. a BoTorch Priorobject) as part of GenerationStep.model_kwargs or GenerationStep.model_gen_kwargswill prevent correct generation strategy storage. If this becomes a problem, feel free to open an issue on our Github: https://github.com/facebook/Ax/issues to get help with adding storage support for a given object.  ","version":"Next","tagName":"h3"},{"title":"3C. Why prefer Models enum entries over a factory function?​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3c-why-prefer-models-enum-entries-over-a-factory-function","content":" Storage potential: a call to, for example, Models.GPEI captures all arguments to the model and model bridge and stores them on a generator runs, subsequently produced by the model. Since the capturing logic is part of Models.__call__function, it is not present in a factory function. Furthermore, there is no safe and flexible way to serialize callables in Python.Standardization: While a 'factory function' is by default more flexible (accepts any specified inputs and produces a ModelBridge with an underlying Model instance based on them), it is not standard in terms of its inputs. Models introduces a standardized interface, making it easy to adapt any example to one's specific case.  ","version":"Next","tagName":"h3"},{"title":"3D. How can I request more modeling setups added to Models and natively supported in Ax?​","type":1,"pageTitle":"Generation Strategy (GS) Tutorial","url":"/Ax/docs/tutorials/generation_strategy/#3d-how-can-i-request-more-modeling-setups-added-to-models-and-natively-supported-in-ax","content":" Please open a Github issue to request a new modeling setup in Ax (or for any other questions or requests). ","version":"Next","tagName":"h3"},{"title":"Developer API Example on Hartmann6","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/gpei_hartmann_developer/","content":"","keywords":"","version":"Next"},{"title":"1. Create Search Space​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#1-create-search-space","content":" First, we define a search space, which defines the type and allowed range for the parameters.  from ax.metrics.l2norm import L2NormMetric from ax.metrics.hartmann6 import Hartmann6Metric hartmann_search_space = SearchSpace( parameters=[ RangeParameter( name=f&quot;x{i}&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0 ) for i in range(6) ] )   Note that there are two other parameter classes, FixedParameter and ChoiceParameter. Although we won't use these in this example, you can create them as follows.  choice_param = ChoiceParameter( name=&quot;choice&quot;, values=[&quot;foo&quot;, &quot;bar&quot;], parameter_type=ParameterType.STRING ) fixed_param = FixedParameter( name=&quot;fixed&quot;, value=[True], parameter_type=ParameterType.BOOL )   Out: /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_31777/1805422181.py:1: AxParameterWarning: is_ordered is not specified for ChoiceParameter &quot;choice&quot;. Defaulting to True since there are exactly two choices.. To override this behavior (or avoid this warning), specify is_ordered during ChoiceParameter construction. Note that choice parameters with exactly 2 choices are always considered ordered and that the user-supplied is_ordered has no effect in this particular case. /var/folders/3n/6xxrlwy978b3gbggg9f573bc0000gn/T/ipykernel_31777/1805422181.py:1: AxParameterWarning: sort_values is not specified for ChoiceParameter &quot;choice&quot;. Defaulting to False for parameters of ParameterType STRING. To override this behavior (or avoid this warning), specify sort_values during ChoiceParameter construction.  Sum constraints enforce that the sum of a set of parameters is greater or less than some bound, and order constraints enforce that one parameter is smaller than the other. We won't use these either, but see two examples below.  sum_constraint = SumConstraint( parameters=[ hartmann_search_space.parameters[&quot;x0&quot;], hartmann_search_space.parameters[&quot;x1&quot;], ], is_upper_bound=True, bound=5.0, ) order_constraint = OrderConstraint( lower_parameter=hartmann_search_space.parameters[&quot;x0&quot;], upper_parameter=hartmann_search_space.parameters[&quot;x1&quot;], )   ","version":"Next","tagName":"h2"},{"title":"2. Create Optimization Config​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#2-create-optimization-config","content":" Second, we define the optimization_config with an objective andoutcome_constraints.  When doing the optimization, we will find points that minimize the objective while obeying the constraints (which in this case means l2norm &lt; 1.25).  Note: we are using Hartmann6Metric and L2NormMetric here, which have built in evaluation functions for testing. For creating your own cutom metrics, see8. Defining custom metrics.  param_names = [f&quot;x{i}&quot; for i in range(6)] optimization_config = OptimizationConfig( objective=Objective( metric=Hartmann6Metric(name=&quot;hartmann6&quot;, param_names=param_names), minimize=True, ), outcome_constraints=[ OutcomeConstraint( metric=L2NormMetric(name=&quot;l2norm&quot;, param_names=param_names, noise_sd=0.2), op=ComparisonOp.LEQ, bound=1.25, relative=False, ) ], )   ","version":"Next","tagName":"h2"},{"title":"3. Define a Runner​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#3-define-a-runner","content":" Before an experiment can collect data, it must have a Runner attached. A runner handles the deployment of trials. A trial must be &quot;run&quot; before it can be evaluated.  Here, we have a dummy runner that does nothing. In practice, a runner might be in charge of pushing an experiment to production.  The only method that needs to be defined for runner subclasses is run, which performs any necessary deployment logic, and returns a dictionary of resulting metadata. This metadata can later be accessed through the trial's run_metadata property.  from ax import Runner class MyRunner(Runner): def run(self, trial): trial_metadata = {&quot;name&quot;: str(trial.index)} return trial_metadata   ","version":"Next","tagName":"h2"},{"title":"4. Create Experiment​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#4-create-experiment","content":" Next, we make an Experiment with our search space, runner, and optimization config.  exp = Experiment( name=&quot;test_hartmann&quot;, search_space=hartmann_search_space, optimization_config=optimization_config, runner=MyRunner(), )   ","version":"Next","tagName":"h2"},{"title":"5. Perform Optimization​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#5-perform-optimization","content":" Run the optimization using the settings defined on the experiment. We will create 5 random sobol points for exploration followed by 15 points generated using the GPEI optimizer.  Instead of a member of the Models enum to produce generator runs, users can leverage aGenerationStrategy. See theGeneration Strategy Tutorial for more info.  from ax.modelbridge.registry import Models NUM_SOBOL_TRIALS = 5 NUM_BOTORCH_TRIALS = 15 print(f&quot;Running Sobol initialization trials...&quot;) sobol = Models.SOBOL(search_space=exp.search_space) for i in range(NUM_SOBOL_TRIALS): # Produce a GeneratorRun from the model, which contains proposed arm(s) and other metadata generator_run = sobol.gen(n=1) # Add generator run to a trial to make it part of the experiment and evaluate arm(s) in it trial = exp.new_trial(generator_run=generator_run) # Start trial run to evaluate arm(s) in the trial trial.run() # Mark trial as completed to record when a trial run is completed # and enable fetching of data for metrics on the experiment # (by default, trials must be completed before metrics can fetch their data, # unless a metric is explicitly configured otherwise) trial.mark_completed() for i in range(NUM_BOTORCH_TRIALS): print( f&quot;Running BO trial {i + NUM_SOBOL_TRIALS + 1}/{NUM_SOBOL_TRIALS + NUM_BOTORCH_TRIALS}...&quot; ) # Reinitialize GP+EI model at each step with updated data. gpei = Models.BOTORCH_MODULAR(experiment=exp, data=exp.fetch_data()) generator_run = gpei.gen(n=1) trial = exp.new_trial(generator_run=generator_run) trial.run() trial.mark_completed() print(&quot;Done!&quot;)   Out: Running Sobol initialization trials... Running BO trial 6/20...  Out: Running BO trial 7/20...  Out: Running BO trial 8/20...  Out: Running BO trial 9/20...  Out: Running BO trial 10/20...  Out: Running BO trial 11/20...  Out: Running BO trial 12/20...  Out: Running BO trial 13/20...  Out: Running BO trial 14/20...  Out: Running BO trial 15/20...  Out: Running BO trial 16/20...  Out: Running BO trial 17/20...  Out: Running BO trial 18/20...  Out: Running BO trial 19/20...  Out: Running BO trial 20/20...  Out: Done!  ","version":"Next","tagName":"h2"},{"title":"6. Inspect trials' data​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#6-inspect-trials-data","content":" Now we can inspect the Experiment's data by calling fetch_data(), which retrieves evaluation data for all trials of the experiment.  To fetch trial data, we need to run it and mark it completed. For most metrics in Ax, data is only available once the status of the trial is COMPLETED, since in real-worlds scenarios, metrics can typically only be fetched after the trial finished running.  NOTE: Metrics classes may implement the is_available_while_running method. When this method returns True, data is available when trials are either RUNNING orCOMPLETED. This can be used to obtain intermediate results from A/B test trials and other online experiments, or when metric values are available immediately, like in the case of synthetic problem metrics.  We can also use the fetch_trials_data function to get evaluation data for a specific trials in the experiment, like so:  trial_data = exp.fetch_trials_data([NUM_SOBOL_TRIALS + NUM_BOTORCH_TRIALS - 1]) trial_data.df   \tarm_name\tmetric_name\tmean\tsem\ttrial_index\tn\tfrac_nonnull0\t19_0\thartmann6\t-3.101\t0\t19\t10000\t-3.101 1\t19_0\tl2norm\t1.04789\t0.2\t19\t10000\t1.04789  The below call to exp.fetch_data() also attaches data to the last trial, which because of the way we looped through Botorch trials in5. Perform Optimization, would otherwise not have data attached. This is necessary to get objective_means in7. Plot results.  exp.fetch_data().df   \tarm_name\tmetric_name\tmean\tsem\ttrial_index\tn\tfrac_nonnull0\t0_0\thartmann6\t-0.142823\t0\t0\t10000\t-0.142823 1\t0_0\tl2norm\t1.80554\t0.2\t0\t10000\t1.80554 2\t1_0\thartmann6\t-1.28957\t0\t1\t10000\t-1.28957 3\t1_0\tl2norm\t0.808545\t0.2\t1\t10000\t0.808545 4\t2_0\thartmann6\t-0.13994\t0\t2\t10000\t-0.13994 5\t2_0\tl2norm\t1.86252\t0.2\t2\t10000\t1.86252 6\t3_0\thartmann6\t-0.002181\t0\t3\t10000\t-0.002181 7\t3_0\tl2norm\t1.21284\t0.2\t3\t10000\t1.21284 8\t4_0\thartmann6\t-0.06596\t0\t4\t10000\t-0.06596 9\t4_0\tl2norm\t1.51017\t0.2\t4\t10000\t1.51017 10\t5_0\thartmann6\t-0.348493\t0\t5\t10000\t-0.348493 11\t5_0\tl2norm\t0.843407\t0.2\t5\t10000\t0.843407 12\t6_0\thartmann6\t-1.33134\t0\t6\t10000\t-1.33134 13\t6_0\tl2norm\t0.784633\t0.2\t6\t10000\t0.784633 14\t7_0\thartmann6\t-0.619971\t0\t7\t10000\t-0.619971 15\t7_0\tl2norm\t0.384483\t0.2\t7\t10000\t0.384483 16\t8_0\thartmann6\t-1.79416\t0\t8\t10000\t-1.79416 17\t8_0\tl2norm\t0.817118\t0.2\t8\t10000\t0.817118 18\t9_0\thartmann6\t-1.29368\t0\t9\t10000\t-1.29368 19\t9_0\tl2norm\t0.606522\t0.2\t9\t10000\t0.606522 20\t10_0\thartmann6\t-2.01843\t0\t10\t10000\t-2.01843 21\t10_0\tl2norm\t1.13715\t0.2\t10\t10000\t1.13715 22\t11_0\thartmann6\t-2.51772\t0\t11\t10000\t-2.51772 23\t11_0\tl2norm\t1.12668\t0.2\t11\t10000\t1.12668 24\t12_0\thartmann6\t-1.95409\t0\t12\t10000\t-1.95409 25\t12_0\tl2norm\t1.21392\t0.2\t12\t10000\t1.21392 26\t13_0\thartmann6\t-0.400392\t0\t13\t10000\t-0.400392 27\t13_0\tl2norm\t0.641234\t0.2\t13\t10000\t0.641234 28\t14_0\thartmann6\t-2.84982\t0\t14\t10000\t-2.84982 29\t14_0\tl2norm\t1.21751\t0.2\t14\t10000\t1.21751 30\t15_0\thartmann6\t-1.93163\t0\t15\t10000\t-1.93163 31\t15_0\tl2norm\t0.621023\t0.2\t15\t10000\t0.621023 32\t16_0\thartmann6\t-2.28376\t0\t16\t10000\t-2.28376 33\t16_0\tl2norm\t1.20182\t0.2\t16\t10000\t1.20182 34\t17_0\thartmann6\t-3.25555\t0\t17\t10000\t-3.25555 35\t17_0\tl2norm\t1.0097\t0.2\t17\t10000\t1.0097 36\t18_0\thartmann6\t-2.17728\t0\t18\t10000\t-2.17728 37\t18_0\tl2norm\t0.686095\t0.2\t18\t10000\t0.686095 38\t19_0\thartmann6\t-3.101\t0\t19\t10000\t-3.101 39\t19_0\tl2norm\t0.937838\t0.2\t19\t10000\t0.937838  ","version":"Next","tagName":"h2"},{"title":"7. Plot results​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#7-plot-results","content":" Now we can plot the results of our optimization:  import numpy as np from ax.plot.trace import optimization_trace_single_method # `plot_single_method` expects a 2-d array of means, because it expects to average means from multiple # optimization runs, so we wrap out best objectives array in another array. objective_means = np.array([[trial.objective_mean for trial in exp.trials.values()]]) best_objective_plot = optimization_trace_single_method( y=np.minimum.accumulate(objective_means, axis=1), optimum=-3.32237, # Known minimum objective for Hartmann6 function. ) render(best_objective_plot)   loading...  ","version":"Next","tagName":"h2"},{"title":"8. Defining custom metrics​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#8-defining-custom-metrics","content":" In order to perform an optimization, we also need to define an optimization config for the experiment. An optimization config is composed of an objective metric to be minimized or maximized in the experiment, and optionally a set of outcome constraints that place restrictions on how other metrics can be moved by the experiment.  In order to define an objective or outcome constraint, we first need to subclass Metric. Metrics are used to evaluate trials, which are individual steps of the experiment sequence. Each trial contains one or more arms for which we will collect data at the same time.  Our custom metric(s) will determine how, given a trial, to compute the mean and SEM of each of the trial's arms.  The only method that needs to be defined for most metric subclasses isfetch_trial_data, which defines how a single trial is evaluated, and returns a pandas dataframe.  The is_available_while_running method is optional and returns a boolean, specifying whether the trial data can be fetched before the trial is complete. See6. Inspect trials' data for more details.  from ax import Data import pandas as pd class BoothMetric(Metric): def fetch_trial_data(self, trial): records = [] for arm_name, arm in trial.arms_by_name.items(): params = arm.parameters records.append( { &quot;arm_name&quot;: arm_name, &quot;metric_name&quot;: self.name, &quot;trial_index&quot;: trial.index, # in practice, the mean and sem will be looked up based on trial metadata # but for this tutorial we will calculate them &quot;mean&quot;: (params[&quot;x1&quot;] + 2 * params[&quot;x2&quot;] - 7) ** 2 + (2 * params[&quot;x1&quot;] + params[&quot;x2&quot;] - 5) ** 2, &quot;sem&quot;: 0.0, } ) return Data(df=pd.DataFrame.from_records(records)) def is_available_while_running(self) -&gt; bool: return True   ","version":"Next","tagName":"h2"},{"title":"9. Save to JSON or SQL​","type":1,"pageTitle":"Developer API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_developer/#9-save-to-json-or-sql","content":" At any point, we can also save our experiment to a JSON file. To ensure that our custom metrics and runner are saved properly, we first need to register them.  from ax.storage.registry_bundle import RegistryBundle bundle = RegistryBundle( metric_clss={BoothMetric: None, L2NormMetric: None, Hartmann6Metric: None}, runner_clss={MyRunner: None}, ) from ax.storage.json_store.load import load_experiment from ax.storage.json_store.save import save_experiment save_experiment(exp, &quot;experiment.json&quot;, encoder_registry=bundle.encoder_registry)   loaded_experiment = load_experiment( &quot;experiment.json&quot;, decoder_registry=bundle.decoder_registry )   To save our experiment to SQL, we must first specify a connection to a database and create all necessary tables.  from ax.storage.sqa_store.db import ( init_engine_and_session_factory, get_engine, create_all_tables, ) from ax.storage.sqa_store.load import load_experiment from ax.storage.sqa_store.save import save_experiment init_engine_and_session_factory(url=&quot;sqlite:///foo3.db&quot;) engine = get_engine() create_all_tables(engine)   from ax.storage.sqa_store.sqa_config import SQAConfig exp.name = &quot;new&quot; sqa_config = SQAConfig( json_encoder_registry=bundle.encoder_registry, json_decoder_registry=bundle.decoder_registry, metric_registry=bundle.metric_registry, runner_registry=bundle.runner_registry, ) save_experiment(exp, config=sqa_config)   load_experiment(exp.name, config=sqa_config)   Out: Experiment(new)   ","version":"Next","tagName":"h2"},{"title":"Loop API Example on Hartmann6","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/gpei_hartmann_loop/","content":"","keywords":"","version":"Next"},{"title":"1. Define evaluation function​","type":1,"pageTitle":"Loop API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_loop/#1-define-evaluation-function","content":" First, we define an evaluation function that is able to compute all the metrics needed for this experiment. This function needs to accept a set of parameter values and can also accept a weight. It should produce a dictionary of metric names to tuples of mean and standard error for those metrics.  def hartmann_evaluation_function(parameterization): x = np.array([parameterization.get(f&quot;x{i+1}&quot;) for i in range(6)]) # In our case, standard error is 0, since we are computing a synthetic function. return {&quot;hartmann6&quot;: (hartmann6(x), 0.0), &quot;l2norm&quot;: (np.sqrt((x**2).sum()), 0.0)}   If there is only one metric in the experiment – the objective – then evaluation function can return a single tuple of mean and SEM, in which case Ax will assume that evaluation corresponds to the objective. It can also return only the mean as a float, in which case Ax will treat SEM as unknown and use a model that can infer it. For more details on evaluation function, refer to the &quot;Trial Evaluation&quot; section in the docs.  ","version":"Next","tagName":"h2"},{"title":"2. Run optimization​","type":1,"pageTitle":"Loop API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_loop/#2-run-optimization","content":" The setup for the loop is fully compatible with JSON. The optimization algorithm is selected based on the properties of the problem search space.  best_parameters, values, experiment, model = optimize( parameters=[ { &quot;name&quot;: &quot;x1&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], &quot;value_type&quot;: &quot;float&quot;, # Optional, defaults to inference from type of &quot;bounds&quot;. &quot;log_scale&quot;: False, # Optional, defaults to False. }, { &quot;name&quot;: &quot;x2&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x3&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x4&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x5&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x6&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, ], experiment_name=&quot;test&quot;, objective_name=&quot;hartmann6&quot;, evaluation_function=hartmann_evaluation_function, minimize=True, # Optional, defaults to False. parameter_constraints=[&quot;x1 + x2 &lt;= 20&quot;], # Optional. outcome_constraints=[&quot;l2norm &lt;= 1.25&quot;], # Optional. total_trials=30, # Optional. )   Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x3. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x4. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x5. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x6. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:36] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[ParameterConstraint(1.0*x1 + 1.0*x2 &lt;= 20.0)]).  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 16:58:36] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.  Out: [INFO 09-29 16:58:36] ax.service.managed_loop: Started full optimization with 30 steps.  Out: [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 1...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 2...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 3...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 4...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 5...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 6...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 7...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 8...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 9...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 10...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 11...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 12...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:58:36] ax.service.managed_loop: Running optimization trial 13...  Out: [INFO 09-29 16:58:43] ax.service.managed_loop: Running optimization trial 14...  Out: [INFO 09-29 16:58:48] ax.service.managed_loop: Running optimization trial 15...  Out: [INFO 09-29 16:58:51] ax.service.managed_loop: Running optimization trial 16...  Out: [INFO 09-29 16:58:54] ax.service.managed_loop: Running optimization trial 17...  Out: [INFO 09-29 16:58:58] ax.service.managed_loop: Running optimization trial 18...  Out: [INFO 09-29 16:59:02] ax.service.managed_loop: Running optimization trial 19...  Out: [INFO 09-29 16:59:07] ax.service.managed_loop: Running optimization trial 20...  Out: [INFO 09-29 16:59:12] ax.service.managed_loop: Running optimization trial 21...  Out: [INFO 09-29 16:59:17] ax.service.managed_loop: Running optimization trial 22...  Out: [INFO 09-29 16:59:22] ax.service.managed_loop: Running optimization trial 23...  Out: [INFO 09-29 16:59:26] ax.service.managed_loop: Running optimization trial 24...  Out: [INFO 09-29 16:59:31] ax.service.managed_loop: Running optimization trial 25...  Out: [INFO 09-29 16:59:35] ax.service.managed_loop: Running optimization trial 26...  Out: [INFO 09-29 16:59:41] ax.service.managed_loop: Running optimization trial 27...  Out: [INFO 09-29 16:59:46] ax.service.managed_loop: Running optimization trial 28...  Out: [INFO 09-29 16:59:51] ax.service.managed_loop: Running optimization trial 29...  Out: [INFO 09-29 16:59:56] ax.service.managed_loop: Running optimization trial 30...  And we can introspect optimization results:  best_parameters   Out: {'x1': 0.0, 'x2': 0.11351079280870778, 'x3': 0.5404433884259263, 'x4': 0.2787959342134476, 'x5': 0.31995048665920317, 'x6': 0.633881309942032}  means, covariances = values means   Out: {'hartmann6': -2.7785287215763312, 'l2norm': 0.9417591992925056}  For comparison, minimum of Hartmann6 is:  hartmann6.fmin   Out: -3.32237  ","version":"Next","tagName":"h2"},{"title":"3. Plot results​","type":1,"pageTitle":"Loop API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_loop/#3-plot-results","content":" Here we arbitrarily select &quot;x1&quot; and &quot;x2&quot; as the two parameters to plot for both metrics, &quot;hartmann6&quot; and &quot;l2norm&quot;.  render(plot_contour(model=model, param_x=&quot;x1&quot;, param_y=&quot;x2&quot;, metric_name=&quot;hartmann6&quot;))   loading...  render(plot_contour(model=model, param_x=&quot;x1&quot;, param_y=&quot;x2&quot;, metric_name=&quot;l2norm&quot;))   loading...  We also plot optimization trace, which shows best hartmann6 objective value seen by each iteration of the optimization:  # `plot_single_method` expects a 2-d array of means, because it expects to average means from multiple # optimization runs, so we wrap out best objectives array in another array. best_objectives = np.array( [[trial.objective_mean for trial in experiment.trials.values()]] ) best_objective_plot = optimization_trace_single_method( y=np.minimum.accumulate(best_objectives, axis=1), optimum=hartmann6.fmin, title=&quot;Model performance vs. # of iterations&quot;, ylabel=&quot;Hartmann6&quot;, ) render(best_objective_plot)   loading... ","version":"Next","tagName":"h2"},{"title":"1. What happens without global stopping? Optimization can run for too long.","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/gss/","content":"Open in GitHubRun in Google Colab This tutorial illustrates use of a Global Stopping Strategy (GSS) in combination with the Service API. For background on the Service API, see the Service API Tutorial:https://ax.dev/tutorials/gpei_hartmann_service.html GSS is also supported in the Scheduler API, where it can be provided as part of SchedulerOptions. For more onScheduler, see the Scheduler tutorial: https://ax.dev/tutorials/scheduler.html Global Stopping stops an optimization loop when some data-based criteria are met which suggest that future trials will not be very helpful. For example, we might stop when there has been very little improvement in the last five trials. This is as opposed to trial-level early stopping, which monitors the results of expensive evaluations and terminates those that are unlikely to produce promising results, freeing resources to explore more promising configurations. For more on trial-level early stopping, see the tutorial: https://ax.dev/tutorials/early_stopping/early_stopping.html import numpy as np from ax.service.ax_client import AxClient, ObjectiveProperties from ax.utils.measurement.synthetic_functions import Branin, branin from ax.utils.notebook.plotting import init_notebook_plotting, render init_notebook_plotting() Out: [INFO 09-30 17:07:19] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell. Out: [INFO 09-30 17:07:19] ax.utils.notebook.plotting: Please see (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering) if visualizations are not rendering. 1. What happens without global stopping? Optimization can run for too long. This example uses the Branin test problem. We run 25 trials, which turns out to be far more than needed, because we get close to the optimum quite quickly. def evaluate(parameters): x = np.array([parameters.get(f&quot;x{i+1}&quot;) for i in range(2)]) return {&quot;branin&quot;: (branin(x), 0.0)} params = [ { &quot;name&quot;: f&quot;x{i + 1}&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [*Branin._domain[i]], &quot;value_type&quot;: &quot;float&quot;, &quot;log_scale&quot;: False, } for i in range(2) ] ax_client = AxClient(random_seed=0, verbose_logging=False) ax_client.create_experiment( name=&quot;branin_test_experiment&quot;, parameters=params, objectives={&quot;branin&quot;: ObjectiveProperties(minimize=True)}, is_test=True, ) Out: [WARNING 09-30 17:07:19] ax.service.ax_client: Random seed set to 0. Note that this setting only affects the Sobol quasi-random generator and BoTorch-powered Bayesian optimization models. For the latter models, setting random seed to the same number for two optimizations will make the generated trials similar, but not exactly the same, and over time the trials will diverge more. Out: [INFO 09-30 17:07:19] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]). Out: [INFO 09-30 17:07:19] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters. Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5 Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5 Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments. Out: [INFO 09-30 17:07:19] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting. %%time for i in range(25): parameters, trial_index = ax_client.get_next_trial() # Local evaluation here can be replaced with deployment to external system. ax_client.complete_trial( trial_index=trial_index, raw_data=evaluate(parameters) ) Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. Out: CPU times: user 1min 7s, sys: 11.8 s, total: 1min 19s Wall time: 14.4 s render(ax_client.get_optimization_trace()) loading... 2. Optimization with global stopping, with the Service API Rather than running a fixed number of trials, we can use a GlobalStoppingStrategy (GSS), which checks whether some stopping criteria have been met when get_next_trial is called. Here, we use an ImprovementGlobalStoppingStrategy, which checks whether the the last window_size trials have improved by more than some threshold amount. For single-objective optimization, which we are doing here,ImprovementGlobalStoppingStrategy checks if an improvement is &quot;significant&quot; by comparing it to the inter-quartile range (IQR) of the objective values attained so far. ImprovementGlobalStoppingStrategy also supports multi-objective optimization (MOO), in which case it checks whether the percentage improvement in hypervolume over the lastwindow_size trials exceeds improvement_bar. from ax.global_stopping.strategies.improvement import ImprovementGlobalStoppingStrategy from ax.exceptions.core import OptimizationShouldStop # Start considering stopping only after the 5 initialization trials + 5 GPEI trials. # Stop if the improvement in the best point in the past 5 trials is less than # 1% of the IQR thus far. stopping_strategy = ImprovementGlobalStoppingStrategy( min_trials=5 + 5, window_size=5, improvement_bar=0.01 ) ax_client_gss = AxClient( global_stopping_strategy=stopping_strategy, random_seed=0, verbose_logging=False ) ax_client_gss.create_experiment( name=&quot;branin_test_experiment&quot;, parameters=params, objectives={&quot;branin&quot;: ObjectiveProperties(minimize=True)}, is_test=True, ) Out: [WARNING 09-30 17:07:35] ax.service.ax_client: Random seed set to 0. Note that this setting only affects the Sobol quasi-random generator and BoTorch-powered Bayesian optimization models. For the latter models, setting random seed to the same number for two optimizations will make the generated trials similar, but not exactly the same, and over time the trials will diverge more. Out: [INFO 09-30 17:07:35] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]). Out: [INFO 09-30 17:07:35] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters. Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5 Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5 Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments. Out: [INFO 09-30 17:07:35] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting. If there has not been much improvement, ImprovementGlobalStoppingStrategy will raise an exception. If the exception is raised, we catch it and terminate optimization. for i in range(25): try: parameters, trial_index = ax_client_gss.get_next_trial() except OptimizationShouldStop as exc: print(exc.message) break ax_client_gss.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters)) Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. Out: The improvement in best objective in the past 5 trials (=0.000) is less than 0.01 times the interquartile range (IQR) of objectives attained so far (IQR=34.894). render(ax_client_gss.get_optimization_trace()) loading... 3. Write your own custom Global Stopping Strategy You can write a custom Global Stopping Strategy by subclassingBaseGlobalStoppingStrategy and use it where ImprovementGlobalStoppingStrategy was used above. from ax.global_stopping.strategies.base import BaseGlobalStoppingStrategy from typing import Tuple from ax.core.experiment import Experiment from ax.core.base_trial import TrialStatus from ax.global_stopping.strategies.improvement import constraint_satisfaction Here, we define SimpleThresholdGlobalStoppingStrategy, which stops when we observe a point better than a provided threshold. This can be useful when there is a known optimum. For example, the Branin function has an optimum of zero. When the optimum is not known, this can still be useful from a satisficing perspective: For example, maybe we need a model to take up less than a certain amount of RAM so it doesn't crash our usual hardware, but there is no benefit to further improvements. class SimpleThresholdGlobalStoppingStrategy(BaseGlobalStoppingStrategy): &quot;&quot;&quot; A GSS that stops when we observe a point better than `threshold`. &quot;&quot;&quot; def __init__( self, min_trials: int, inactive_when_pending_trials: bool = True, threshold: float = 0.1 ): self.threshold = threshold super().__init__( min_trials=min_trials, inactive_when_pending_trials=inactive_when_pending_trials ) def _should_stop_optimization( self, experiment: Experiment ) -&gt; Tuple[bool, str]: &quot;&quot;&quot; Check if the best seen is better than `self.threshold`. &quot;&quot;&quot; feasible_objectives = [ trial.objective_mean for trial in experiment.trials_by_status[TrialStatus.COMPLETED] if constraint_satisfaction(trial) ] # Computing the interquartile for scaling the difference if len(feasible_objectives) &lt;= 1: message = &quot;There are not enough feasible arms tried yet.&quot; return False, message minimize = experiment.optimization_config.objective.minimize if minimize: best = np.min(feasible_objectives) stop = best &lt; self.threshold else: best = np.max(feasible_objectives) stop = best &gt; self.threshold comparison = &quot;less&quot; if minimize else &quot;greater&quot; if stop: message = ( f&quot;The best objective seen is {best:.3f}, which is {comparison} &quot; f&quot;than the threshold of {self.threshold:.3f}.&quot; ) else: message = &quot;&quot; return stop, message stopping_strategy = SimpleThresholdGlobalStoppingStrategy(min_trials=5, threshold=1.) ax_client_custom_gss = AxClient( global_stopping_strategy=stopping_strategy, random_seed=0, verbose_logging=False, ) ax_client_custom_gss.create_experiment( name=&quot;branin_test_experiment&quot;, parameters=params, objectives={&quot;branin&quot;: ObjectiveProperties(minimize=True)}, is_test=True, ) Out: [WARNING 09-30 17:07:40] ax.service.ax_client: Random seed set to 0. Note that this setting only affects the Sobol quasi-random generator and BoTorch-powered Bayesian optimization models. For the latter models, setting random seed to the same number for two optimizations will make the generated trials similar, but not exactly the same, and over time the trials will diverge more. Out: [INFO 09-30 17:07:40] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]). Out: [INFO 09-30 17:07:40] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters. Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5 Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5 Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments. Out: [INFO 09-30 17:07:40] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting. for i in range(25): try: parameters, trial_index = ax_client_custom_gss.get_next_trial() except OptimizationShouldStop as exc: print(exc.message) break ax_client_custom_gss.complete_trial( trial_index=trial_index, raw_data=evaluate(parameters) ) Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. Out: The best objective seen is 0.401, which is less than the threshold of 1.000. render(ax_client_custom_gss.get_optimization_trace()) loading...","keywords":"","version":"Next"},{"title":"Trial-Level Early Stopping","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/early_stopping/","content":"","keywords":"","version":"Next"},{"title":"Trial-level early stopping in Ax​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#trial-level-early-stopping-in-ax","content":" This tutorial illustrates how to add a trial-level early stopping strategy to an Ax hyper-parameter optimization (HPO) loop. The goal of trial-level early stopping is to monitor the results of expensive evaluations and terminate those that are unlikely to produce promising results, freeing up resources to explore more configurations.  Most of this tutorial is adapted from thePyTorch Ax Multiobjective NAS Tutorial. The training job is different from the original in that we do not optimize batch_sizeor epochs. This was done for illustrative purposes, as each validation curve now has the same number of points. The companion training file mnist_train_nas.py has also been altered to log to Tensorboard during training.  NOTE: Although the original NAS tutorial is for a multi-objective problem, this tutorial focuses on a single objective (validation accuracy) problem. Early stopping currently does not support &quot;true&quot; multi-objective stopping, although one can uselogical compositions of early stopping strategiesto target multiple objectives separately. Early stopping for the multi-objective case is currently a work in progress.  import os import tempfile from pathlib import Path import torchx from ax.core import Experiment, Objective, ParameterType, RangeParameter, SearchSpace from ax.core.optimization_config import OptimizationConfig from ax.early_stopping.strategies import PercentileEarlyStoppingStrategy from ax.metrics.tensorboard import TensorboardMetric from ax.modelbridge.dispatch_utils import choose_generation_strategy from ax.runners.torchx import TorchXRunner from ax.service.scheduler import Scheduler, SchedulerOptions from ax.service.utils.report_utils import exp_to_df from tensorboard.backend.event_processing import plugin_event_multiplexer as event_multiplexer from torchx import specs from torchx.components import utils from matplotlib import pyplot as plt %matplotlib inline   SMOKE_TEST = os.environ.get(&quot;SMOKE_TEST&quot;)   ","version":"Next","tagName":"h2"},{"title":"Defining the TorchX App​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#defining-the-torchx-app","content":" Our goal is to optimize the PyTorch Lightning training job defined inmnist_train_nas.py. To do this using TorchX, we write a helper function that takes in the values of the architcture and hyperparameters of the training job and creates aTorchX AppDef with the appropriate settings.  if SMOKE_TEST: epochs = 3 else: epochs = 10   def trainer( log_path: str, hidden_size_1: int, hidden_size_2: int, learning_rate: float, dropout: float, trial_idx: int = -1, ) -&gt; specs.AppDef: # define the log path so we can pass it to the TorchX AppDef if trial_idx &gt;= 0: log_path = Path(log_path).joinpath(str(trial_idx)).absolute().as_posix() batch_size = 32 return utils.python( # command line args to the training script &quot;--log_path&quot;, log_path, &quot;--hidden_size_1&quot;, str(hidden_size_1), &quot;--hidden_size_2&quot;, str(hidden_size_2), &quot;--learning_rate&quot;, str(learning_rate), &quot;--epochs&quot;, str(epochs), &quot;--dropout&quot;, str(dropout), &quot;--batch_size&quot;, str(batch_size), # other config options name=&quot;trainer&quot;, script=&quot;tutorials/early_stopping/mnist_train_nas.py&quot;, image=torchx.version.TORCHX_IMAGE, )   ","version":"Next","tagName":"h2"},{"title":"Setting up the Runner​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#setting-up-the-runner","content":" Ax’s Runner abstraction allows writing interfaces to various backends. Ax already comes with Runner for TorchX, so we just need to configure it. For the purpose of this tutorial, we run jobs locally in a fully asynchronous fashion. In order to launch them on a cluster, you can instead specify a different TorchX scheduler and adjust the configuration appropriately. For example, if you have a Kubernetes cluster, you just need to change the scheduler fromlocal_cwd to kubernetes.  The training job launched by this runner will log partial results to Tensorboard, which will then be monitored by the early stopping strategy. We will show how this is done using an AxTensorboardMetricbelow.  # Make a temporary dir to log our results into log_dir = tempfile.mkdtemp() ax_runner = TorchXRunner( tracker_base=&quot;/tmp/&quot;, component=trainer, # NOTE: To launch this job on a cluster instead of locally you can # specify a different scheduler and adjust args appropriately. scheduler=&quot;local_cwd&quot;, component_const_params={&quot;log_path&quot;: log_dir}, cfg={}, )   ","version":"Next","tagName":"h2"},{"title":"Setting up the SearchSpace​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#setting-up-the-searchspace","content":" First, we define our search space. Ax supports both range parameters of type integer and float as well as choice parameters which can have non-numerical types such as strings. We will tune the hidden sizes, learning rate, and dropout parameters.  parameters = [ # NOTE: In a real-world setting, hidden_size_1 and hidden_size_2 # should probably be powers of 2, but in our simple example this # would mean that num_params can't take on that many values, which # in turn makes the Pareto frontier look pretty weird. RangeParameter( name=&quot;hidden_size_1&quot;, lower=16, upper=128, parameter_type=ParameterType.INT, log_scale=True, ), RangeParameter( name=&quot;hidden_size_2&quot;, lower=16, upper=128, parameter_type=ParameterType.INT, log_scale=True, ), RangeParameter( name=&quot;learning_rate&quot;, lower=1e-4, upper=1e-2, parameter_type=ParameterType.FLOAT, log_scale=True, ), RangeParameter( name=&quot;dropout&quot;, lower=0.0, upper=0.5, parameter_type=ParameterType.FLOAT, ), ] search_space = SearchSpace( parameters=parameters, # NOTE: In practice, it may make sense to add a constraint # hidden_size_2 &lt;= hidden_size_1 parameter_constraints=[], )   ","version":"Next","tagName":"h2"},{"title":"Setting up Metrics​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#setting-up-metrics","content":" Ax has the concept of a Metric that defines properties of outcomes and how observations are obtained for these outcomes. This allows e.g. encodig how data is fetched from some distributed execution backend and post-processed before being passed as input to Ax.  We will optimize the validation accuracy, which is a TensorboardMetric that points to the logging directory assigned above. Note that we have setis_available_while_running, allowing for the metric to be queried as the trial progresses. This is critical for the early stopping strategy to monitor partial results.  class MyTensorboardMetric(TensorboardMetric): # NOTE: We need to tell the new Tensorboard metric how to get the id / # file handle for the tensorboard logs from a trial. In this case # our convention is to just save a separate file per trial in # the pre-specified log dir. def _get_event_multiplexer_for_trial(self, trial): mul = event_multiplexer.EventMultiplexer(max_reload_threads=20) mul.AddRunsFromDirectory(Path(log_dir).joinpath(str(trial.index)).as_posix(), None) mul.Reload() return mul # This indicates whether the metric is queryable while the trial is # still running. This is required for early stopping to monitor the # progress of the running trial.ArithmeticError @classmethod def is_available_while_running(cls): return True   val_acc = MyTensorboardMetric( name=&quot;val_acc&quot;, tag=&quot;val_acc&quot;, lower_is_better=False, )   ","version":"Next","tagName":"h2"},{"title":"Setting up the OptimizationConfig​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#setting-up-the-optimizationconfig","content":" The OptimizationConfig specifies the objective for Ax to optimize.  opt_config = OptimizationConfig( objective=Objective( metric=val_acc, minimize=False, ) )   ","version":"Next","tagName":"h2"},{"title":"Defining an Early Stopping Strategy​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#defining-an-early-stopping-strategy","content":" A PercentileEarlyStoppingStrategy is a simple method that stops a trial if its performance falls below a certain percentile of other trials at the same step (e.g., when percentile_threshold is 50, at a given point in time, if a trial ranks in the bottom 50% of trials, it is stopped).  We make use of normalize_progressions which normalizes the progression column (e.g. timestamp, epochs, training data used) to be in [0, 1]. This is useful because one doesn't need to know the maximum progression values of the curve (which might be, e.g., the total number of data points in the training dataset).The min_progression parameter specifies that trials should only be considered for stopping if the latest progression value is greater than this threshold.The min_curves parameter specifies the minimum number of completed curves (i.e., fully completed training jobs) before early stopping will be considered. This should be larger than zero if normalize_progression is used. In general, we want a few completed curves to have a baseline for comparison.  Note that PercentileEarlyStoppingStrategy does not make use of learning curve modeling or prediction. More sophisticated model-based methods will be available in future versions of Ax.  percentile_early_stopping_strategy = PercentileEarlyStoppingStrategy( # stop if in bottom 70% of runs at the same progression percentile_threshold=70, # the trial must have passed `min_progression` steps before early stopping is initiated # note that we are using `normalize_progressions`, so this is on a scale of [0, 1] min_progression=0.3, # there must be `min_curves` completed trials and `min_curves` trials reporting data in # order for early stopping to be applicable min_curves=5, # specify, e.g., [0, 1] if the first two trials should never be stopped trial_indices_to_ignore=None, # check for new data every 10 seconds seconds_between_polls=10, normalize_progressions=True, )   ","version":"Next","tagName":"h2"},{"title":"Creating the Ax Experiment​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#creating-the-ax-experiment","content":" In Ax, the Experiment object is the object that stores all the information about the problem setup.  experiment = Experiment( name=&quot;torchx_mnist&quot;, search_space=search_space, optimization_config=opt_config, runner=ax_runner, )   ","version":"Next","tagName":"h2"},{"title":"Choosing the GenerationStrategy​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#choosing-the-generationstrategy","content":" AGenerationStrategyis the abstract representation of how we would like to perform the optimization. While this can be customized (if you’d like to do so, seethis tutorial), in most cases Ax can automatically determine an appropriate strategy based on the search space, optimization config, and the total number of trials we want to run.  Typically, Ax chooses to evaluate a number of random configurations before starting a model-based Bayesian Optimization strategy.  We remark that in Ax, generation strategies and early stopping strategies are separate, a design decision motivated by ease-of-use. However, we should acknowledge that jointly considering generation and stopping using a single strategy would likely be the &quot;proper&quot; formulation.  if SMOKE_TEST: total_trials = 6 else: total_trials = 15 # total evaluation budget gs = choose_generation_strategy( search_space=experiment.search_space, optimization_config=experiment.optimization_config, num_trials=total_trials, )   Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=15 use_batch_trials=False  Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-30 16:59:48] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  ","version":"Next","tagName":"h2"},{"title":"Configuring the Scheduler​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#configuring-the-scheduler","content":" The Scheduler acts as the loop control for the optimization. It communicates with the backend to launch trials, check their status, retrieve (partial) results, and importantly for this tutorial, calls the early stopping strategy. If the early stopping strategy suggests a trial to be the stopped, the Scheduler communicates with the backend to terminate the trial.  The Scheduler requires the Experiment and the GenerationStrategy. A set of options can be passed in via SchedulerOptions. Here, we configure the number of total evaluations as well as max_pending_trials, the maximum number of trials that should run concurrently. In our local setting, this is the number of training jobs running as individual processes, while in a remote execution setting, this would be the number of machines you want to use in parallel.  scheduler = Scheduler( experiment=experiment, generation_strategy=gs, options=SchedulerOptions( total_trials=total_trials, max_pending_trials=5, early_stopping_strategy=percentile_early_stopping_strategy, ), )   Out: [INFO 09-30 16:59:48] Scheduler: Scheduler requires experiment to have immutable search space and optimization config. Setting property immutable_search_space_and_opt_config to True on experiment.  Execution using papermill encountered an exception here and stopped:  %%time scheduler.run_all_trials()   Out: [INFO 09-30 16:59:48] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 16:59:48] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 16:59:48] Scheduler: Running trials [0]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 16:59:49] Scheduler: Running trials [1]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 16:59:50] Scheduler: Running trials [2]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 16:59:51] Scheduler: Running trials [3]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 16:59:52] Scheduler: Running trials [4]...  Out: [WARNING 09-30 16:59:53] Scheduler: Both init_seconds_between_polls and early_stopping_strategy supplied. init_seconds_between_polls=1 will be overrridden by early_stopping_strategy.seconds_between_polls=10 and polling will take place at a constant rate.  Out: [INFO 09-30 16:59:53] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 16:59:53] Scheduler: Fetching data for trials: 0 - 4 because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 16:59:53] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;)  Out: [INFO 09-30 16:59:53] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;)  Out: [INFO 09-30 16:59:53] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;)  Out: [INFO 09-30 16:59:53] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;)  Out: [INFO 09-30 16:59:53] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;)  Out: [ERROR 09-30 16:59:53] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 16:59:53] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 16:59:53] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 16:59:53] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 16:59:53] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 16:59:53] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;).  Out: [INFO 09-30 16:59:53] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 16:59:53] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;).  Out: [INFO 09-30 16:59:53] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 16:59:53] Scheduler: Failed to fetch val_acc for trial 2, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;).  Out: [INFO 09-30 16:59:53] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 2 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 16:59:53] Scheduler: Failed to fetch val_acc for trial 3, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;).  Out: [INFO 09-30 16:59:53] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 3 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 16:59:53] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd01850&gt;&quot;).  Out: [INFO 09-30 16:59:53] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [INFO 09-30 16:59:53] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: [INFO 09-30 16:59:53] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:03] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:03] Scheduler: Fetching data for trials: [0, 1, 3, 4] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:03] Scheduler: Retrieved FAILED trials: [2].  Out: [INFO 09-30 17:00:03] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd44fe0&gt;&quot;)  Out: [INFO 09-30 17:00:03] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;)  Out: [INFO 09-30 17:00:03] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;)  Out: [INFO 09-30 17:00:03] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;)  Out: [ERROR 09-30 17:00:03] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd44fe0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:03] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:03] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:03] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 17:00:03] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd44fe0&gt;&quot;).  Out: [INFO 09-30 17:00:03] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:03] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;).  Out: [INFO 09-30 17:00:03] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:03] Scheduler: Failed to fetch val_acc for trial 3, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;).  Out: [INFO 09-30 17:00:03] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 3 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:03] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30f74d460&gt;&quot;).  Out: [INFO 09-30 17:00:03] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [INFO 09-30 17:00:03] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:00:03] Scheduler: Running trials [5]...  Out: [WARNING 09-30 17:00:04] Scheduler: Both init_seconds_between_polls and early_stopping_strategy supplied. init_seconds_between_polls=1 will be overrridden by early_stopping_strategy.seconds_between_polls=10 and polling will take place at a constant rate.  Out: [INFO 09-30 17:00:04] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:04] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:04] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;)  Out: [INFO 09-30 17:00:04] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;)  Out: [INFO 09-30 17:00:04] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;)  Out: [INFO 09-30 17:00:04] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;)  Out: [INFO 09-30 17:00:04] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350de80&gt;&quot;)  Out: [ERROR 09-30 17:00:04] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:04] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:04] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:04] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:04] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350de80&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 17:00:04] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;).  Out: [INFO 09-30 17:00:04] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:04] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;).  Out: [INFO 09-30 17:00:04] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:04] Scheduler: Failed to fetch val_acc for trial 3, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;).  Out: [INFO 09-30 17:00:04] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 3 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:04] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd7b680&gt;&quot;).  Out: [INFO 09-30 17:00:04] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:04] Scheduler: Failed to fetch val_acc for trial 5, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350de80&gt;&quot;).  Out: [INFO 09-30 17:00:04] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 5 is still RUNNING continuing the experiment and retrying on next poll...  Out: [INFO 09-30 17:00:04] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: [INFO 09-30 17:00:04] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:14] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:14] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:14] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46060&gt;&quot;)  Out: [INFO 09-30 17:00:14] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;)  Out: [INFO 09-30 17:00:14] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;)  Out: [INFO 09-30 17:00:14] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;)  Out: [INFO 09-30 17:00:14] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;)  Out: [ERROR 09-30 17:00:14] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46060&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:14] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:14] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:14] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:14] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 17:00:14] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46060&gt;&quot;).  Out: [INFO 09-30 17:00:14] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:14] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;).  Out: [INFO 09-30 17:00:14] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:14] Scheduler: Failed to fetch val_acc for trial 3, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;).  Out: [INFO 09-30 17:00:14] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 3 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:14] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;).  Out: [INFO 09-30 17:00:14] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:14] Scheduler: Failed to fetch val_acc for trial 5, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fc7eea0&gt;&quot;).  Out: [INFO 09-30 17:00:14] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 5 is still RUNNING continuing the experiment and retrying on next poll...  Out: [INFO 09-30 17:00:14] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: [INFO 09-30 17:00:14] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:24] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:24] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350d7c0&gt;&quot;)  Out: [INFO 09-30 17:00:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;)  Out: [INFO 09-30 17:00:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;)  Out: [INFO 09-30 17:00:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;)  Out: [INFO 09-30 17:00:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;)  Out: [ERROR 09-30 17:00:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350d7c0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 17:00:25] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x12350d7c0&gt;&quot;).  Out: [INFO 09-30 17:00:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:25] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;).  Out: [INFO 09-30 17:00:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:25] Scheduler: Failed to fetch val_acc for trial 3, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd474d0&gt;&quot;).  Out: [INFO 09-30 17:00:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 3 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:25] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;).  Out: [INFO 09-30 17:00:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:25] Scheduler: Failed to fetch val_acc for trial 5, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa47230&gt;&quot;).  Out: [INFO 09-30 17:00:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 5 is still RUNNING continuing the experiment and retrying on next poll...  Out: [INFO 09-30 17:00:25] ax.early_stopping.strategies.base: PercentileEarlyStoppingStrategy received empty data. Not stopping any trials.  Out: [INFO 09-30 17:00:25] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:35] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:35] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:35] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:00:35] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:00:35] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46ff0&gt;&quot;)  Out: [ERROR 09-30 17:00:35] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:35] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:35] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46ff0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:00:35] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:35] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:35] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:35] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:35] Scheduler: Failed to fetch val_acc for trial 5, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fd46ff0&gt;&quot;).  Out: [INFO 09-30 17:00:35] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 5 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:00:35] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:00:35] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:45] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:45] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:45] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:00:45] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:00:45] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:45] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:00:45] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:45] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:45] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:45] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:00:45] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:00:45] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:00:55] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:00:55] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:00:55] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:00:55] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:00:55] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:00:55] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:00:55] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:55] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:00:55] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:00:55] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:00:55] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:00:55] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:05] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:05] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:05] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:05] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:05] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:05] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:05] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:05] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:05] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:05] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:05] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:05] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:15] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:15] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:15] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:15] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:15] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:15] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:15] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:15] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:15] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:15] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:15] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:15] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:25] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:25] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:25] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:25] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:25] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:25] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:25] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:25] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:25] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:35] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:35] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:35] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:35] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:36] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:36] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:36] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:36] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:36] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:36] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:36] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:36] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:36] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:36] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:36] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:36] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:46] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:46] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:46] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:46] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:46] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:46] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:46] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:46] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:46] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:46] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:46] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:46] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:46] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:46] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:46] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:46] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:01:56] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:01:56] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:01:56] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:56] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:01:56] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:01:56] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:56] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:01:56] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:01:56] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:56] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:56] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:56] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:01:56] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:01:56] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:01:56] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:01:56] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:06] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:06] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:06] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:06] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:06] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:06] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:06] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:06] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:06] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:06] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:06] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:06] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:06] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:06] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:06] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:06] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:16] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:16] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:16] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:16] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:16] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:16] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:16] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:16] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:16] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:16] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:16] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:16] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:16] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:16] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:16] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:16] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:26] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:26] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:26] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:26] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:26] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:26] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:26] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:26] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:26] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:26] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:26] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:26] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:26] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:26] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:26] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:26] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:36] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:36] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:36] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:37] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:37] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:37] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:37] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:37] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:37] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:37] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:37] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:37] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:37] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:47] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:47] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:47] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:47] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:47] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:47] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:47] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:47] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:47] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:47] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:47] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:47] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:47] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:47] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:47] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:47] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:02:57] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:02:57] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:02:57] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:57] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:02:57] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:02:57] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:57] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:02:57] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:02:57] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:57] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:57] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:57] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:02:57] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:02:57] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:02:57] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:02:57] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:03:07] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:03:07] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:07] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:07] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:07] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:03:07] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:07] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:07] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:07] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:07] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:07] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:07] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:07] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:07] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:07] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:03:07] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:03:17] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:03:17] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:17] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:17] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:17] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:03:17] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:17] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:17] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:17] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:17] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:17] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:17] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:17] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:17] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:17] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:03:17] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:03:27] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:03:27] Scheduler: Fetching data for trials: [0, 1, 3, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:27] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:27] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:27] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:03:27] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:27] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:27] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:27] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:27] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:27] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:27] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 1 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:27] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:27] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:27] ax.early_stopping.strategies.base: The number of completed trials (0) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:03:27] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 5).  Out: [INFO 09-30 17:03:37] Scheduler: Fetching data for newly completed trials: [1, 3].  Out: [INFO 09-30 17:03:37] Scheduler: Fetching data for trials: [0, 4, 5] because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:37] Scheduler: Retrieved COMPLETED trials: [1, 3].  Out: [INFO 09-30 17:03:37] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:37] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:37] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [ERROR 09-30 17:03:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:37] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:37] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:37] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 0 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:37] Scheduler: Failed to fetch val_acc for trial 1, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [WARNING 09-30 17:03:37] Scheduler: MetricFetchE INFO: Because val_acc is an objective, marking trial 1 as TrialStatus.FAILED.  Out: [WARNING 09-30 17:03:37] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:37] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:37] ax.early_stopping.strategies.base: The number of completed trials (1) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:03:38] Scheduler: Running trials [6]...  Out: [INFO 09-30 17:03:39] Scheduler: Generated all trials that can be generated currently. Model requires more data to generate more trials.  Out: [WARNING 09-30 17:03:39] Scheduler: Both init_seconds_between_polls and early_stopping_strategy supplied. init_seconds_between_polls=1 will be overrridden by early_stopping_strategy.seconds_between_polls=10 and polling will take place at a constant rate.  Out: [INFO 09-30 17:03:39] Scheduler: Fetching data for newly completed trials: [0].  Out: [INFO 09-30 17:03:39] Scheduler: Fetching data for trials: 4 - 6 because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:39] Scheduler: Retrieved COMPLETED trials: [0].  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d56cc0&gt;&quot;)  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d56cc0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 0, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [WARNING 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is an objective, marking trial 0 as TrialStatus.FAILED.  Out: [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 6, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d56cc0&gt;&quot;).  Out: [INFO 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 6 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:39] ax.early_stopping.strategies.base: The number of completed trials (1) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) [INFO 09-30 17:03:39] Scheduler: Running trials [7]...  Out: [INFO 09-30 17:03:39] Scheduler: Generated all trials that can be generated currently. Model requires more data to generate more trials.  Out: [WARNING 09-30 17:03:39] Scheduler: Both init_seconds_between_polls and early_stopping_strategy supplied. init_seconds_between_polls=1 will be overrridden by early_stopping_strategy.seconds_between_polls=10 and polling will take place at a constant rate.  Out: [INFO 09-30 17:03:39] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-30 17:03:39] Scheduler: Fetching data for trials: 4 - 7 because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;)  Out: [INFO 09-30 17:03:39] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;)  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:39] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [INFO 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 4 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 6, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;).  Out: [INFO 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 6 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:39] Scheduler: Failed to fetch val_acc for trial 7, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x171d3a480&gt;&quot;).  Out: [INFO 09-30 17:03:39] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 7 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:39] ax.early_stopping.strategies.base: The number of completed trials (1) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [INFO 09-30 17:03:39] Scheduler: Waiting for completed trials (for 10 sec, currently running trials: 4).  Out: [INFO 09-30 17:03:49] Scheduler: Fetching data for newly completed trials: 4 - 5.  Out: [INFO 09-30 17:03:49] Scheduler: Fetching data for trials: 6 - 7 because some metrics on experiment are available while trials are running.  Out: [INFO 09-30 17:03:49] Scheduler: Retrieved COMPLETED trials: 4 - 5.  Out: [INFO 09-30 17:03:49] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:49] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data  Out: [INFO 09-30 17:03:49] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;)  Out: [INFO 09-30 17:03:49] ax.core.metric: MetricFetchE INFO: Initialized MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;)  Out: [ERROR 09-30 17:03:49] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:49] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data . Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:49] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [ERROR 09-30 17:03:49] ax.core.experiment: Discovered Metric fetching Err while attaching data MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;). Ignoring for now -- will retry query on next call to fetch.  Out: [WARNING 09-30 17:03:49] Scheduler: Failed to fetch val_acc for trial 4, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [WARNING 09-30 17:03:49] Scheduler: MetricFetchE INFO: Because val_acc is an objective, marking trial 4 as TrialStatus.FAILED.  Out: [WARNING 09-30 17:03:49] Scheduler: Failed to fetch val_acc for trial 5, found MetricFetchE(message=&quot;Failed to fetch data for val_acc&quot;, exception=Found NaNs or Infs in data) with Traceback: Traceback (most recent call last): File &quot;/Users/cristianlara/Projects/Ax-1.0/ax/metrics/tensorboard.py&quot;, line 173, in bulk_fetch_trial_data raise ValueError(&quot;Found NaNs or Infs in data&quot;) ValueError: Found NaNs or Infs in data .  Out: [WARNING 09-30 17:03:49] Scheduler: MetricFetchE INFO: Because val_acc is an objective, marking trial 5 as TrialStatus.FAILED.  Out: [WARNING 09-30 17:03:49] Scheduler: Failed to fetch val_acc for trial 6, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;).  Out: [INFO 09-30 17:03:49] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 6 is still RUNNING continuing the experiment and retrying on next poll...  Out: [WARNING 09-30 17:03:49] Scheduler: Failed to fetch val_acc for trial 7, found MetricFetchE(message=&quot;No 'scalar' data found for trial in multiplexer mul=&lt;tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer object at 0x30fa46cc0&gt;&quot;).  Out: [INFO 09-30 17:03:49] Scheduler: MetricFetchE INFO: Because val_acc is available_while_running and trial 7 is still RUNNING continuing the experiment and retrying on next poll...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [INFO 09-30 17:03:49] ax.early_stopping.strategies.base: The number of completed trials (1) is less than the minimum number of curves needed for early stopping (5). Not early stopping.  Out: [WARNING 09-30 17:03:49] Scheduler: MetricFetchE INFO: Sweep aborted due to an exceeded error rate, which was primarily caused by failure to fetch metrics. Please check if anything could cause your metrics to be flaky or broken.  Out: Failure rate exceeds the tolerated trial failure rate of 0.5 (at least 5 out of first 6 trials failed or were abandoned). Checks are triggered both at the end of a optimization and if at least 5 trials have either failed, or have been abandoned, potentially automatically due to issues with the trial.  ","version":"Next","tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#results","content":" First, we examine the data stored on the experiment. This shows that each trial is associated with an entire learning curve, represented by the column &quot;steps&quot;.  experiment.lookup_data().map_df.head(n=10)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat(  \tarm_name\tmetric_name\tmean\tsem\ttrial_index\tstep0\t1_0\tval_acc\t-5.51856e+29\tnan\t1\t1874 1\t1_0\tval_acc\t3.63924e+31\tnan\t1\t3749 2\t1_0\tval_acc\t1.32168e+31\tnan\t1\t5624 3\t3_0\tval_acc\t-5.52222e+29\tnan\t3\t1874 4\t3_0\tval_acc\t-4.50869e+29\tnan\t3\t3749 5\t3_0\tval_acc\t-4.2755e+29\tnan\t3\t5624 6\t3_0\tval_acc\t-5.04816e+29\tnan\t3\t7499 7\t3_0\tval_acc\t-5.33289e+29\tnan\t3\t9374 8\t3_0\tval_acc\t-2.8559e+29\tnan\t3\t11249 9\t3_0\tval_acc\t-4.42968e+29\tnan\t3\t13124  Below is a summary of the experiment, showing that a portion of trials have been early stopped.  exp_to_df(experiment)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:49] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tval_acc\thidden_size_1\thidden_size_2\tlearning_rate\tdropout0\t0\t0_0\tFAILED\tSobol\tnan\t22\t85\t0.009309\t0.212433 1\t1\t1_0\tFAILED\tSobol\t1.32168e+31\t92\t23\t0.000652\t0.397744 2\t2\t2_0\tFAILED\tSobol\tnan\t67\t59\t0.0026\t0.113261 3\t3\t3_0\tCOMPLETED\tSobol\t-4.8447e+29\t29\t34\t0.000182\t0.309786 4\t4\t4_0\tFAILED\tSobol\tnan\t42\t56\t0.000362\t0.338057 5\t5\t5_0\tFAILED\tSobol\t2.95774e+30\t47\t36\t0.004476\t0.020423 6\t6\t6_0\tRUNNING\tSobol\tnan\t100\t118\t0.00013\t0.484715 7\t7\t7_0\tRUNNING\tSobol\tnan\t20\t17\t0.001605\t0.186138  We can give a very rough estimate of the amount of computational savings due to early stopping, by looking at the total number of steps used when early stopping is used versus the number of steps used if we ran all trials to completion. Note to do a true comparison, one should run full HPO loops with and without early stopping (as early stopping will influence the model and future points selected by the generation strategy).  map_df = experiment.lookup_data().map_df trial_to_max_steps = map_df.groupby(&quot;trial_index&quot;)[&quot;step&quot;].max() completed_trial_steps = trial_to_max_steps.iloc[0] savings = 1.0 - trial_to_max_steps.sum() / ( completed_trial_steps * len(trial_to_max_steps) ) # TODO format nicer print(f&quot;A rough estimate of the computational savings is {100 * savings}%.&quot;)   Out: A rough estimate of the computational savings is -144.4760550023708%.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat(  ","version":"Next","tagName":"h2"},{"title":"Visualizations​","type":1,"pageTitle":"Trial-Level Early Stopping","url":"/Ax/docs/tutorials/early_stopping/#visualizations","content":" Finally, we show a visualization of learning curves versus actual elapsed wall time. This helps to illustrate that stopped trials make room for additional trials to be run.  # helper function for getting trial start times def time_started(row): trial_index = row[&quot;trial_index&quot;] return experiment.trials[trial_index].time_run_started # helper function for getting trial completion times def time_completed(row): trial_index = row[&quot;trial_index&quot;] return experiment.trials[trial_index].time_completed # helper function for getting relevant data from experiment # with early stopping into useful dfs def early_stopping_exp_to_df(experiment): trials_df = exp_to_df(experiment) curve_df = experiment.lookup_data().map_df training_row_df = ( curve_df.groupby(&quot;trial_index&quot;).max().reset_index()[[&quot;trial_index&quot;, &quot;steps&quot;]] ) trials_df = trials_df.merge(training_row_df, on=&quot;trial_index&quot;) trials_df[&quot;time_started&quot;] = trials_df.apply(func=time_started, axis=1) trials_df[&quot;time_completed&quot;] = trials_df.apply(func=time_completed, axis=1) start_time = trials_df[&quot;time_started&quot;].min() trials_df[&quot;time_started_rel&quot;] = ( trials_df[&quot;time_started&quot;] - start_time ).dt.total_seconds() trials_df[&quot;time_completed_rel&quot;] = ( trials_df[&quot;time_completed&quot;] - start_time ).dt.total_seconds() return trials_df, curve_df def plot_curves_by_wall_time(trials_df, curve_df): trials = set(curve_df[&quot;trial_index&quot;]) fig, ax = plt.subplots(1, 1, figsize=(10, 6)) ax.set(xlabel=&quot;seconds since start&quot;, ylabel=&quot;validation accuracy&quot;) for trial_index in trials: this_trial_df = curve_df[curve_df[&quot;trial_index&quot;] == trial_index] start_time_rel = trials_df[&quot;time_started_rel&quot;].iloc[trial_index] completed_time_rel = trials_df[&quot;time_completed_rel&quot;].iloc[trial_index] total_steps = trials_df.loc[trial_index, &quot;steps&quot;] smoothed_curve = this_trial_df[&quot;mean&quot;].rolling(window=3).mean() x = ( start_time_rel + (completed_time_rel - start_time_rel) / total_steps * this_trial_df[&quot;steps&quot;] ) ax.plot( x, smoothed_curve, label=f&quot;trial #{trial_index}&quot; if trial_index % 2 == 1 else None, ) ax.legend()   # wrap in try/except in case of flaky I/O issues try: trials_df, curve_df = early_stopping_exp_to_df(experiment) plot_curves_by_wall_time(trials_df, curve_df) except Exception as e: print(f&quot;Encountered exception while plotting results: {e}&quot;)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat( [WARNING 09-30 17:03:49] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Encountered exception while plotting results: &quot;['steps'] not in index&quot;  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/map_data.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. df = pd.concat(   ","version":"Next","tagName":"h2"},{"title":"Service API Example on Hartmann6","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/gpei_hartmann_service/","content":"","keywords":"","version":"Next"},{"title":"1. Initialize client​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#1-initialize-client","content":" Create a client object to interface with Ax APIs. By default this runs locally without storage.  ax_client = AxClient()   Out: [INFO 09-29 16:57:16] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  ","version":"Next","tagName":"h2"},{"title":"2. Set up experiment​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#2-set-up-experiment","content":" An experiment consists of a search space (parameters and parameter constraints) andoptimization configuration (objectives and outcome constraints). Note that:  Only parameters, and objectives arguments are required.Dictionaries in parameters have the following required keys: &quot;name&quot; - parameter name, &quot;type&quot; - parameter type (&quot;range&quot;, &quot;choice&quot; or &quot;fixed&quot;), &quot;bounds&quot; for range parameters, &quot;values&quot; for choice parameters, and &quot;value&quot; for fixed parameters.Dictionaries in parameters can optionally include &quot;value_type&quot; (&quot;int&quot;, &quot;float&quot;, &quot;bool&quot; or &quot;str&quot;), &quot;log_scale&quot; flag for range parameters, and &quot;is_ordered&quot; flag for choice parameters.parameter_constraints should be a list of strings of form &quot;p1 &gt;= p2&quot; or &quot;p1 + p2 &lt;= some_bound&quot;.outcome_constraints should be a list of strings of form &quot;constrained_metric &lt;= some_bound&quot;.  ax_client.create_experiment( name=&quot;hartmann_test_experiment&quot;, parameters=[ { &quot;name&quot;: &quot;x1&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], &quot;value_type&quot;: &quot;float&quot;, # Optional, defaults to inference from type of &quot;bounds&quot;. &quot;log_scale&quot;: False, # Optional, defaults to False. }, { &quot;name&quot;: &quot;x2&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x3&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x4&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x5&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, { &quot;name&quot;: &quot;x6&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, ], objectives={&quot;hartmann6&quot;: ObjectiveProperties(minimize=True)}, parameter_constraints=[&quot;x1 + x2 &lt;= 2.0&quot;], # Optional. outcome_constraints=[&quot;l2norm &lt;= 1.25&quot;], # Optional. )   Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x3. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x4. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x5. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x6. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:57:16] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[ParameterConstraint(1.0*x1 + 1.0*x2 &lt;= 2.0)]).  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 16:57:16] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.  ","version":"Next","tagName":"h2"},{"title":"3. Define how to evaluate trials​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#3-define-how-to-evaluate-trials","content":" When using Ax a service, evaluation of parameterizations suggested by Ax is done either locally or, more commonly, using an external scheduler. Below is a dummy evaluation function that outputs data for two metrics &quot;hartmann6&quot; and &quot;l2norm&quot;. Note that all returned metrics correspond to either the objectives set on experiment creation or the metric names mentioned in outcome_constraints.  import numpy as np def evaluate(parameterization): x = np.array([parameterization.get(f&quot;x{i+1}&quot;) for i in range(6)]) # In our case, standard error is 0, since we are computing a synthetic function. return {&quot;hartmann6&quot;: (hartmann6(x), 0.0), &quot;l2norm&quot;: (np.sqrt((x**2).sum()), 0.0)}   Result of the evaluation should generally be a mapping of the format:\\{metric_name -&gt; (mean, SEM)\\}. If there is only one metric in the experiment – the objective – then evaluation function can return a single tuple of mean and SEM, in which case Ax will assume that evaluation corresponds to the objective. It can also return only the mean as a float, in which case Ax will treat SEM as unknown and use a model that can infer it.  For more details on evaluation function, refer to the &quot;Trial Evaluation&quot; section in the Ax docs at ax.dev  ","version":"Next","tagName":"h2"},{"title":"4. Run optimization loop​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#4-run-optimization-loop","content":" With the experiment set up, we can start the optimization loop.  At each step, the user queries the client for a new trial then submits the evaluation of that trial back to the client.  Note that Ax auto-selects an appropriate optimization algorithm based on the search space. For more advance use cases that require a specific optimization algorithm, pass ageneration_strategy argument into the AxClient constructor. Note that when Bayesian Optimization is used, generating new trials may take a few minutes.  for i in range(25): parameterization, trial_index = ax_client.get_next_trial() # Local evaluation here can be replaced with deployment to external system. ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 0 with parameters {'x1': 0.153071, 'x2': 0.527557, 'x3': 0.389267, 'x4': 0.101972, 'x5': 0.230721, 'x6': 0.72288} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 0 with data: {'hartmann6': (-1.306389, 0.0), 'l2norm': (1.01954, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 1 with parameters {'x1': 0.550232, 'x2': 0.361861, 'x3': 0.912849, 'x4': 0.896951, 'x5': 0.500848, 'x6': 0.343655} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 1 with data: {'hartmann6': (-0.031438, 0.0), 'l2norm': (1.562198, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 2 with parameters {'x1': 0.756349, 'x2': 0.964395, 'x3': 0.233609, 'x4': 0.448135, 'x5': 0.778651, 'x6': 0.010256} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 2 with data: {'hartmann6': (-0.293083, 0.0), 'l2norm': (1.537505, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 3 with parameters {'x1': 0.415348, 'x2': 0.176964, 'x3': 0.694753, 'x4': 0.551707, 'x5': 0.486126, 'x6': 0.923208} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 3 with data: {'hartmann6': (-0.550851, 0.0), 'l2norm': (1.442055, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 4 with parameters {'x1': 0.28918, 'x2': 0.791496, 'x3': 0.863847, 'x4': 0.637525, 'x5': 0.317184, 'x6': 0.603778} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 4 with data: {'hartmann6': (-0.181671, 0.0), 'l2norm': (1.525751, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.882628, 'x2': 0.067148, 'x3': 0.34063, 'x4': 0.36207, 'x5': 0.914397, 'x6': 0.454686} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 5 with data: {'hartmann6': (-0.001877, 0.0), 'l2norm': (1.439974, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.67395, 'x2': 0.732191, 'x3': 0.520724, 'x4': 0.795273, 'x5': 0.67761, 'x6': 0.129878} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 6 with data: {'hartmann6': (-0.409175, 0.0), 'l2norm': (1.539464, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.029241, 'x2': 0.378418, 'x3': 0.059946, 'x4': 0.203411, 'x5': 0.08717, 'x6': 0.811659} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 7 with data: {'hartmann6': (-0.664824, 0.0), 'l2norm': (0.924885, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.118752, 'x2': 0.888864, 'x3': 0.615011, 'x4': 0.277306, 'x5': 0.60691, 'x6': 0.97283} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 8 with data: {'hartmann6': (-0.087945, 0.0), 'l2norm': (1.604386, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.709272, 'x2': 0.219292, 'x3': 0.091429, 'x4': 0.724515, 'x5': 0.126852, 'x6': 0.093692} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 9 with data: {'hartmann6': (-0.023022, 0.0), 'l2norm': (1.053236, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 10 with parameters {'x1': 0.972308, 'x2': 0.571837, 'x3': 0.769935, 'x4': 0.181147, 'x5': 0.402761, 'x6': 0.260342} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 10 with data: {'hartmann6': (-0.04802, 0.0), 'l2norm': (1.458763, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 16:57:17] ax.service.ax_client: Generated new trial 11 with parameters {'x1': 0.324667, 'x2': 0.288283, 'x3': 0.308792, 'x4': 0.819762, 'x5': 0.860295, 'x6': 0.673138} using model Sobol.  Out: [INFO 09-29 16:57:17] ax.service.ax_client: Completed trial 11 with data: {'hartmann6': (-0.02347, 0.0), 'l2norm': (1.465981, 0.0)}.  Out: [INFO 09-29 16:57:20] ax.service.ax_client: Generated new trial 12 with parameters {'x1': 0.150641, 'x2': 0.313917, 'x3': 0.413272, 'x4': 0.057123, 'x5': 0.217892, 'x6': 0.812328} using model BoTorch.  Out: [INFO 09-29 16:57:20] ax.service.ax_client: Completed trial 12 with data: {'hartmann6': (-1.407953, 0.0), 'l2norm': (1.001322, 0.0)}.  Out: [INFO 09-29 16:57:24] ax.service.ax_client: Generated new trial 13 with parameters {'x1': 0.0, 'x2': 0.083117, 'x3': 0.416058, 'x4': 0.609, 'x5': 0.224715, 'x6': 0.937201} using model BoTorch.  Out: [INFO 09-29 16:57:24] ax.service.ax_client: Completed trial 13 with data: {'hartmann6': (-0.494109, 0.0), 'l2norm': (1.216444, 0.0)}.  Out: [INFO 09-29 16:57:27] ax.service.ax_client: Generated new trial 14 with parameters {'x1': 0.280153, 'x2': 0.084903, 'x3': 0.423798, 'x4': 0.0, 'x5': 0.254748, 'x6': 0.882275} using model BoTorch.  Out: [INFO 09-29 16:57:27] ax.service.ax_client: Completed trial 14 with data: {'hartmann6': (-1.01778, 0.0), 'l2norm': (1.052903, 0.0)}.  Out: [INFO 09-29 16:57:31] ax.service.ax_client: Generated new trial 15 with parameters {'x1': 0.125586, 'x2': 0.401714, 'x3': 0.496758, 'x4': 0.0, 'x5': 0.154412, 'x6': 0.794533} using model BoTorch.  Out: [INFO 09-29 16:57:31] ax.service.ax_client: Completed trial 15 with data: {'hartmann6': (-0.935041, 0.0), 'l2norm': (1.038769, 0.0)}.  Out: [INFO 09-29 16:57:35] ax.service.ax_client: Generated new trial 16 with parameters {'x1': 0.037922, 'x2': 0.332036, 'x3': 0.233085, 'x4': 0.113017, 'x5': 0.231807, 'x6': 0.81388} using model BoTorch.  Out: [INFO 09-29 16:57:35] ax.service.ax_client: Completed trial 16 with data: {'hartmann6': (-1.34966, 0.0), 'l2norm': (0.946004, 0.0)}.  Out: [INFO 09-29 16:57:40] ax.service.ax_client: Generated new trial 17 with parameters {'x1': 0.217877, 'x2': 0.0, 'x3': 0.28475, 'x4': 0.07931, 'x5': 0.205823, 'x6': 1.0} using model BoTorch.  Out: [INFO 09-29 16:57:40] ax.service.ax_client: Completed trial 17 with data: {'hartmann6': (-0.649063, 0.0), 'l2norm': (1.084991, 0.0)}.  Out: [INFO 09-29 16:57:46] ax.service.ax_client: Generated new trial 18 with parameters {'x1': 0.333294, 'x2': 0.315926, 'x3': 0.38454, 'x4': 0.0, 'x5': 0.230724, 'x6': 0.718245} using model BoTorch.  Out: [INFO 09-29 16:57:46] ax.service.ax_client: Completed trial 18 with data: {'hartmann6': (-1.208567, 0.0), 'l2norm': (0.963262, 0.0)}.  Out: [INFO 09-29 16:57:52] ax.service.ax_client: Generated new trial 19 with parameters {'x1': 0.0, 'x2': 0.681865, 'x3': 0.541129, 'x4': 0.0, 'x5': 0.222552, 'x6': 0.803556} using model BoTorch.  Out: [INFO 09-29 16:57:52] ax.service.ax_client: Completed trial 19 with data: {'hartmann6': (-0.503675, 0.0), 'l2norm': (1.205401, 0.0)}.  Out: [INFO 09-29 16:57:56] ax.service.ax_client: Generated new trial 20 with parameters {'x1': 0.072429, 'x2': 0.32688, 'x3': 0.3496, 'x4': 0.144298, 'x5': 0.232258, 'x6': 0.798205} using model BoTorch.  Out: [INFO 09-29 16:57:56] ax.service.ax_client: Completed trial 20 with data: {'hartmann6': (-1.759588, 0.0), 'l2norm': (0.972736, 0.0)}.  Out: [INFO 09-29 16:58:00] ax.service.ax_client: Generated new trial 21 with parameters {'x1': 0.032325, 'x2': 0.193541, 'x3': 0.368322, 'x4': 0.228741, 'x5': 0.222972, 'x6': 0.776981} using model BoTorch.  Out: [INFO 09-29 16:58:00] ax.service.ax_client: Completed trial 21 with data: {'hartmann6': (-2.227752, 0.0), 'l2norm': (0.938031, 0.0)}.  Out: [INFO 09-29 16:58:04] ax.service.ax_client: Generated new trial 22 with parameters {'x1': 0.0, 'x2': 0.036509, 'x3': 0.420134, 'x4': 0.260533, 'x5': 0.242153, 'x6': 0.740498} using model BoTorch.  Out: [INFO 09-29 16:58:04] ax.service.ax_client: Completed trial 22 with data: {'hartmann6': (-2.370043, 0.0), 'l2norm': (0.923417, 0.0)}.  Out: [INFO 09-29 16:58:10] ax.service.ax_client: Generated new trial 23 with parameters {'x1': 0.396217, 'x2': 0.039192, 'x3': 0.414575, 'x4': 0.284219, 'x5': 0.123238, 'x6': 0.746905} using model BoTorch.  Out: [INFO 09-29 16:58:10] ax.service.ax_client: Completed trial 23 with data: {'hartmann6': (-1.5316, 0.0), 'l2norm': (0.992085, 0.0)}.  Out: [INFO 09-29 16:58:14] ax.service.ax_client: Generated new trial 24 with parameters {'x1': 0.0, 'x2': 0.150789, 'x3': 0.449015, 'x4': 0.249784, 'x5': 0.307963, 'x6': 0.693384} using model BoTorch.  Out: [INFO 09-29 16:58:14] ax.service.ax_client: Completed trial 24 with data: {'hartmann6': (-2.786172, 0.0), 'l2norm': (0.928637, 0.0)}.  ","version":"Next","tagName":"h2"},{"title":"How many trials can run in parallel?​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#how-many-trials-can-run-in-parallel","content":" By default, Ax restricts number of trials that can run in parallel for some optimization stages, in order to improve the optimization performance and reduce the number of trials that the optimization will require. To check the maximum parallelism for each optimization stage:  ax_client.get_max_parallelism()   Out: [(12, 12), (-1, 3)]  The output of this function is a list of tuples of form (number of trials, max parallelism), so the example above means &quot;the max parallelism is 12 for the first 12 trials and 3 for all subsequent trials.&quot; This is because the first 12 trials are produced quasi-randomly and can all be evaluated at once, and subsequent trials are produced via Bayesian optimization, which converges on optimal point in fewer trials when parallelism is limited. MaxParallelismReachedException indicates that the parallelism limit has been reached –– refer to the 'Service API Exceptions Meaning and Handling' section at the end of the tutorial for handling.  ","version":"Next","tagName":"h3"},{"title":"How to view all existing trials during optimization?​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#how-to-view-all-existing-trials-during-optimization","content":" ax_client.generation_strategy.trials_as_df   Out: [INFO 09-29 16:58:15] ax.modelbridge.generation_strategy: Note that parameter values in dataframe are rounded to 2 decimal points; the values in the dataframe are thus not the exact ones suggested by Ax in trials.  \tGeneration Step\tGeneration Model(s)\tTrial Index\tTrial Status\tArm Parameterizations0\t[GenerationStep_0]\t[Sobol]\t0\tCOMPLETED\t{'0_0': {'x1': 0.15, 'x2': 0.53, 'x3': 0.39, '... 1\t[GenerationStep_0]\t[Sobol]\t1\tCOMPLETED\t{'1_0': {'x1': 0.55, 'x2': 0.36, 'x3': 0.91, '... 2\t[GenerationStep_0]\t[Sobol]\t2\tCOMPLETED\t{'2_0': {'x1': 0.76, 'x2': 0.96, 'x3': 0.23, '... 3\t[GenerationStep_0]\t[Sobol]\t3\tCOMPLETED\t{'3_0': {'x1': 0.42, 'x2': 0.18, 'x3': 0.69, '... 4\t[GenerationStep_0]\t[Sobol]\t4\tCOMPLETED\t{'4_0': {'x1': 0.29, 'x2': 0.79, 'x3': 0.86, '... 5\t[GenerationStep_0]\t[Sobol]\t5\tCOMPLETED\t{'5_0': {'x1': 0.88, 'x2': 0.07, 'x3': 0.34, '... 6\t[GenerationStep_0]\t[Sobol]\t6\tCOMPLETED\t{'6_0': {'x1': 0.67, 'x2': 0.73, 'x3': 0.52, '... 7\t[GenerationStep_0]\t[Sobol]\t7\tCOMPLETED\t{'7_0': {'x1': 0.03, 'x2': 0.38, 'x3': 0.06, '... 8\t[GenerationStep_0]\t[Sobol]\t8\tCOMPLETED\t{'8_0': {'x1': 0.12, 'x2': 0.89, 'x3': 0.62, '... 9\t[GenerationStep_0]\t[Sobol]\t9\tCOMPLETED\t{'9_0': {'x1': 0.71, 'x2': 0.22, 'x3': 0.09, '... 10\t[GenerationStep_0]\t[Sobol]\t10\tCOMPLETED\t{'10_0': {'x1': 0.97, 'x2': 0.57, 'x3': 0.77, ... 11\t[GenerationStep_0]\t[Sobol]\t11\tCOMPLETED\t{'11_0': {'x1': 0.32, 'x2': 0.29, 'x3': 0.31, ... 12\t[GenerationStep_1]\t[BoTorch]\t12\tCOMPLETED\t{'12_0': {'x1': 0.15, 'x2': 0.31, 'x3': 0.41, ... 13\t[GenerationStep_1]\t[BoTorch]\t13\tCOMPLETED\t{'13_0': {'x1': 0.0, 'x2': 0.08, 'x3': 0.42, '... 14\t[GenerationStep_1]\t[BoTorch]\t14\tCOMPLETED\t{'14_0': {'x1': 0.28, 'x2': 0.08, 'x3': 0.42, ... 15\t[GenerationStep_1]\t[BoTorch]\t15\tCOMPLETED\t{'15_0': {'x1': 0.13, 'x2': 0.4, 'x3': 0.5, 'x... 16\t[GenerationStep_1]\t[BoTorch]\t16\tCOMPLETED\t{'16_0': {'x1': 0.04, 'x2': 0.33, 'x3': 0.23, ... 17\t[GenerationStep_1]\t[BoTorch]\t17\tCOMPLETED\t{'17_0': {'x1': 0.22, 'x2': 0.0, 'x3': 0.28, '... 18\t[GenerationStep_1]\t[BoTorch]\t18\tCOMPLETED\t{'18_0': {'x1': 0.33, 'x2': 0.32, 'x3': 0.38, ... 19\t[GenerationStep_1]\t[BoTorch]\t19\tCOMPLETED\t{'19_0': {'x1': 0.0, 'x2': 0.68, 'x3': 0.54, '... 20\t[GenerationStep_1]\t[BoTorch]\t20\tCOMPLETED\t{'20_0': {'x1': 0.07, 'x2': 0.33, 'x3': 0.35, ... 21\t[GenerationStep_1]\t[BoTorch]\t21\tCOMPLETED\t{'21_0': {'x1': 0.03, 'x2': 0.19, 'x3': 0.37, ... 22\t[GenerationStep_1]\t[BoTorch]\t22\tCOMPLETED\t{'22_0': {'x1': 0.0, 'x2': 0.04, 'x3': 0.42, '... 23\t[GenerationStep_1]\t[BoTorch]\t23\tCOMPLETED\t{'23_0': {'x1': 0.4, 'x2': 0.04, 'x3': 0.41, '... 24\t[GenerationStep_1]\t[BoTorch]\t24\tCOMPLETED\t{'24_0': {'x1': 0.0, 'x2': 0.15, 'x3': 0.45, '...  ","version":"Next","tagName":"h3"},{"title":"5. Retrieve best parameters​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#5-retrieve-best-parameters","content":" Once it's complete, we can access the best parameters found, as well as the corresponding metric values.  best_parameters, values = ax_client.get_best_parameters() best_parameters   Out: {'x1': 2.377190650665489e-16, 'x2': 0.15078873937241558, 'x3': 0.4490146600613049, 'x4': 0.24978372446784, 'x5': 0.3079634202053345, 'x6': 0.6933838786685046}  means, covariances = values means   Out: {'l2norm': 0.9286373327291121, 'hartmann6': -2.786165442089028}  For comparison, Hartmann6 minimum:  hartmann6.fmin   Out: -3.32237  ","version":"Next","tagName":"h2"},{"title":"6. Plot the response surface and optimization trace​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#6-plot-the-response-surface-and-optimization-trace","content":" Here we arbitrarily select &quot;x1&quot; and &quot;x2&quot; as the two parameters to plot for both metrics, &quot;hartmann6&quot; and &quot;l2norm&quot;.  render(ax_client.get_contour_plot())   Out: [INFO 09-29 16:58:16] ax.service.ax_client: Retrieving contour plot with parameter 'x1' on X-axis and 'x2' on Y-axis, for metric 'hartmann6'. Remaining parameters are affixed to the middle of their range.  loading...  We can also retrieve a contour plot for the other metric, &quot;l2norm&quot; –– say, we are interested in seeing the response surface for parameters &quot;x3&quot; and &quot;x4&quot; for this one.  render(ax_client.get_contour_plot(param_x=&quot;x3&quot;, param_y=&quot;x4&quot;, metric_name=&quot;l2norm&quot;))   Out: [INFO 09-29 16:58:17] ax.service.ax_client: Retrieving contour plot with parameter 'x3' on X-axis and 'x4' on Y-axis, for metric 'l2norm'. Remaining parameters are affixed to the middle of their range.  loading...  Here we plot the optimization trace, showing the progression of finding the point with the optimal objective:  render( ax_client.get_optimization_trace(objective_optimum=hartmann6.fmin) ) # Objective_optimum is optional.   loading...  ","version":"Next","tagName":"h2"},{"title":"7. Save / reload optimization to JSON / SQL​","type":1,"pageTitle":"Service API Example on Hartmann6","url":"/Ax/docs/tutorials/gpei_hartmann_service/#7-save--reload-optimization-to-json--sql","content":" We can serialize the state of optimization to JSON and save it to a .json file or save it to the SQL backend. For the former:  ax_client.save_to_json_file() # For custom filepath, pass `filepath` argument.   Out: [INFO 09-29 16:58:18] ax.service.ax_client: Saved JSON-serialized state of optimization to ax_client_snapshot.json.  restored_ax_client = ( AxClient.load_from_json_file() ) # For custom filepath, pass `filepath` argument.   Out: [INFO 09-29 16:58:18] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  To store state of optimization to an SQL backend, first followsetup instructions on Ax website.  Having set up the SQL backend, pass DBSettings to AxClient on instantiation (note that SQLAlchemy dependency will have to be installed – for installation, refer tooptional dependencies on Ax website):  from ax.storage.sqa_store.structs import DBSettings # URL is of the form &quot;dialect+driver://username:password@host:port/database&quot;. db_settings = DBSettings(url=&quot;sqlite:///foo.db&quot;) # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary. new_ax = AxClient(db_settings=db_settings)   Out: [INFO 09-29 16:58:18] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  When valid DBSettings are passed into AxClient, a unique experiment name is a required argument (name) to ax_client.create_experiment. The state of the optimization is auto-saved any time it changes (i.e. a new trial is added or completed, etc).  To reload an optimization state later, instantiate AxClient with the same DBSettingsand use ax_client.load_experiment_from_database(experiment_name=&quot;my_experiment&quot;).  Special Cases  Evaluation failure: should any optimization iterations fail during evaluation,log_trial_failure will ensure that the same trial is not proposed again.  _, trial_index = ax_client.get_next_trial() ax_client.log_trial_failure(trial_index=trial_index)   Out: [INFO 09-29 16:58:24] ax.service.ax_client: Generated new trial 25 with parameters {'x1': 0.0, 'x2': 0.189512, 'x3': 0.534758, 'x4': 0.251695, 'x5': 0.367481, 'x6': 0.608447} using model BoTorch.  Out: [INFO 09-29 16:58:24] ax.service.ax_client: Registered failure of trial 25.  Adding custom trials: should there be need to evaluate a specific parameterization,attach_trial will add it to the experiment.  ax_client.attach_trial( parameters={&quot;x1&quot;: 0.9, &quot;x2&quot;: 0.9, &quot;x3&quot;: 0.9, &quot;x4&quot;: 0.9, &quot;x5&quot;: 0.9, &quot;x6&quot;: 0.9} )   Out: [INFO 09-29 16:58:24] ax.core.experiment: Attached custom parameterizations [{'x1': 0.9, 'x2': 0.9, 'x3': 0.9, 'x4': 0.9, 'x5': 0.9, 'x6': 0.9}] as trial 26.  Out: ({'x1': 0.9, 'x2': 0.9, 'x3': 0.9, 'x4': 0.9, 'x5': 0.9, 'x6': 0.9}, 26)  Need to run many trials in parallel: for optimal results and optimization efficiency, we strongly recommend sequential optimization (generating a few trials, then waiting for them to be completed with evaluation data). However, if your use case needs to dispatch many trials in parallel before they are updated with data and you are running into the &quot;All trials for current model have been generated, but not enough data has been observed to fit next model&quot; error, instantiate AxClient asAxClient(enforce_sequential_optimization=False).    Nonlinear parameter constraints and/or constraints on non-Range parameters: Ax parameter constraints can currently only support linear inequalities (discussion). Users may be able to simulate this functionality, however, by substituting the following evaluate function for that defined in section 3 above.  def evaluate(parameterization): x = np.array([parameterization.get(f&quot;x{i+1}&quot;) for i in range(6)]) # First calculate the nonlinear quantity to be constrained. l2norm = np.sqrt((x**2).sum()) # Then define a constraint consistent with an outcome constraint on this experiment. if l2norm &gt; 1.25: return {&quot;l2norm&quot;: (l2norm, 0.0)} return {&quot;hartmann6&quot;: (hartmann6(x), 0.0), &quot;l2norm&quot;: (l2norm, 0.0)}   For this to work, the constraint quantity (l2norm in this case) should have a corresponding outcome constraint on the experiment. See the outcome_constraint arg to ax_client.create_experiment in section 2 above for how to specify outcome constraints.  This setup accomplishes the following:  Allows computation of an arbitrarily complex constraint value.Skips objective computation when the constraint is violated, useful when the objective is relatively expensive to compute.Constraint metric values are returned even when there is a violation. This helps the model understand + avoid constraint violations.  Service API Exceptions Meaning and Handling  DataRequiredError: Ax generation strategy needs to be updated with more data to proceed to the next optimization model. When the optimization moves from initialization stage to the Bayesian optimization stage, the underlying BayesOpt model needs sufficient data to train. For optimal results and optimization efficiency (finding the optimal point in the least number of trials), we recommend sequential optimization (generating a few trials, then waiting for them to be completed with evaluation data). Therefore, the correct way to handle this exception is to wait until more trial evaluations complete and log their data via ax_client.complete_trial(...).  However, if there is strong need to generate more trials before more data is available, instantiate AxClient as AxClient(enforce_sequential_optimization=False). With this setting, as many trials will be generated from the initialization stage as requested, and the optimization will move to the BayesOpt stage whenever enough trials are completed.  MaxParallelismReachedException: generation strategy restricts the number of trials that can be ran simultaneously (to encourage sequential optimization), and the parallelism limit has been reached. The correct way to handle this exception is the same as DataRequiredError – to wait until more trial evluations complete and log their data via ax_client.complete_trial(...).  In some cases higher parallelism is important, soenforce_sequential_optimization=False kwarg to AxClient allows to suppress limiting of parallelism. It's also possible to override the default parallelism setting for all stages of the optimization by passing choose_generation_strategy_kwargs toax_client.create_experiment:  ax_client = AxClient() ax_client.create_experiment( parameters=[ {&quot;name&quot;: &quot;x&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [-5.0, 10.0]}, {&quot;name&quot;: &quot;y&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 15.0]}, ], # Sets max parallelism to 10 for all steps of the generation strategy. choose_generation_strategy_kwargs={&quot;max_parallelism_override&quot;: 10}, )   Out: [INFO 09-29 16:58:25] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 16:58:25] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:25] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter y. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 16:58:25] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='y', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]).  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 16:58:25] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  ax_client.get_max_parallelism() # Max parallelism is now 10 for all stages of the optimization.   Out: [(5, 10), (-1, 10)] ","version":"Next","tagName":"h2"},{"title":"Setup and Usage of BoTorch Models in Ax","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/modular_botax/","content":"","keywords":"","version":"Next"},{"title":"1. Quick-start example​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#1-quick-start-example","content":" Here we set up a BoTorchModel with SingleTaskGP with qLogNoisyExpectedImprovement, one of the most popular combinations in Ax:  experiment = get_branin_experiment(with_trial=True) data = get_branin_data(trials=[experiment.trials[0]])   Out: [INFO 09-29 17:02:46] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  # `Models` automatically selects a model + model bridge combination. # For `BOTORCH_MODULAR`, it will select `BoTorchModel` and `TorchModelBridge`. model_bridge_with_GPEI = Models.BOTORCH_MODULAR( experiment=experiment, data=data, surrogate=Surrogate(SingleTaskGP), # Optional, will use default if unspecified botorch_acqf_class=qLogNoisyExpectedImprovement, # Optional, will use default if unspecified )   Out: [INFO 09-29 17:02:46] ax.modelbridge.transforms.standardize_y: Outcome branin is constant, within tolerance.  Now we can use this model to generate candidates (gen), predict outcome at a point (predict), or evaluate acquisition function value at a given point (evaluate_acquisition_function).  generator_run = model_bridge_with_GPEI.gen(n=1) generator_run.arms[0]   Out: Arm(parameters={'x1': -5.0, 'x2': 0.0})    Before you read the rest of this tutorial:  Note that the concept of ‘model’ is Ax is somewhat a misnomer; we use'model' to refer to an optimization setup capable of producing candidate points for optimization (and often capable of being fit to data, with exception for quasi-random generators). SeeModels documentation page for more information.Learn about ModelBridge in Ax, as users should rarely be interacting with a Modelobject directly (more about ModelBridge, a data transformation layer in Ax,here).  ","version":"Next","tagName":"h2"},{"title":"2. BoTorchModel = Surrogate + Acquisition​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#2-botorchmodel--surrogate--acquisition","content":" A BoTorchModel in Ax consists of two main subcomponents: a surrogate model and an acquisition function. A surrogate model is represented as an instance of Ax’sSurrogate class, which is a wrapper around BoTorch's Model class. The acquisition function is represented as an instance of Ax’s Acquisition class, a wrapper around BoTorch's AcquisitionFunction class.  ","version":"Next","tagName":"h2"},{"title":"2A. Example that uses defaults and requires no options​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#2a-example-that-uses-defaults-and-requires-no-options","content":" BoTorchModel does not always require surrogate and acquisition specification. If instantiated without one or both components specified, defaults are selected based on properties of experiment and data (see Appendix 2 for auto-selection logic).  # The surrogate is not specified, so it will be auto-selected # during `model.fit`. GPEI_model = BoTorchModel(botorch_acqf_class=qLogExpectedImprovement) # The acquisition class is not specified, so it will be # auto-selected during `model.gen` or `model.evaluate_acquisition` GPEI_model = BoTorchModel(surrogate=Surrogate(SingleTaskGP)) # Both the surrogate and acquisition class will be auto-selected. GPEI_model = BoTorchModel()   ","version":"Next","tagName":"h3"},{"title":"2B. Example with all the options​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#2b-example-with-all-the-options","content":" Below are the full set of configurable settings of a BoTorchModel with their descriptions:  model = BoTorchModel( # Optional `Surrogate` specification to use instead of default surrogate=Surrogate( # BoTorch `Model` type botorch_model_class=SingleTaskGP, # Optional, MLL class with which to optimize model parameters mll_class=ExactMarginalLogLikelihood, # Optional, dictionary of keyword arguments to underlying # BoTorch `Model` constructor model_options={}, ), # Optional BoTorch `AcquisitionFunction` to use instead of default botorch_acqf_class=qLogExpectedImprovement, # Optional dict of keyword arguments, passed to the input # constructor for the given BoTorch `AcquisitionFunction` acquisition_options={}, # Optional Ax `Acquisition` subclass (if the given BoTorch # `AcquisitionFunction` requires one, which is rare) acquisition_class=None, # Less common model settings shown with default values, refer # to `BoTorchModel` documentation for detail refit_on_cv=False, warm_start_refit=True, )   ","version":"Next","tagName":"h3"},{"title":"2C. Surrogate and Acquisition Q&A​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#2c-surrogate-and-acquisition-qa","content":" Why is the surrogate argument expected to be an instance, but botorch_acqf_class–– a class? Because a BoTorch AcquisitionFunction object (and therefore its Ax wrapper, Acquisition) is ephemeral: it is constructed, immediately used, and destroyed during BoTorchModel.gen, so there is no reason to keep around an Acquisitioninstance. A Surrogate, on another hand, is kept in memory as long as its parentBoTorchModel is.  How to know when to use specify acquisition_class (and thereby a non-default Acquisition type) instead of just passing in botorch_acqf_class? In short, customAcquisition subclasses are needed when a given AcquisitionFunction in BoTorch needs some non-standard subcomponents or inputs (e.g. a custom BoTorchMCAcquisitionObjective).  Please post any other questions you have to our dedicated issue on Github:https://github.com/facebook/Ax/issues/363. This functionality is in beta-release and your feedback will be of great help to us!  ","version":"Next","tagName":"h2"},{"title":"3. I know which Botorch Model and AcquisitionFunction I'd like to combine in Ax. How do set this up?​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#3-i-know-which-botorch-model-and-acquisitionfunction-id-like-to-combine-in-ax-how-do-set-this-up","content":" ","version":"Next","tagName":"h2"},{"title":"3a. Making a Surrogate from BoTorch Model:​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#3a-making-a-surrogate-from-botorch-model","content":" Most models should work with base Surrogate in Ax, except for BoTorch ModelListGP.ModelListGP is a special case because its purpose is to combine multiple sub-models into a single Model in BoTorch. It is most commonly used for multi-objective and constrained optimization. Whether or not ModelListGP is used is determined automatically based on the Model class and the data being used via theax.models.torch.botorch_modular.utils.use_model_list function.  If your Model is not a ModelListGP, the steps to set it up as a Surrogate are:  Implement aconstruct_inputs class method. The purpose of this method is to produce arguments to a particular model from a standardized set of inputs passed to BoTorch Model-s fromSurrogate.constructin Ax. It should accept training data in form of a SupervisedDataset container and optionally other keyword arguments and produce a dictionary of arguments to__init__ of the Model. SeeSingleTaskMultiFidelityGP.construct_inputsfor an example.Pass any additional needed keyword arguments for the Model constructor (that cannot be constructed from the training data and other arguments to construct_inputs) viamodel_options argument to Surrogate.  from botorch.models.model import Model from botorch.utils.datasets import SupervisedDataset class MyModelClass(Model): ... # Implementation of `MyModelClass` @classmethod def construct_inputs( cls, training_data: SupervisedDataset, **kwargs ) -&gt; Dict[str, Any]: fidelity_features = kwargs.get(&quot;fidelity_features&quot;) if fidelity_features is None: raise ValueError(f&quot;Fidelity features required for {cls.__name__}.&quot;) return { **super().construct_inputs(training_data=training_data, **kwargs), &quot;fidelity_features&quot;: fidelity_features, } surrogate = Surrogate( botorch_model_class=MyModelClass, # Must implement `construct_inputs` # Optional dict of additional keyword arguments to `MyModelClass` model_options={}, )   NOTE: if you run into a case where base Surrogate does not work with your BoTorchModel, please let us know in this Github issue:https://github.com/facebook/Ax/issues/363, so we can find the right solution and augment this tutorial.  ","version":"Next","tagName":"h3"},{"title":"3B. Using an arbitrary BoTorch AcquisitionFunction in Ax​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#3b-using-an-arbitrary-botorch-acquisitionfunction-in-ax","content":" Steps to set up any AcquisitionFunction in Ax are:  Define an input constructor function. The purpose of this method is to produce arguments to a acquisition function from a standardized set of inputs passed to BoTorch AcquisitionFunction-s from Acquisition.__init__ in Ax. For example, seeconstruct_inputs_qEHVI, which creates a fairly complex set of arguments needed byqExpectedHypervolumeImprovement –– a popular multi-objective optimization acquisition function offered in Ax and BoTorch. For more examples, see this collection in BoTorch:botorch/acquisition/input_constructors.py Note that the new input constructor needs to be decorated with@acqf_input_constructor(AcquisitionFunctionClass) to register it. (Optional) If a given AcquisitionFunction requires specific options passed to the BoTorch optimize_acqf, it's possible to add default optimizer options for a givenAcquisitionFunction to avoid always manually passing them viaacquisition_options.Specify the BoTorch AcquisitionFunction class as botorch_acqf_class toBoTorchModel(Optional) Pass any additional keyword arguments to acquisition function constructor or to the optimizer function via acquisition_options argument to BoTorchModel.  from ax.models.torch.botorch_modular.optimizer_argparse import optimizer_argparse from botorch.acquisition.acquisition import AcquisitionFunction from botorch.acquisition.input_constructors import acqf_input_constructor, MaybeDict from botorch.utils.datasets import SupervisedDataset from torch import Tensor class MyAcquisitionFunctionClass(AcquisitionFunction): ... # Actual contents of the acquisition function class. # 1. Add input constructor @acqf_input_constructor(MyAcquisitionFunctionClass) def construct_inputs_my_acqf( model: Model, training_data: MaybeDict[SupervisedDataset], objective_thresholds: Tensor, **kwargs: Any, ) -&gt; Dict[str, Any]: pass # 2. Register default optimizer options @optimizer_argparse.register(MyAcquisitionFunctionClass) def _argparse_my_acqf( acqf: MyAcquisitionFunctionClass, sequential: bool = True ) -&gt; dict: return { &quot;sequential&quot;: sequential } # default to sequentially optimizing batches of queries # 3-4. Specifying `botorch_acqf_class` and `acquisition_options` BoTorchModel( botorch_acqf_class=MyAcquisitionFunctionClass, acquisition_options={ &quot;alpha&quot;: 10**-6, # The sub-dict by the key &quot;optimizer_options&quot; can be passed # to propagate options to `optimize_acqf`, used in # `Acquisition.optimize`, to add/override the default # optimizer options registered above. &quot;optimizer_options&quot;: {&quot;sequential&quot;: False}, }, )   Out: BoTorchModel  See section 2A for combining the resulting Surrogate instance and Acquisition type into a BoTorchModel. You can also leverage Models.BOTORCH_MODULAR for ease of use; more on it in section 4 below or in section 1 quick-start example.  ","version":"Next","tagName":"h3"},{"title":"4. Using Models.BOTORCH_MODULAR​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#4-using-modelsbotorch_modular","content":" To simplify the instantiation of an Ax ModelBridge and its undelying Model, Ax provides aModels registry enum. When calling entries of that enum (e.g. Models.BOTORCH_MODULAR(experiment, data)), the inputs are automatically distributed between a Model and a ModelBridge for a given setup. A call to a Model enum member yields a model bridge with an underlying model, ready for use to generate candidates.  Here we use Models.BOTORCH_MODULAR to set up a model with all-default subcomponents:  model_bridge_with_GPEI = Models.BOTORCH_MODULAR( experiment=experiment, data=data, ) model_bridge_with_GPEI.gen(1)   Out: [INFO 09-29 17:02:46] ax.modelbridge.transforms.standardize_y: Outcome branin is constant, within tolerance.  Out: GeneratorRun(1 arms, total weight 1.0)  model_bridge_with_GPEI.model.botorch_acqf_class   Out: botorch.acquisition.logei.qLogNoisyExpectedImprovement  model_bridge_with_GPEI.model.surrogate.botorch_model_class   We can use the same Models.BOTORCH_MODULAR to set up a model for multi-objective optimization:  model_bridge_with_EHVI = Models.BOTORCH_MODULAR( experiment=get_branin_experiment_with_multi_objective( has_objective_thresholds=True, with_batch=True ), data=get_branin_data_multi_objective(), ) model_bridge_with_EHVI.gen(1)   Out: [INFO 09-29 17:02:48] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:48] ax.modelbridge.transforms.standardize_y: Outcome branin_a is constant, within tolerance.  Out: [INFO 09-29 17:02:48] ax.modelbridge.transforms.standardize_y: Outcome branin_b is constant, within tolerance.  Out: GeneratorRun(1 arms, total weight 1.0)  model_bridge_with_EHVI.model.botorch_acqf_class   Out: botorch.acquisition.multi_objective.logei.qLogNoisyExpectedHypervolumeImprovement  model_bridge_with_EHVI.model.surrogate.botorch_model_class   Furthermore, the quick-start example at the top of this tutorial shows how to specify surrogate and acquisition subcomponents to Models.BOTORCH_MODULAR.  ","version":"Next","tagName":"h2"},{"title":"5. Utilizing BoTorchModel in generation strategies​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#5-utilizing-botorchmodel-in-generation-strategies","content":" Generation strategy is a key concept in Ax, enabling use of Service API (a.k.a.AxClient) and many other higher-level abstractions. A GenerationStrategy allows to chain multiple models in Ax and thereby automate candidate generation. Refer to the &quot;Generation Strategy&quot; tutorial for more detail in generation strategies.  An example generation stategy with the modular BoTorchModel would look like this:  from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy from ax.modelbridge.modelbridge_utils import get_pending_observation_features gs = GenerationStrategy( steps=[ GenerationStep( # Initialization step # Which model to use for this step model=Models.SOBOL, # How many generator runs (each of which is then made a trial) # to produce with this step num_trials=5, # How many trials generated from this step must be `COMPLETED` # before the next one min_trials_observed=5, ), GenerationStep( # BayesOpt step model=Models.BOTORCH_MODULAR, # No limit on how many generator runs will be produced num_trials=-1, model_kwargs={ # Kwargs to pass to `BoTorchModel.__init__` &quot;surrogate&quot;: Surrogate(SingleTaskGP), &quot;botorch_acqf_class&quot;: qLogNoisyExpectedImprovement, }, ), ] )   Set up an experiment and generate 10 trials in it, adding synthetic data to experiment after each one:  experiment = get_branin_experiment(minimize=True) assert len(experiment.trials) == 0 experiment.search_space   Out: [INFO 09-29 17:02:49] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[])  ","version":"Next","tagName":"h2"},{"title":"5a. Specifying pending_observations​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#5a-specifying-pending_observations","content":" Note that it's important to specify pending observations to the call to gen to avoid getting the same points re-suggested. Without pending_observations argument, Ax models are not aware of points that should be excluded from generation. Points are considered &quot;pending&quot; when they belong to STAGED, RUNNING, or ABANDONED trials (with the latter included so model does not re-suggest points that are considered &quot;bad&quot; and should not be re-suggested).  If the call to get_pending_observation_features becomes slow in your setup (since it performs data-fetching etc.), you can opt forget_pending_observation_features_based_on_trial_status (also fromax.modelbridge.modelbridge_utils), but note the limitations of that utility (detailed in its docstring).  for _ in range(10): # Produce a new generator run and attach it to experiment as a trial generator_run = gs.gen( experiment=experiment, n=1, pending_observations=get_pending_observation_features(experiment=experiment), ) trial = experiment.new_trial(generator_run) # Mark the trial as 'RUNNING' so we can mark it 'COMPLETED' later trial.mark_running(no_runner_required=True) # Attach data for the new trial and mark it 'COMPLETED' experiment.attach_data(get_branin_data(trials=[trial])) trial.mark_completed() print(f&quot;Completed trial #{trial.index}, suggested by {generator_run._model_key}.&quot;)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e)) /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. warn(&quot;Encountered exception in computing model fit quality: &quot; + str(e))  Out: Completed trial #0, suggested by Sobol. Completed trial #1, suggested by Sobol. Completed trial #2, suggested by Sobol. Completed trial #3, suggested by Sobol. Completed trial #4, suggested by Sobol.  Out: Completed trial #5, suggested by BoTorch.  Out: Completed trial #6, suggested by BoTorch.  Out: Completed trial #7, suggested by BoTorch.  Out: Completed trial #8, suggested by BoTorch.  Out: Completed trial #9, suggested by BoTorch.  Now we examine the experiment and observe the trials that were added to it and produced by the generation strategy:  exp_to_df(experiment)   Out: [WARNING 09-29 17:02:54] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tbranin\tx1\tx20\t0\t0_0\tCOMPLETED\tSobol\t25.6242\t-0.723826\t4.31631 1\t1\t1_0\tCOMPLETED\tSobol\t114.994\t6.5899\t10.9117 2\t2\t2_0\tCOMPLETED\tSobol\t21.8386\t6.21173\t2.60204 3\t3\t3_0\tCOMPLETED\tSobol\t16.701\t-2.04487\t13.1241 4\t4\t4_0\tCOMPLETED\tSobol\t137.49\t-3.2435\t0.814772 5\t5\t5_0\tCOMPLETED\tBoTorch\t7.39147\t3.70754\t4.21946 6\t6\t6_0\tCOMPLETED\tBoTorch\t18.3949\t-5\t14.8059 7\t7\t7_0\tCOMPLETED\tBoTorch\t3.27782\t10\t4.15824 8\t8\t8_0\tCOMPLETED\tBoTorch\t10.8335\t8.19733\t3.65068 9\t9\t9_0\tCOMPLETED\tBoTorch\t9.8066\t9.14436\t5.25422  ","version":"Next","tagName":"h2"},{"title":"6. Customizing a Surrogate or Acquisition​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#6-customizing-a-surrogate-or-acquisition","content":" We expect the base Surrogate and Acquisition classes to work with most BoTorch components, but there could be a case where you would need to subclass one of aforementioned abstractions to handle a given BoTorch component. If you run into a case like this, feel free to open an issue on ourGithub issues page –– it would be very useful for us to know  One such example would be a need for a custom MCAcquisitionObjective or posterior transform. To subclass Acquisition accordingly, one would override theget_botorch_objective_and_transform method:  from botorch.acquisition.objective import MCAcquisitionObjective, PosteriorTransform from botorch.acquisition.risk_measures import RiskMeasureMCObjective class CustomObjectiveAcquisition(Acquisition): def get_botorch_objective_and_transform( self, botorch_acqf_class: Type[AcquisitionFunction], model: Model, objective_weights: Tensor, objective_thresholds: Optional[Tensor] = None, outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None, X_observed: Optional[Tensor] = None, risk_measure: Optional[RiskMeasureMCObjective] = None, ) -&gt; Tuple[Optional[MCAcquisitionObjective], Optional[PosteriorTransform]]: ... # Produce the desired `MCAcquisitionObjective` and `PosteriorTransform` instead of the default   Then to use the new subclass in BoTorchModel, just specify acquisition_classargument along with botorch_acqf_class (to BoTorchModel directly or toModels.BOTORCH_MODULAR, which just passes the relevant arguments to BoTorchModelunder the hood, as discussed in section 4):  Models.BOTORCH_MODULAR( experiment=experiment, data=data, acquisition_class=CustomObjectiveAcquisition, botorch_acqf_class=MyAcquisitionFunctionClass, )   Out: [INFO 09-29 17:02:54] ax.modelbridge.transforms.standardize_y: Outcome branin is constant, within tolerance.  Out: TorchModelBridge(model=BoTorchModel)  To use a custom Surrogate subclass, pass the surrogate argument of that type:  Models.BOTORCH_MODULAR( experiment=experiment, data=data, surrogate=CustomSurrogate(botorch_model_class=MyModelClass), )     ","version":"Next","tagName":"h2"},{"title":"Appendix 1: Methods available on BoTorchModel​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#appendix-1-methods-available-on-botorchmodel","content":" Note that usually all these methods are used through ModelBridge –– a convertion and transformation layer that adapts Ax abstractions to inputs required by the given model.  Core methods on BoTorchModel:  fit selects a surrogate if needed and fits the surrogate model to data viaSurrogate.fit,predict estimates metric values at a given point via Surrogate.predict,gen instantiates an acquisition function via Acquisition.__init__ and optimizes it to generate candidates.  Other methods on BoTorchModel:  update updates surrogate model with training data and optionally reoptimizes model parameters via Surrogate.update,cross_validate re-fits the surrogate model to subset of training data and makes predictions for test data,evaluate_acquisition_function instantiates an acquisition function and evaluates it for a given point.    ","version":"Next","tagName":"h2"},{"title":"Appendix 2: Default surrogate models and acquisition functions​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#appendix-2-default-surrogate-models-and-acquisition-functions","content":" By default, the chosen surrogate model will be:  if fidelity parameters are present in search space: SingleTaskMultiFidelityGP,if task parameters are present: a set of MultiTaskGP wrapped in a ModelListGP and each modeling one task,SingleTaskGP otherwise.  The chosen acquisition function will be:  for multi-objective settings: qLogExpectedHypervolumeImprovement,for single-objective settings: qLogNoisyExpectedImprovement.    ","version":"Next","tagName":"h2"},{"title":"Appendix 3: Handling storage errors that arise from objects that don't have serialization logic in A​","type":1,"pageTitle":"Setup and Usage of BoTorch Models in Ax","url":"/Ax/docs/tutorials/modular_botax/#appendix-3-handling-storage-errors-that-arise-from-objects-that-dont-have-serialization-logic-in-a","content":" Attempting to store a generator run produced via Models.BOTORCH_MODULAR instance that included options without serization logic with will produce an error like:&quot;Object &lt;SomeAcquisitionOption object&gt; passed to 'object_to_json' (of type &lt;class SomeAcquisitionOption'&gt;) is not registered with a corresponding encoder in ENCODER_REGISTRY.&quot;  The two options for handling this error are:  disabling storage of BoTorchModel's options by passingno_model_options_storage=True to Models.BOTORCH_MODULAR(...) call –– this will prevent model options from being stored on the generator run, so a generator run can be saved but cannot be used to restore the model that produced it,specifying serialization logic for a given object that needs to occur among theModel or AcquisitionFunction options. Tutorial for this is in the works, but in the meantime you canpost an issue on the Ax GitHub to get help with this. ","version":"Next","tagName":"h2"},{"title":"Using Ax for Human-in-the-loop Experimentation¶","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/human_in_the_loop/","content":"","keywords":"","version":"Next"},{"title":"Experiment Setup​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#experiment-setup","content":" For this tutorial, we will assume our experiment has already been created.  import inspect import os from ax import ( Data, Metric, OptimizationConfig, Objective, OutcomeConstraint, ComparisonOp, json_load, ) from ax.modelbridge.cross_validation import cross_validate from ax.modelbridge.factory import get_GPEI from ax.plot.diagnostic import tile_cross_validation from ax.plot.scatter import plot_multiple_metrics, tile_fitted from ax.utils.notebook.plotting import render, init_notebook_plotting import pandas as pd init_notebook_plotting()   Out: [INFO 09-30 17:08:16] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.  Out: [INFO 09-30 17:08:16] ax.utils.notebook.plotting: Please see (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering) if visualizations are not rendering.  NOTE: The path below assumes the tutorial is being run from the root directory of the Ax package. This is needed since the jupyter notebooks may change active directory during runtime, making it tricky to find the file in a consistent way.  curr_dir = os.path.join(os.getcwd(), &quot;tutorials&quot;, &quot;human_in_the_loop&quot;) experiment = json_load.load_experiment(os.path.join(curr_dir, &quot;hitl_exp.json&quot;))   ","version":"Next","tagName":"h2"},{"title":"Initial Sobol Trial​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#initial-sobol-trial","content":" Bayesian Optimization experiments almost always begin with a set of random points. In this experiment, these points were chosen via a Sobol sequence, accessible via theModelBridge factory.  A collection of points run and analyzed together form a BatchTrial. A Trial object provides metadata pertaining to the deployment of these points, including details such as when they were deployed, and the current status of their experiment.  Here, we see an initial experiment has finished running (COMPLETED status).  experiment.trials[0]   Out: BatchTrial(experiment_name='human_in_the_loop_tutorial', index=0, status=TrialStatus.COMPLETED)  experiment.trials[0].time_created   Out: datetime.datetime(2019, 3, 29, 18, 10, 6)  # Number of arms in first experiment, including status_quo len(experiment.trials[0].arms)   Out: 65  # Sample arm configuration experiment.trials[0].arms[0]   Out: Arm(name='0_0', parameters={'x_excellent': 0.9715802669525146, 'x_good': 0.8615524768829346, 'x_moderate': 0.7668091654777527, 'x_poor': 0.34871453046798706, 'x_unknown': 0.7675797343254089, 'y_excellent': 2.900710028409958, 'y_good': 1.5137152910232545, 'y_moderate': 0.6775947093963622, 'y_poor': 0.4974367544054985, 'y_unknown': 1.0852564811706542, 'z_excellent': 517803.49761247635, 'z_good': 607874.5171427727, 'z_moderate': 1151881.2023103237, 'z_poor': 2927449.2621421814, 'z_unknown': 2068407.6935052872})  ","version":"Next","tagName":"h3"},{"title":"Experiment Analysis​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#experiment-analysis","content":" Optimization Config  An important construct for analyzing an experiment is an OptimizationConfig. An OptimizationConfig contains an objective, and outcome constraints. Experiment's can have a default OptimizationConfig, but models can also take an OptimizationConfig as input independent of the default.  Objective: A metric to optimize, along with a direction to optimize (default: maximize)  Outcome Constraint: A metric to constrain, along with a constraint direction (&lt;= or &gt;=), as well as a bound.  Let's start with a simple OptimizationConfig. By default, our objective metric will be maximized, but can be minimized by setting the minimize flag. Our outcome constraint will, by default, be evaluated as a relative percentage change. This percentage change is computed relative to the experiment's status quo arm.  experiment.status_quo   Out: Arm(name='status_quo', parameters={'x_excellent': 0, 'x_good': 0, 'x_moderate': 0, 'x_poor': 0, 'x_unknown': 0, 'y_excellent': 1, 'y_good': 1, 'y_moderate': 1, 'y_poor': 1, 'y_unknown': 1, 'z_excellent': 1000000, 'z_good': 1000000, 'z_moderate': 1000000, 'z_poor': 1000000, 'z_unknown': 1000000})  objective_metric = Metric(name=&quot;metric_1&quot;) constraint_metric = Metric(name=&quot;metric_2&quot;) experiment.optimization_config = OptimizationConfig( objective=Objective(objective_metric, minimize=False), outcome_constraints=[ OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=5), ], )   Data  Another critical piece of analysis is data itself! Ax data follows a standard format, shown below. This format is imposed upon the underlying data structure, which is a Pandas DataFrame.  A key set of fields are required for all data, for use with Ax models.  It's a good idea to double check our data before fitting models -- let's make sure all of our expected metrics and arms are present.  data = Data(pd.read_json(os.path.join(curr_dir, &quot;hitl_data.json&quot;))) data.df.head()   \tarm_name\tmetric_name\tmean\tsem\ttrial_index\tstart_time\tend_time\tn0\t0_1\tmetric_1\t495.763\t2.62164\t0\t2019-03-30\t2019-04-03\t1599994 1\t0_23\tmetric_1\t524.368\t2.73165\t0\t2019-03-30\t2019-04-03\t1596356 2\t0_14\tmetric_2\t21.4602\t0.069457\t0\t2019-03-30\t2019-04-03\t1600182 3\t0_53\tmetric_2\t21.4374\t0.069941\t0\t2019-03-30\t2019-04-03\t1601081 4\t0_53\tmetric_1\t548.388\t2.89349\t0\t2019-03-30\t2019-04-03\t1601081  data.df[&quot;arm_name&quot;].unique()   Out: array(['0_1', '0_23', '0_14', '0_53', '0_0', '0_54', '0_55', '0_56', '0_27', '0_57', '0_58', '0_13', '0_59', '0_6', '0_60', '0_61', '0_62', '0_63', '0_7', '0_28', '0_15', '0_16', '0_17', '0_18', '0_19', '0_29', '0_2', '0_20', '0_21', '0_22', '0_3', '0_30', '0_8', '0_10', '0_31', '0_24', '0_32', '0_33', '0_34', '0_35', '0_36', '0_37', '0_38', '0_9', '0_39', '0_4', '0_25', '0_11', '0_40', '0_41', '0_42', '0_43', '0_44', '0_45', 'status_quo', '0_46', '0_47', '0_48', '0_26', '0_49', '0_12', '0_5', '0_50', '0_51', '0_52'], dtype=object)  data.df[&quot;metric_name&quot;].unique()   Out: array(['metric_1', 'metric_2'], dtype=object)  Search Space  The final component necessary for human-in-the-loop optimization is a SearchSpace. A SearchSpace defines the feasible region for our parameters, as well as their types.  Here, we have both parameters and a set of constraints on those parameters.  Without a SearchSpace, our models are unable to generate new candidates. By default, the models will read the search space off of the experiment, when they are told to generate candidates. SearchSpaces can also be specified by the user at this time. Sometimes, the first round of an experiment is too restrictive--perhaps the experimenter was too cautious when defining their initial ranges for exploration! In this case, it can be useful to generate candidates from new, expanded search spaces, beyond that specified in the experiment.  experiment.search_space.parameters   Out: {'x_excellent': RangeParameter(name='x_excellent', parameter_type=FLOAT, range=[0.0, 1.0]), 'x_good': RangeParameter(name='x_good', parameter_type=FLOAT, range=[0.0, 1.0]), 'x_moderate': RangeParameter(name='x_moderate', parameter_type=FLOAT, range=[0.0, 1.0]), 'x_poor': RangeParameter(name='x_poor', parameter_type=FLOAT, range=[0.0, 1.0]), 'x_unknown': RangeParameter(name='x_unknown', parameter_type=FLOAT, range=[0.0, 1.0]), 'y_excellent': RangeParameter(name='y_excellent', parameter_type=FLOAT, range=[0.1, 3.0]), 'y_good': RangeParameter(name='y_good', parameter_type=FLOAT, range=[0.1, 3.0]), 'y_moderate': RangeParameter(name='y_moderate', parameter_type=FLOAT, range=[0.1, 3.0]), 'y_poor': RangeParameter(name='y_poor', parameter_type=FLOAT, range=[0.1, 3.0]), 'y_unknown': RangeParameter(name='y_unknown', parameter_type=FLOAT, range=[0.1, 3.0]), 'z_excellent': RangeParameter(name='z_excellent', parameter_type=FLOAT, range=[50000.0, 5000000.0]), 'z_good': RangeParameter(name='z_good', parameter_type=FLOAT, range=[50000.0, 5000000.0]), 'z_moderate': RangeParameter(name='z_moderate', parameter_type=FLOAT, range=[50000.0, 5000000.0]), 'z_poor': RangeParameter(name='z_poor', parameter_type=FLOAT, range=[50000.0, 5000000.0]), 'z_unknown': RangeParameter(name='z_unknown', parameter_type=FLOAT, range=[50000.0, 5000000.0])}  experiment.search_space.parameter_constraints   Out: [OrderConstraint(x_poor &lt;= x_moderate), OrderConstraint(x_moderate &lt;= x_good), OrderConstraint(x_good &lt;= x_excellent), OrderConstraint(y_poor &lt;= y_moderate), OrderConstraint(y_moderate &lt;= y_good), OrderConstraint(y_good &lt;= y_excellent)]  ","version":"Next","tagName":"h2"},{"title":"Model Fit​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#model-fit","content":" Fitting BoTorch's GPEI will allow us to predict new candidates based on our first Sobol batch. Here, we make use of the default settings for GP-EI defined in the ModelBridge factory.  gp = get_GPEI( experiment=experiment, data=data, )   We can validate the model fits using cross validation, shown below for each metric of interest. Here, our model fits leave something to be desired--the tail ends of each metric are hard to model. In this situation, there are three potential actions to take:  Increase the amount of traffic in this experiment, to reduce the measurement noise.Increase the number of points run in the random batch, to assist the GP in covering the space.Reduce the number of parameters tuned at one time.  However, away from the tail effects, the fits do show a strong correlations, so we will proceed with candidate generation.  cv_result = cross_validate(gp) render(tile_cross_validation(cv_result))   loading...  The parameters from the initial batch have a wide range of effects on the metrics of interest, as shown from the outcomes from our fitted GP model.  render(tile_fitted(gp, rel=True))   loading...  METRIC_X_AXIS = &quot;metric_1&quot; METRIC_Y_AXIS = &quot;metric_2&quot; render( plot_multiple_metrics( gp, metric_x=METRIC_X_AXIS, metric_y=METRIC_Y_AXIS, ) )   loading...  ","version":"Next","tagName":"h3"},{"title":"Candidate Generation​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#candidate-generation","content":" With our fitted GPEI model, we can optimize EI (Expected Improvement) based on any optimization config. We can start with our initial optimization config, and aim to simply maximize the playback smoothness, without worrying about the constraint on quality.  unconstrained = gp.gen( n=3, optimization_config=OptimizationConfig( objective=Objective(objective_metric, minimize=False), ), )   Let's plot the tradeoffs again, but with our new arms.  render( plot_multiple_metrics( gp, metric_x=METRIC_X_AXIS, metric_y=METRIC_Y_AXIS, generator_runs_dict={ &quot;unconstrained&quot;: unconstrained, }, ) )   loading...  ","version":"Next","tagName":"h3"},{"title":"Change Objectives​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#change-objectives","content":" With our unconstrained optimization, we generate some candidates which are pretty promising with respect to our objective! However, there is a clear regression in our constraint metric, above our initial 5% desired constraint. Let's add that constraint back in.  constraint_5 = OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=5) constraint_5_results = gp.gen( n=3, optimization_config=OptimizationConfig( objective=Objective(objective_metric, minimize=False), outcome_constraints=[constraint_5] ), )   This yields a GeneratorRun, which contains points according to our specified optimization config, along with metadata about how the points were generated. Let's plot the tradeoffs in these new points.  from ax.plot.scatter import plot_multiple_metrics render( plot_multiple_metrics( gp, metric_x=METRIC_X_AXIS, metric_y=METRIC_Y_AXIS, generator_runs_dict={&quot;constraint_5&quot;: constraint_5_results}, ) )   loading...  It is important to note that the treatment of constraints in GP EI is probabilistic. The acquisition function weights our objective by the probability that each constraint is feasible. Thus, we may allow points with a very small probability of violating the constraint to be generated, as long as the chance of the points increasing our objective is high enough.  You can see above that the point estimate for each point is significantly below a 5% increase in the constraint metric, but that there is uncertainty in our prediction, and the tail probabilities do include probabilities of small regressions beyond 5%.  constraint_1 = OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=1) constraint_1_results = gp.gen( n=3, optimization_config=OptimizationConfig( objective=Objective(objective_metric, minimize=False), outcome_constraints=[constraint_1], ), )   render( plot_multiple_metrics( gp, metric_x=METRIC_X_AXIS, metric_y=METRIC_Y_AXIS, generator_runs_dict={ &quot;constraint_1&quot;: constraint_1_results, }, ) )   loading...  Finally, let's view all three sets of candidates together.  render( plot_multiple_metrics( gp, metric_x=METRIC_X_AXIS, metric_y=METRIC_Y_AXIS, generator_runs_dict={ &quot;unconstrained&quot;: unconstrained, &quot;loose_constraint&quot;: constraint_5_results, &quot;tight_constraint&quot;: constraint_1_results, }, ) )   loading...  ","version":"Next","tagName":"h3"},{"title":"Creating a New Trial​","type":1,"pageTitle":"Using Ax for Human-in-the-loop Experimentation¶","url":"/Ax/docs/tutorials/human_in_the_loop/#creating-a-new-trial","content":" Having done the analysis and candidate generation for three different optimization configs, we can easily create a new BatchTrial which combines the candidates from these three different optimizations. Each set of candidates looks promising -- the point estimates are higher along both metric values than in the previous batch. However, there is still a good bit of uncertainty in our predictions. It is hard to choose between the different constraint settings without reducing this noise, so we choose to run a new trial with all three constraint settings. However, we're generally convinced that the tight constraint is too conservative. We'd still like to reduce our uncertainty in that region, but we'll only take one arm from that set.  # We can add entire generator runs, when constructing a new trial. trial = ( experiment.new_batch_trial() .add_generator_run(unconstrained) .add_generator_run(constraint_5_results) ) # Or, we can hand-pick arms. trial.add_arm(constraint_1_results.arms[0])   Out: BatchTrial(experiment_name='human_in_the_loop_tutorial', index=1, status=TrialStatus.CANDIDATE)  The arms are combined into a single trial, along with the status_quo arm. Their generator can be accessed from the trial as well.  experiment.trials[1].arms   Out: [Arm(name='1_0', parameters={'x_excellent': 0.4887085137700609, 'x_good': 0.0, 'x_moderate': 0.0, 'x_poor': 0.0, 'x_unknown': 0.46554974801730076, 'y_excellent': 3.0, 'y_good': 1.3362655882996775, 'y_moderate': 1.3362655882996806, 'y_poor': 0.5401686736617711, 'y_unknown': 2.999999999999998, 'z_excellent': 5000000.0, 'z_good': 3739540.136491846, 'z_moderate': 3742794.879014561, 'z_poor': 3245908.299134002, 'z_unknown': 4999999.999999999}), Arm(name='1_1', parameters={'x_excellent': 0.1969157557787065, 'x_good': 4.031091266722689e-17, 'x_moderate': 0.0, 'x_poor': 2.3429438199037504e-17, 'x_unknown': 1.0, 'y_excellent': 2.292139264615643, 'y_good': 2.016329618630986, 'y_moderate': 0.10000000000000046, 'y_poor': 0.10000000000000082, 'y_unknown': 3.0, 'z_excellent': 5000000.0, 'z_good': 2909455.7848198484, 'z_moderate': 50000.000000000204, 'z_poor': 1664591.9261659516, 'z_unknown': 1320771.5945464878}), Arm(name='1_2', parameters={'x_excellent': 0.37778865473076795, 'x_good': 1.2639566896823953e-09, 'x_moderate': 1.9999488166376815e-10, 'x_poor': 2.056470204292342e-10, 'x_unknown': 0.9999999985652391, 'y_excellent': 2.999999999337196, 'y_good': 2.325559007590524, 'y_moderate': 2.325559006067812, 'y_poor': 0.46107899165156674, 'y_unknown': 3.0, 'z_excellent': 4999999.997775518, 'z_good': 4175152.179426201, 'z_moderate': 4999999.996229719, 'z_poor': 4999999.965354999, 'z_unknown': 5000000.0}), Arm(name='1_3', parameters={'x_excellent': 0.5230690509285771, 'x_good': 0.5230690504696931, 'x_moderate': 0.16004610492118335, 'x_poor': 1.274047909561916e-12, 'x_unknown': 1.0, 'y_excellent': 3.0, 'y_good': 1.9022176369545807, 'y_moderate': 0.5842955756909501, 'y_poor': 0.10000000009953319, 'y_unknown': 3.0, 'z_excellent': 4999999.999985329, 'z_good': 2716860.7045556875, 'z_moderate': 50000.0003598255, 'z_poor': 4683748.900338292, 'z_unknown': 3327881.951435663}), Arm(name='1_4', parameters={'x_excellent': 0.46823873325713233, 'x_good': 0.46823873325713283, 'x_moderate': 7.205984753965287e-16, 'x_poor': 6.427166701657783e-17, 'x_unknown': 0.9999999999999982, 'y_excellent': 2.999999999999997, 'y_good': 0.7376069478400028, 'y_moderate': 0.7376069478400025, 'y_poor': 0.7376069478400036, 'y_unknown': 2.999999999999996, 'z_excellent': 5000000.0, 'z_good': 3518537.398914982, 'z_moderate': 5000000.0, 'z_poor': 4999999.999999992, 'z_unknown': 4999999.999999984}), Arm(name='1_5', parameters={'x_excellent': 0.45279093814506016, 'x_good': 0.45279093814505106, 'x_moderate': 0.4527909381451087, 'x_poor': 0.45279093814503696, 'x_unknown': 2.468391842317501e-14, 'y_excellent': 3.0, 'y_good': 2.008477698764803, 'y_moderate': 2.0084776987649096, 'y_poor': 0.7108703597846797, 'y_unknown': 3.0, 'z_excellent': 5000000.0, 'z_good': 4296312.244769551, 'z_moderate': 5000000.0, 'z_poor': 5000000.0, 'z_unknown': 4999999.999999883}), Arm(name='1_6', parameters={'x_excellent': 0.6548530820299868, 'x_good': 0.6548530820528661, 'x_moderate': 0.6170463669447903, 'x_poor': 1.3955995491261087e-12, 'x_unknown': 1.0, 'y_excellent': 2.9999999994662283, 'y_good': 2.187720155540752, 'y_moderate': 0.33496982650898555, 'y_poor': 0.1, 'y_unknown': 2.1994854168643476, 'z_excellent': 4999999.999987346, 'z_good': 2626030.9788904227, 'z_moderate': 50000.00085672577, 'z_poor': 4999999.999414883, 'z_unknown': 667806.193477366})]  The original GeneratorRuns can be accessed from within the trial as well. This is useful for later analyses, allowing introspection of the OptimizationConfig used for generation (as well as other information, e.g. SearchSpace used for generation).  experiment.trials[1]._generator_run_structs   Out: [GeneratorRunStruct(generator_run=GeneratorRun(3 arms, total weight 3.0), weight=1.0), GeneratorRunStruct(generator_run=GeneratorRun(3 arms, total weight 3.0), weight=1.0), GeneratorRunStruct(generator_run=GeneratorRun(1 arms, total weight 1.0), weight=1.0)]  Here, we can see the unconstrained set-up used for our first set of candidates.  experiment.trials[1]._generator_run_structs[0].generator_run.optimization_config   Out: OptimizationConfig(objective=Objective(metric_name=&quot;metric_1&quot;, minimize=False), outcome_constraints=[]) ","version":"Next","tagName":"h2"},{"title":"Multi-task Bayesian Optimization","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/multi_task/","content":"","keywords":"","version":"Next"},{"title":"1. Define Metric classes​","type":1,"pageTitle":"Multi-task Bayesian Optimization","url":"/Ax/docs/tutorials/multi_task/#1-define-metric-classes","content":" For this example, the online system is optimizing a Hartmann6 function. The Metric objects for these are directly imported above. We create analagous offline versions of this metrics which are identical but have a transform applied (a piecewise linear function). We construct Metric objects for each of them.  # Create metric with artificial offline bias, for the objective # by passing the true values through a piecewise linear function. class OfflineHartmann6Metric(Hartmann6Metric): def f(self, x: np.ndarray) -&gt; float: raw_res = super().f(x) m = -0.35 if raw_res &lt; m: return (1.5 * (raw_res - m)) + m else: return (6.0 * (raw_res - m)) + m   ","version":"Next","tagName":"h2"},{"title":"2. Create experiment​","type":1,"pageTitle":"Multi-task Bayesian Optimization","url":"/Ax/docs/tutorials/multi_task/#2-create-experiment","content":" A MultiTypeExperiment is used for managing online and offline trials together. It is constructed in several steps:   Create the search space - This is done in the usual way.Specify optimization config - Also done in the usual way.Initialize Experiment - In addition to the search_space and optimization_config, specify that &quot;online&quot; is the default trial_type. This is the main trial type for which we're optimizing. Optimization metrics are defined to be for this type and new trials assume this trial type by default.Establish offline trial_type - Register the &quot;offline&quot; trial type and specify how to deploy trials of this type.Add offline metrics - Create the offline metrics and add them to the experiment. When adding the metrics, we need to specify the trial type (&quot;offline&quot;) and online metric name it is associated with so the model can link them.  Finally, because this is a synthetic benchmark problem where the true function values are known, we will also register metrics with the true (noiseless) function values for plotting below.  def get_experiment(include_true_metric=True): noise_sd = 0.1 # Observations will have this much Normal noise added to them # 1. Create simple search space for [0,1]^d, d=6 param_names = [f&quot;x{i}&quot; for i in range(6)] parameters = [ RangeParameter( name=param_names[i], parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0, ) for i in range(6) ] search_space = SearchSpace(parameters=parameters) # 2. Specify optimization config online_objective = Hartmann6Metric( &quot;objective&quot;, param_names=param_names, noise_sd=noise_sd ) opt_config = OptimizationConfig( objective=Objective(online_objective, minimize=True) ) # 3. Init experiment exp = MultiTypeExperiment( name=&quot;mt_exp&quot;, search_space=search_space, default_trial_type=&quot;online&quot;, default_runner=SyntheticRunner(), optimization_config=opt_config, ) # 4. Establish offline trial_type, and how those trials are deployed exp.add_trial_type(&quot;offline&quot;, SyntheticRunner()) # 5. Add offline metrics that provide biased estimates of the online metrics offline_objective = OfflineHartmann6Metric( &quot;offline_objective&quot;, param_names=param_names, noise_sd=noise_sd ) # Associate each offline metric with corresponding online metric exp.add_tracking_metric( metric=offline_objective, trial_type=&quot;offline&quot;, canonical_name=&quot;objective&quot; ) return exp   ","version":"Next","tagName":"h2"},{"title":"3. Vizualize the simulator bias​","type":1,"pageTitle":"Multi-task Bayesian Optimization","url":"/Ax/docs/tutorials/multi_task/#3-vizualize-the-simulator-bias","content":" These figures compare the online measurements to the offline measurements on a random set of points, for the objective metric. You can see the offline measurements are biased but highly correlated. This produces Fig. S3 from the paper.  # Generate 50 points from a Sobol sequence exp = get_experiment(include_true_metric=False) s = get_sobol(exp.search_space, scramble=False) gr = s.gen(50) # Deploy them both online and offline exp.new_batch_trial(trial_type=&quot;online&quot;, generator_run=gr).run() exp.new_batch_trial(trial_type=&quot;offline&quot;, generator_run=gr).run() # Fetch data data = exp.fetch_data() observations = observations_from_data(exp, data) # Plot the arms in batch 0 (online) vs. batch 1 (offline) render(interact_batch_comparison(observations, exp, 1, 0))   loading...  ","version":"Next","tagName":"h2"},{"title":"4. The Bayesian optimization loop​","type":1,"pageTitle":"Multi-task Bayesian Optimization","url":"/Ax/docs/tutorials/multi_task/#4-the-bayesian-optimization-loop","content":" Here we construct a Bayesian optimization loop that interleaves online and offline batches. The loop defined here is described in Algorithm 1 of the paper. We compare multi-task Bayesian optimization to regular Bayesian optimization using only online observations.  Here we measure performance over 3 repetitions of the loop. Each one takes 1-2 hours so the whole benchmark run will take several hours to complete.  # Settings for the optimization benchmark. # Number of repeated experiments, each with independent observation noise. # This should be changed to 50 to reproduce the results from the paper. if SMOKE_TEST: n_batches = 1 n_init_online = 2 n_init_offline = 2 n_opt_online = 2 n_opt_offline = 2 else: n_batches = 3 # Number of optimized BO batches n_init_online = 5 # Size of the quasirandom initialization run online n_init_offline = 20 # Size of the quasirandom initialization run offline n_opt_online = 5 # Batch size for BO selected points to be run online n_opt_offline = 20 # Batch size for BO selected to be run offline   4a. Optimization with online observations only​  For the online-only case, we run n_init_online sobol points followed by n_batchesbatches of n_opt_online points selected by the GP. This is a normal Bayesian optimization loop.  # This function runs a Bayesian optimization loop, making online observations only. def run_online_only_bo(): t1 = time.time() ### Do BO with online only ## Quasi-random initialization exp_online = get_experiment() m = get_sobol(exp_online.search_space, scramble=False) gr = m.gen(n=n_init_online) exp_online.new_batch_trial(trial_type=&quot;online&quot;, generator_run=gr).run() ## Do BO for b in range(n_batches): print(&quot;Online-only batch&quot;, b, time.time() - t1) # Fit the GP m = Models.BOTORCH_MODULAR( experiment=exp_online, data=exp_online.fetch_data(), search_space=exp_online.search_space, ) # Generate the new batch gr = m.gen( n=n_opt_online, search_space=exp_online.search_space, optimization_config=exp_online.optimization_config, ) exp_online.new_batch_trial(trial_type=&quot;online&quot;, generator_run=gr).run()   4b. Multi-task Bayesian optimization​  Here we incorporate offline observations to accelerate the optimization, while using the same total number of online observations as in the loop above. The strategy here is that outlined in Algorithm 1 of the paper.   Initialization - Run n_init_online Sobol points online, andn_init_offline Sobol points offline. Fit model - Fit an MTGP to both online and offline observations. Generate candidates - Generate n_opt_offline candidates using NEI. Launch offline batch - Run the n_opt_offline candidates offline and observe their offline metrics. Update model - Update the MTGP with the new offline observations. Select points for online batch - Select the best (maximum utility)n_opt_online of the NEI candidates, after incorporating their offline observations, and run them online. Update model and repeat - Update the model with the online observations, and repeat from step 3 for the next batch.  def get_MTGP( experiment: Experiment, data: Data, search_space: Optional[SearchSpace] = None, trial_index: Optional[int] = None, device: torch.device = torch.device(&quot;cpu&quot;), dtype: torch.dtype = torch.double, ) -&gt; TorchModelBridge: &quot;&quot;&quot;Instantiates a Multi-task Gaussian Process (MTGP) model that generates points with EI. If the input experiment is a MultiTypeExperiment then a Multi-type Multi-task GP model will be instantiated. Otherwise, the model will be a Single-type Multi-task GP. &quot;&quot;&quot; if isinstance(experiment, MultiTypeExperiment): trial_index_to_type = { t.index: t.trial_type for t in experiment.trials.values() } transforms = MT_MTGP_trans transform_configs = { &quot;TrialAsTask&quot;: {&quot;trial_level_map&quot;: {&quot;trial_type&quot;: trial_index_to_type}}, &quot;ConvertMetricNames&quot;: tconfig_from_mt_experiment(experiment), } else: # Set transforms for a Single-type MTGP model. transforms = ST_MTGP_trans transform_configs = None # Choose the status quo features for the experiment from the selected trial. # If trial_index is None, we will look for a status quo from the last # experiment trial to use as a status quo for the experiment. if trial_index is None: trial_index = len(experiment.trials) - 1 elif trial_index &gt;= len(experiment.trials): raise ValueError(&quot;trial_index is bigger than the number of experiment trials&quot;) status_quo = experiment.trials[trial_index].status_quo if status_quo is None: status_quo_features = None else: status_quo_features = ObservationFeatures( parameters=status_quo.parameters, trial_index=trial_index, # pyre-ignore[6] ) return checked_cast( TorchModelBridge, Models.ST_MTGP( experiment=experiment, search_space=search_space or experiment.search_space, data=data, transforms=transforms, transform_configs=transform_configs, torch_dtype=dtype, torch_device=device, status_quo_features=status_quo_features, ), )   # Online batches are constructed by selecting the maximum utility points from the offline # batch, after updating the model with the offline results. This function selects the max utility points according # to the MTGP predictions. def max_utility_from_GP(n, m, experiment, search_space, gr): obsf = [] for arm in gr.arms: params = deepcopy(arm.parameters) params[&quot;trial_type&quot;] = &quot;online&quot; obsf.append(ObservationFeatures(parameters=params)) # Make predictions f, cov = m.predict(obsf) # Compute expected utility u = -np.array(f[&quot;objective&quot;]) best_arm_indx = np.flip(np.argsort(u))[:n] gr_new = GeneratorRun( arms=[gr.arms[i] for i in best_arm_indx], weights=[1.0] * n, ) return gr_new # This function runs a multi-task Bayesian optimization loop, as outlined in Algorithm 1 and above. def run_mtbo(): t1 = time.time() online_trials = [] ## 1. Quasi-random initialization, online and offline exp_multitask = get_experiment() # Online points m = get_sobol(exp_multitask.search_space, scramble=False) gr = m.gen( n=n_init_online, ) tr = exp_multitask.new_batch_trial(trial_type=&quot;online&quot;, generator_run=gr) tr.run() online_trials.append(tr.index) # Offline points m = get_sobol(exp_multitask.search_space, scramble=False) gr = m.gen( n=n_init_offline, ) exp_multitask.new_batch_trial(trial_type=&quot;offline&quot;, generator_run=gr).run() ## Do BO for b in range(n_batches): print(&quot;Multi-task batch&quot;, b, time.time() - t1) # (2 / 7). Fit the MTGP m = get_MTGP( experiment=exp_multitask, data=exp_multitask.fetch_data(), search_space=exp_multitask.search_space, ) # 3. Finding the best points for the online task gr = m.gen( n=n_opt_offline, optimization_config=exp_multitask.optimization_config, fixed_features=ObservationFeatures( parameters={}, trial_index=online_trials[-1] ), ) # 4. But launch them offline exp_multitask.new_batch_trial(trial_type=&quot;offline&quot;, generator_run=gr).run() # 5. Update the model m = get_MTGP( experiment=exp_multitask, data=exp_multitask.fetch_data(), search_space=exp_multitask.search_space, ) # 6. Select max-utility points from the offline batch to generate an online batch gr = max_utility_from_GP( n=n_opt_online, m=m, experiment=exp_multitask, search_space=exp_multitask.search_space, gr=gr, ) tr = exp_multitask.new_batch_trial(trial_type=&quot;online&quot;, generator_run=gr) tr.run() online_trials.append(tr.index)   4c. Run both loops​  Run both Bayesian optimization loops and aggregate results.  runners = { &quot;GP, online only&quot;: run_online_only_bo, &quot;MTGP&quot;: run_mtbo, } for k, r in runners.items(): r()   Out: Online-only batch 0 0.0018839836120605469  Out: Online-only batch 1 4.511662006378174  Out: Online-only batch 2 8.593715190887451  Out: Multi-task batch 0 0.0032148361206054688  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated. Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:643.) /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/acquisition/cached_cholesky.py:87: RuntimeWarning: cache_root is only supported for GPyTorchModels that are not MultiTask models and don't produce a TransformedPosterior. Got a model of type &lt;class 'botorch.models.model_list_gp_regression.ModelListGP'&gt;. Setting cache_root = False.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values.  Out: Multi-task batch 1 66.65755605697632  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/acquisition/cached_cholesky.py:87: RuntimeWarning: cache_root is only supported for GPyTorchModels that are not MultiTask models and don't produce a TransformedPosterior. Got a model of type &lt;class 'botorch.models.model_list_gp_regression.ModelListGP'&gt;. Setting cache_root = False.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values.  Out: Multi-task batch 2 118.62788200378418  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/acquisition/cached_cholesky.py:87: RuntimeWarning: cache_root is only supported for GPyTorchModels that are not MultiTask models and don't produce a TransformedPosterior. Got a model of type &lt;class 'botorch.models.model_list_gp_regression.ModelListGP'&gt;. Setting cache_root = False.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/transforms/base.py:94: AxParameterWarning: Changing is_ordered to True for ChoiceParameter 'trial_type' since there are only two possible values.  References​  Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-offline experimentation. arXiv preprint arXiv:1603.09326, 2019.  Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. InAdvances in Neural Information Processing Systems 26, NIPS, pages 2004–2012, 2013. ","version":"Next","tagName":"h2"},{"title":"High-Dimensional Bayesian Optimization with SAASBO","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/saasbo/","content":"","keywords":"","version":"Next"},{"title":"Setup search space and metric​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#setup-search-space-and-metric","content":" In this simple experiment we use the Branin function embedded in a 30-dimensional space. Additional resources:  To set up a custom metric for your problem, refer to the dedicated section of the Developer API tutorial:https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics.To avoid needing to setup up custom metrics by Ax Service API:https://ax.dev/tutorials/gpei_hartmann_service.html.  search_space = SearchSpace( parameters=[ RangeParameter( name=f&quot;x{i}&quot;, parameter_type=ParameterType.FLOAT, lower=-5.0, upper=10.0 ) for i in range(25) ] + [ RangeParameter( name=f&quot;x{i + 25}&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=15.0, ) for i in range(25) ] ) optimization_config = OptimizationConfig( objective=Objective( metric=BraninMetric( name=&quot;objective&quot;, param_names=[&quot;x19&quot;, &quot;x34&quot;], # Set noise_sd=None if you want to learn the noise, set to 0.0 for no noise noise_sd=1e-4, ), minimize=True, ) )   ","version":"Next","tagName":"h2"},{"title":"Run benchmark​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#run-benchmark","content":" N_INIT = 10 BATCH_SIZE = 3 N_BATCHES = 1 if SMOKE_TEST else 10 print(f&quot;Doing {N_INIT + N_BATCHES * BATCH_SIZE} evaluations&quot;)   Out: Doing 40 evaluations  # Experiment experiment = Experiment( name=&quot;saasbo_experiment&quot;, search_space=search_space, optimization_config=optimization_config, runner=SyntheticRunner(), )   # Initial Sobol points sobol = Models.SOBOL(search_space=experiment.search_space) for _ in range(N_INIT): experiment.new_trial(sobol.gen(1)).run()   %%time # Run SAASBO data = experiment.fetch_data() for i in range(N_BATCHES): model = Models.SAASBO(experiment=experiment, data=data) generator_run = model.gen(BATCH_SIZE) trial = experiment.new_batch_trial(generator_run=generator_run) trial.run() data = Data.from_multiple_data([data, trial.fetch_data()]) new_value = trial.fetch_data().df[&quot;mean&quot;].min() print( f&quot;Iteration: {i}, Best in iteration {new_value:.3f}, Best so far: {data.df['mean'].min():.3f}&quot; )   Out: Iteration: 0, Best in iteration 10.961, Best so far: 3.963  Out: Iteration: 1, Best in iteration 10.961, Best so far: 3.963  Out: Iteration: 2, Best in iteration 1.947, Best so far: 1.947  Out: Iteration: 3, Best in iteration 0.562, Best so far: 0.562  Out: Iteration: 4, Best in iteration 13.889, Best so far: 0.562  Out: Iteration: 5, Best in iteration 0.770, Best so far: 0.562  Out: Iteration: 6, Best in iteration 0.535, Best so far: 0.535  Out: Iteration: 7, Best in iteration 0.402, Best so far: 0.402  Out: Iteration: 8, Best in iteration 0.410, Best so far: 0.402  Out: Iteration: 9, Best in iteration 10.961, Best so far: 0.402 CPU times: user 1h 5min 27s, sys: 19min 39s, total: 1h 25min 6s Wall time: 16min 3s  ","version":"Next","tagName":"h2"},{"title":"Plot results​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#plot-results","content":" SAASBO is able to find a solution close to the global optimal value of 0.398  %matplotlib inline matplotlib.rcParams.update({&quot;font.size&quot;: 16}) fig, ax = plt.subplots(figsize=(8, 6)) res_saasbo = data.df[&quot;mean&quot;] ax.plot(np.minimum.accumulate(res_saasbo), color=&quot;b&quot;, label=&quot;SAASBO&quot;) ax.plot([0, len(res_saasbo)], [0.398, 0.398], &quot;--&quot;, c=&quot;g&quot;, lw=3, label=&quot;Optimal value&quot;) ax.grid(True) ax.set_title(&quot;Branin, D=50&quot;, fontsize=20) ax.set_xlabel(&quot;Number of evaluations&quot;, fontsize=20) ax.set_xlim([0, len(res_saasbo)]) ax.set_ylabel(&quot;Best value found&quot;, fontsize=20) ax.set_ylim([0, 8]) ax.legend(fontsize=18) plt.show()     ","version":"Next","tagName":"h2"},{"title":"SAAS model fit​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#saas-model-fit","content":" We can also instantiate a SAAS model via Models.BOTORCH_MODULAR by specifying aSaasFullyBayesianSingleTaskGP as the botorch_model_class. This also gives us the option to change several Pyro-specific parameters such as num_samples andwarmup_steps.  model = Models.BOTORCH_MODULAR( experiment=experiment, data=data, surrogate=Surrogate( botorch_model_class=SaasFullyBayesianSingleTaskGP, mll_options={ &quot;num_samples&quot;: 256, # Increasing this may result in better model fits &quot;warmup_steps&quot;: 512, # Increasing this may result in better model fits }, ) )   ","version":"Next","tagName":"h2"},{"title":"Cross-validation plot​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#cross-validation-plot","content":" We have tools for cross-validation in Ax, but plotly doesn't render on Github so we make a simple plot using Matplotlib here. To use the built-in cross-validation functionality, you can do something like this:  from ax.modelbridge.cross_validation import cross_validate, compute_diagnostics from ax.plot.diagnostic import interact_cross_validation from ax.utils.notebook.plotting import render, init_notebook_plotting cv = cross_validate(model) diagnostics = compute_diagnostics(cv) init_notebook_plotting() plotconfig = interact_cross_validation(cv) render(plotconfig)   # Cross-validate model cv = cross_validate(model) y_true = np.stack([cv_.observed.data.means for cv_ in cv]).ravel() y_saas_mean = np.stack([cv_.predicted.means for cv_ in cv]).ravel() y_saas_std = np.stack( [np.sqrt(np.diag(cv_.predicted.covariance)) for cv_ in cv] ).ravel() # Cross-validation plot fig, ax = plt.subplots(1, 1, figsize=(6, 6)) min_val, max_val = -5, 120 ax.plot([min_val, max_val], [min_val, max_val], &quot;b--&quot;, lw=2) markers, caps, bars = ax.errorbar( y_true, y_saas_mean, yerr=1.96 * y_saas_std, fmt=&quot;.&quot;, capsize=4, elinewidth=2.0, ms=14, c=&quot;k&quot;, ecolor=&quot;gray&quot;, ) [bar.set_alpha(0.8) for bar in bars] [cap.set_alpha(0.8) for cap in caps] ax.set_xlim([min_val, max_val]) ax.set_ylim([min_val, max_val]) ax.set_xlabel(&quot;True value&quot;, fontsize=20) ax.set_ylabel(&quot;Predicted value&quot;, fontsize=20) ax.grid(True)     ","version":"Next","tagName":"h3"},{"title":"Lengthscales​","type":1,"pageTitle":"High-Dimensional Bayesian Optimization with SAASBO","url":"/Ax/docs/tutorials/saasbo/#lengthscales","content":" As SAASBO places strong priors on the inverse lengthscales, we only expect parameters 19 and 44 to be identified as important by the model since the other parameters have no effect. We can confirm that this is the case below as the lengthscales of parameters 19 and 44 are close to 1 with all other lengthscales being larger than 1000.  median_lengthscales = ( model.model.surrogate.model .covar_module.base_kernel.lengthscale.squeeze() .median(axis=0) .values ) for i in median_lengthscales.argsort()[:10]: print(f&quot;Parameter {i:2}) Median lengthscale = {median_lengthscales[i]:.2e}&quot;)   Out: Parameter 33) Median lengthscale = 1.20e-01 Parameter 41) Median lengthscale = 6.28e-01 Parameter 0) Median lengthscale = 7.96e-01 Parameter 19) Median lengthscale = 3.88e+01 Parameter 45) Median lengthscale = 3.90e+01 Parameter 21) Median lengthscale = 4.01e+01 Parameter 22) Median lengthscale = 4.25e+01 Parameter 1) Median lengthscale = 4.25e+01 Parameter 34) Median lengthscale = 4.29e+01 Parameter 42) Median lengthscale = 4.30e+01   ","version":"Next","tagName":"h3"},{"title":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/saasbo_nehvi/","content":"","keywords":"","version":"Next"},{"title":"This Tutorial​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#this-tutorial","content":" This tutorial will show how to use qNEHVI with fully bayesian inference for multi-objective optimization.  Multi-objective optimization (MOO) covers the case where we care about multiple outcomes in our experiment but we do not know before hand a specific weighting of those objectives (covered by ScalarizedObjective) or a specific constraint on one objective (covered by OutcomeConstraints) that will produce the best result.  The solution in this case is to find a whole Pareto frontier, a surface in outcome-space containing points that can't be improved on in every outcome. This shows us the tradeoffs between objectives that we can choose to make.  ","version":"Next","tagName":"h3"},{"title":"Problem Statement​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#problem-statement","content":" Optimize a list of M objective functions $ \\bigl(f^{(1)}( x),..., f^{(M)}( x) \\bigr)$ over a bounded search space $\\mathcal X \\subset \\mathbb R^d$.  We assume $f^{(i)}$ are expensive-to-evaluate black-box functions with no known analytical expression, and no observed gradients. For instance, a machine learning model where we're interested in maximizing accuracy and minimizing inference time, with $\\mathcal X$ the set of possible configuration spaces  ","version":"Next","tagName":"h3"},{"title":"Fully Bayesian Inference​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#fully-bayesian-inference","content":" Previous work, has shown that using a fully Bayesian treatment of GP model hyperparameters $\\boldsymbol \\theta$ can lead to improved closed loop Bayesian optimization performance [1]. Snoek et al [1] propose to use an integrated acquisition function $\\alpha_{MCMC}$ where the base acquisition function $\\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D)$ is integrated over the the posterior distribution over the hyperparameters $p({\\boldsymbol{\\theta}} | \\mathcal{D})$, where $ \\mathcal{D} = {{\\mathbf{x}}i, y_i}{i=1}^n$:  $\\alpha_{MCMC}(\\mathbf{x}, \\mathcal D) = \\int \\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D) p(\\boldsymbol \\theta | \\mathcal D) d\\boldsymbol \\theta$  Since $p({\\boldsymbol{\\theta}} | \\mathcal{D})$ typically cannot be expressed in closed-form, Markov Chain Monte-Carlo (MCMC) methods are used to draw samples from $p({\\boldsymbol{\\theta}} | \\mathcal{D})$. In this tutorial we use the NUTS sampler from the pyro package for automatic, robust fully Bayesian inference.  [1] J. Snoek, H. Larochelle, R. P. Adams, Practical Bayesian Optimization of Machine Learning Algorithms. Advances in Neural Information Processing Systems 26, 2012.  ","version":"Next","tagName":"h3"},{"title":"SAAS Priors (SAASBO)​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#saas-priors-saasbo","content":" Recently Eriksson et al [2] propose using sparse axis-aligned subspace priors for Bayesian optimization over high-dimensional search spaces. Specifically, the authors propose using a hierarchical sparsity prior consisting of a global shrinkage parameter with a Half-Cauchy prior $\\tau \\sim \\mathcal{HC}(\\beta)$, and ARD lengthscales $\\rho_d \\sim \\mathcal{HC}(\\tau)$ for $d=1, ..., D$. See [2] for details.  [2] D. Eriksson, M. Jankowiak. High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces. Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, 2021.  ","version":"Next","tagName":"h3"},{"title":"qNEHVI​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#qnehvi","content":" In this tutorial, we use qNEHVI [3] as our acquisition function for multi-objective optimization. We integrate qNEHVI over the posterior distribution of the GP hyperparameters as proposed in [4].  [3] S. Daulton, M. Balandat, E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Arxiv, 2021.  [4] D. Eriksson, P. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu, S. Zhao, A. Aly, G. Venkatesh, M. Balandat. Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. ICML AutoML Workshop, 2021.  ","version":"Next","tagName":"h3"},{"title":"Further Information​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#further-information","content":" For a deeper explanation of multi-objective optimization, please refer to the dedicated multi-objective optimization tutorial:https://ax.dev/tutorials/multiobjective_optimization.html.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#setup","content":" In this tutorial, we use Ax Developer API. Additional resources:  To learn more about the developer API, refer to the dedicated tutorial:https://ax.dev/tutorials/gpei_hartmann_developer.html.To set up a GenerationStrategy with multi-objective SAASBO (and use it in Ax Service API), follow the generation strategy tutorial:https://ax.dev/tutorials/generation_strategy.html and use Models.SAASBO for the Bayesian optimization generation step.To learn about multi-objective optimization in Ax Service API:https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API.  import os import matplotlib import numpy as np import pandas as pd import torch from ax.core.data import Data from ax.core.experiment import Experiment from ax.core.metric import Metric from ax.core.objective import MultiObjective, Objective from ax.core.optimization_config import ( MultiObjectiveOptimizationConfig, ObjectiveThreshold, ) from ax.core.parameter import ParameterType, RangeParameter from ax.core.search_space import SearchSpace from ax.metrics.noisy_function import GenericNoisyFunctionMetric from ax.modelbridge.cross_validation import compute_diagnostics, cross_validate # Analysis utilities, including a method to evaluate hypervolumes from ax.modelbridge.modelbridge_utils import observed_hypervolume # Model registry for creating multi-objective optimization models. from ax.modelbridge.registry import Models from ax.models.torch.botorch_modular.surrogate import Surrogate from ax.plot.contour import plot_contour from ax.plot.diagnostic import tile_cross_validation from ax.plot.pareto_frontier import plot_pareto_frontier from ax.plot.pareto_utils import compute_posterior_pareto_frontier from ax.runners.synthetic import SyntheticRunner from ax.service.utils.report_utils import exp_to_df # Plotting imports and initialization from ax.utils.notebook.plotting import init_notebook_plotting, render from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP from botorch.test_functions.multi_objective import DTLZ2 from botorch.utils.multi_objective.box_decompositions.dominated import ( DominatedPartitioning, ) from matplotlib import pyplot as plt from matplotlib.cm import ScalarMappable   init_notebook_plotting()   Out: [INFO 09-29 17:32:39] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.  Out: [INFO 09-29 17:32:39] ax.utils.notebook.plotting: Please see (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering) if visualizations are not rendering.  SMOKE_TEST = os.environ.get(&quot;SMOKE_TEST&quot;)   ","version":"Next","tagName":"h2"},{"title":"Load our sample 2-objective problem​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#load-our-sample-2-objective-problem","content":" d = 10 tkwargs = { &quot;dtype&quot;: torch.double, &quot;device&quot;: torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;), } problem = DTLZ2(num_objectives=2, dim=d, negate=True).to(**tkwargs)   ","version":"Next","tagName":"h3"},{"title":"Define experiment configurations​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#define-experiment-configurations","content":" ","version":"Next","tagName":"h2"},{"title":"Search Space​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#search-space","content":" search_space = SearchSpace( parameters=[ RangeParameter( name=f&quot;x{i}&quot;, lower=0, upper=1, parameter_type=ParameterType.FLOAT ) for i in range(d) ], )   ","version":"Next","tagName":"h3"},{"title":"MultiObjectiveOptimizationConfig​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#multiobjectiveoptimizationconfig","content":" To optimize multiple objective we must create a MultiObjective containing the metrics we'll optimize and MultiObjectiveOptimizationConfig (which containsObjectiveThresholds) instead of our more typical Objective and OptimizationConfig. Additional resources:  To set up a custom metric for your problem, refer to the dedicated section of the Developer API tutorial:https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics.To avoid needing to setup up custom metrics by using multi-objective optimization in Ax Service API:https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API.  We define GenericNoisyFunctionMetrics to wrap our synthetic Branin-Currin problem's outputs.  param_names = [f&quot;x{i}&quot; for i in range(d)]   def f1(x) -&gt; float: x_sorted = [x[p_name] for p_name in param_names] return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0.0, 1.0))[0]) def f2(x) -&gt; float: x_sorted = [x[p_name] for p_name in param_names] return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0.0, 1.0))[1]) metric_a = GenericNoisyFunctionMetric(&quot;a&quot;, f=f1, noise_sd=0.0, lower_is_better=False) metric_b = GenericNoisyFunctionMetric(&quot;b&quot;, f=f2, noise_sd=0.0, lower_is_better=False)   mo = MultiObjective( objectives=[Objective(metric=metric_a), Objective(metric=metric_b)], )   objective_thresholds = [ ObjectiveThreshold(metric=metric, bound=val, relative=False) for metric, val in zip(mo.metrics, problem.ref_point) ]   optimization_config = MultiObjectiveOptimizationConfig( objective=mo, objective_thresholds=objective_thresholds, )   ","version":"Next","tagName":"h3"},{"title":"Define experiment creation utilities​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#define-experiment-creation-utilities","content":" These construct our experiment, then initialize with Sobol points before we fit a Gaussian Process model to those initial points.  N_INIT = 2 * (d + 1)   def build_experiment(): experiment = Experiment( name=&quot;pareto_experiment&quot;, search_space=search_space, optimization_config=optimization_config, runner=SyntheticRunner(), ) return experiment   def initialize_experiment(experiment): sobol = Models.SOBOL(search_space=experiment.search_space) experiment.new_batch_trial(sobol.gen(N_INIT)).run() return experiment.fetch_data()   ","version":"Next","tagName":"h2"},{"title":"qNEHVI + SAASBO​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#qnehvi--saasbo","content":" Noisy expected hypervolume improvement + fully Bayesian inference with SAAS priors.  experiment = build_experiment() data = initialize_experiment(experiment)   BATCH_SIZE = 4 if SMOKE_TEST: N_BATCH = 1 num_samples = 128 warmup_steps = 256 else: N_BATCH = 10 BATCH_SIZE = 4 num_samples = 256 warmup_steps = 512   hv_list = [] model = None for i in range(N_BATCH): model = Models.BOTORCH_MODULAR( experiment=experiment, data=data, surrogate=Surrogate( botorch_model_class=SaasFullyBayesianSingleTaskGP, mll_options={ &quot;num_samples&quot;: num_samples, # Increasing this may result in better model fits &quot;warmup_steps&quot;: warmup_steps, # Increasing this may result in better model fits }, ) ) generator_run = model.gen(BATCH_SIZE) trial = experiment.new_batch_trial(generator_run=generator_run) trial.run() data = Data.from_multiple_data([data, trial.fetch_data()]) exp_df = exp_to_df(experiment) outcomes = torch.tensor(exp_df[[&quot;a&quot;, &quot;b&quot;]].values, **tkwargs) partitioning = DominatedPartitioning(ref_point=problem.ref_point, Y=outcomes) try: hv = partitioning.compute_hypervolume().item() except: hv = 0 print(&quot;Failed to compute hv&quot;) hv_list.append(hv) print(f&quot;Iteration: {i}, HV: {hv}&quot;) df = exp_to_df(experiment).sort_values(by=[&quot;trial_index&quot;]) outcomes = df[[&quot;a&quot;, &quot;b&quot;]].values   Out: [WARNING 09-29 17:33:39] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 0, HV: 0.0  Out: [WARNING 09-29 17:35:13] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 1, HV: 0.0  Out: [WARNING 09-29 17:36:42] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 2, HV: 0.0  Out: [WARNING 09-29 17:38:07] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 3, HV: 0.0  Out: [WARNING 09-29 17:40:10] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 4, HV: 0.0  Out: [WARNING 09-29 17:41:39] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 5, HV: 0.10184905275879104  Out: [WARNING 09-29 17:43:20] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 6, HV: 0.14763339627312216  Out: [WARNING 09-29 17:45:13] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 7, HV: 0.22774806529547137  Out: [WARNING 09-29 17:48:07] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 8, HV: 0.26454018953161695  Out: [WARNING 09-29 17:51:49] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:51:49] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 9, HV: 0.28197310248067803  ","version":"Next","tagName":"h2"},{"title":"Plot empirical data​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#plot-empirical-data","content":" Plot observed hypervolume, with color representing the iteration that a point was generated on.​  %matplotlib inline matplotlib.rcParams.update({&quot;font.size&quot;: 16}) fig, axes = plt.subplots(1, 1, figsize=(8, 6)) algos = [&quot;qNEHVI&quot;] train_obj = outcomes cm = matplotlib.colormaps[&quot;viridis&quot;] n_results = N_INIT + N_BATCH * BATCH_SIZE batch_number = df.trial_index.values sc = axes.scatter(train_obj[:, 0], train_obj[:, 1], c=batch_number, alpha=0.8) axes.set_title(algos[0]) axes.set_xlabel(&quot;Objective 1&quot;) axes.set_ylabel(&quot;Objective 2&quot;) norm = plt.Normalize(batch_number.min(), batch_number.max()) sm = ScalarMappable(norm=norm, cmap=cm) sm.set_array([]) fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7]) cbar = fig.colorbar(sm, cax=cbar_ax) cbar.ax.set_title(&quot;Iteration&quot;)   Out: Text(0.5, 1.0, 'Iteration')    Hypervolume statistics  The hypervolume of the space dominated by points that dominate the reference point.  Plot the results​  The plot below shows a common metric of multi-objective optimization performance when the true Pareto frontier is known: the log difference between the hypervolume of the true Pareto front and the hypervolume of the approximate Pareto front identified by qNEHVI.  iters = np.arange(1, N_BATCH + 1) log_hv_difference = np.log10(problem.max_hv - np.asarray(hv_list))[: N_BATCH + 1] fig, ax = plt.subplots(1, 1, figsize=(8, 6)) ax.plot(iters, log_hv_difference, label=&quot;qNEHVI+SAASBO&quot;, linewidth=1.5) ax.set(xlabel=&quot;Batch Iterations&quot;, ylabel=&quot;Log Hypervolume Difference&quot;) ax.legend(loc=&quot;lower right&quot;)   Out: &lt;matplotlib.legend.Legend at 0x1687f0110&gt;    ","version":"Next","tagName":"h2"},{"title":"Inspect Model fits​","type":1,"pageTitle":"Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO","url":"/Ax/docs/tutorials/saasbo_nehvi/#inspect-model-fits","content":" Here, we examine the GP model fits using the fully bayesian inference with SAAS priors. We plot the leave-one-out cross-validation below. Note: model hyperparameters are not re-sampled on each fold to reduce the runtime.  saas_model = Models.SAASBO(experiment=experiment, data=data) cv = cross_validate(model) render(tile_cross_validation(cv))   loading...  # compute out-of-sample log likelihood compute_diagnostics(cv)[&quot;Log likelihood&quot;]   Out: /Users/cristianlara/Projects/Ax-1.0/ax/utils/stats/model_fit_stats.py:199: RuntimeWarning: divide by zero encountered in divide  Out: {'a': 29.150805853849192, 'b': 32.87828773974143}  Finally, we examine the GP model fits using MAP estimation for comparison. The fully bayesian model has a higher log-likelihood than the MAP model.  map_model = Models.BOTORCH_MODULAR(experiment=experiment, data=data) map_cv = cross_validate(map_model) render(tile_cross_validation(map_cv))   loading...  # compute out-of-sample log likelihood compute_diagnostics(map_cv)[&quot;Log likelihood&quot;]   Out: /Users/cristianlara/Projects/Ax-1.0/ax/utils/stats/model_fit_stats.py:199: RuntimeWarning: divide by zero encountered in divide  Out: {'a': 34.81961564797097, 'b': 44.67622316633294}   ","version":"Next","tagName":"h2"},{"title":"Configurable closed-loop optimization with Ax Scheduler","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/scheduler/","content":"","keywords":"","version":"Next"},{"title":"Contents:​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#contents","content":" Scheduler and external systems for trial evalution –– overview of how scheduler works with an external system to run a closed-loop optimization.Set up a mock external system –– creating a dummy external system client, which will be used to illustrate a scheduler setup in this tutorial.Set up an experiment according to the mock external system –– set up a runner that deploys trials to the dummy external system from part 2 and a metric that fetches trial results from that system, then leverage those runner and metric and set up an experiment.Set up a scheduler, given an experiment. Create a scheduler subclass to poll trial status.Set up a generation strategy using an auto-selection utility. Running the optimization via Scheduler.run_n_trials.Leveraging SQL storage and experiment resumption –– resuming an experiment in one line of code.Configuring the scheduler –– overview of the many options scheduler provides to configure the closed-loop down to granular detail.Advanced functionality: Reporting results to an external system during the optimization.Using Scheduler.run_trials_and_yield_results to run the optimization via a generator method.  ","version":"Next","tagName":"h3"},{"title":"1. Scheduler and external systems for trial evaluation​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#1-scheduler-and-external-systems-for-trial-evaluation","content":" Scheduler is a closed-loop manager class in Ax that continuously deploys trial runs to an arbitrary external system in an asynchronous fashion, polls their status from that system, and leverages known trial results to generate more trials.  Key features of the Scheduler:  Maintains user-set concurrency limits for trials run in parallel, keep track of tolerated level of failed trial runs, and 'oversee' the optimization in other ways,Leverages an Ax Experiment for optimization setup (an optimization config with metrics, a search space, a runner for trial evaluations),Uses an Ax GenerationStrategy for flexible specification of an optimization algorithm used to generate new trials to run,Supports SQL storage and allows for easy resumption of stored experiments.  This scheme summarizes how the scheduler interacts with any external system used to run trial evaluations:    ","version":"Next","tagName":"h2"},{"title":"2. Set up a mock external execution system​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#2-set-up-a-mock-external-execution-system","content":" An example of an 'external system' running trial evaluations could be a remote server executing scheduled jobs, a subprocess conducting ML training runs, an engine running physics simulations, etc. For the sake of example here, let us assume a dummy external system with the following client:  from random import randint from time import time from typing import Any, Dict, NamedTuple, Union from ax.core.base_trial import TrialStatus from ax.utils.measurement.synthetic_functions import branin class MockJob(NamedTuple): &quot;&quot;&quot;Dummy class to represent a job scheduled on `MockJobQueue`.&quot;&quot;&quot; id: int parameters: Dict[str, Union[str, float, int, bool]] class MockJobQueueClient: &quot;&quot;&quot;Dummy class to represent a job queue where the Ax `Scheduler` will deploy trial evaluation runs during optimization. &quot;&quot;&quot; jobs: Dict[str, MockJob] = {} def schedule_job_with_parameters( self, parameters: Dict[str, Union[str, float, int, bool]] ) -&gt; int: &quot;&quot;&quot;Schedules an evaluation job with given parameters and returns job ID.&quot;&quot;&quot; # Code to actually schedule the job and produce an ID would go here; # using timestamp in microseconds as dummy ID for this example. job_id = int(time() * 1e6) self.jobs[job_id] = MockJob(job_id, parameters) return job_id def get_job_status(self, job_id: int) -&gt; TrialStatus: &quot;&quot;&quot; &quot;Get status of the job by a given ID. For simplicity of the example, return an Ax `TrialStatus`. &quot;&quot;&quot; job = self.jobs[job_id] # Instead of randomizing trial status, code to check actual job status # would go here. if randint(0, 3) &gt; 0: return TrialStatus.COMPLETED return TrialStatus.RUNNING def get_outcome_value_for_completed_job(self, job_id: int) -&gt; Dict[str, float]: &quot;&quot;&quot;Get evaluation results for a given completed job.&quot;&quot;&quot; job = self.jobs[job_id] # In a real external system, this would retrieve real relevant outcomes and # not a synthetic function value. return {&quot;branin&quot;: branin(job.parameters.get(&quot;x1&quot;), job.parameters.get(&quot;x2&quot;))} MOCK_JOB_QUEUE_CLIENT = MockJobQueueClient() def get_mock_job_queue_client() -&gt; MockJobQueueClient: &quot;&quot;&quot;Obtain the singleton job queue instance.&quot;&quot;&quot; return MOCK_JOB_QUEUE_CLIENT   ","version":"Next","tagName":"h2"},{"title":"3. Set up an experiment according to the mock external system​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#3-set-up-an-experiment-according-to-the-mock-external-system","content":" As mentioned above, using a Scheduler requires a fully set up experiment with metrics and a runner. Refer to the &quot;Building Blocks of Ax&quot; tutorial to learn more about those components, as here we assume familiarity with them.  The following runner and metric set up intractions between the Scheduler and the mock external system we assume:  from collections import defaultdict from typing import Iterable, Set from ax.core.base_trial import BaseTrial from ax.core.runner import Runner from ax.core.trial import Trial class MockJobRunner(Runner): # Deploys trials to external system. def run(self, trial: BaseTrial) -&gt; Dict[str, Any]: &quot;&quot;&quot;Deploys a trial based on custom runner subclass implementation. Args: trial: The trial to deploy. Returns: Dict of run metadata from the deployment process. &quot;&quot;&quot; if not isinstance(trial, Trial): raise ValueError(&quot;This runner only handles `Trial`.&quot;) mock_job_queue = get_mock_job_queue_client() job_id = mock_job_queue.schedule_job_with_parameters( parameters=trial.arm.parameters ) # This run metadata will be attached to trial as `trial.run_metadata` # by the base `Scheduler`. return {&quot;job_id&quot;: job_id} def poll_trial_status( self, trials: Iterable[BaseTrial] ) -&gt; Dict[TrialStatus, Set[int]]: &quot;&quot;&quot;Checks the status of any non-terminal trials and returns their indices as a mapping from TrialStatus to a list of indices. Required for runners used with Ax ``Scheduler``. NOTE: Does not need to handle waiting between polling calls while trials are running; this function should just perform a single poll. Args: trials: Trials to poll. Returns: A dictionary mapping TrialStatus to a list of trial indices that have the respective status at the time of the polling. This does not need to include trials that at the time of polling already have a terminal (ABANDONED, FAILED, COMPLETED) status (but it may). &quot;&quot;&quot; status_dict = defaultdict(set) for trial in trials: mock_job_queue = get_mock_job_queue_client() status = mock_job_queue.get_job_status( job_id=trial.run_metadata.get(&quot;job_id&quot;) ) status_dict[status].add(trial.index) return status_dict   import pandas as pd from ax.core.metric import Metric, MetricFetchResult, MetricFetchE from ax.core.base_trial import BaseTrial from ax.core.data import Data from ax.utils.common.result import Ok, Err class BraninForMockJobMetric(Metric): # Pulls data for trial from external system. def fetch_trial_data(self, trial: BaseTrial) -&gt; MetricFetchResult: &quot;&quot;&quot;Obtains data via fetching it from ` for a given trial.&quot;&quot;&quot; if not isinstance(trial, Trial): raise ValueError(&quot;This metric only handles `Trial`.&quot;) try: mock_job_queue = get_mock_job_queue_client() # Here we leverage the &quot;job_id&quot; metadata created by `MockJobRunner.run`. branin_data = mock_job_queue.get_outcome_value_for_completed_job( job_id=trial.run_metadata.get(&quot;job_id&quot;) ) df_dict = { &quot;trial_index&quot;: trial.index, &quot;metric_name&quot;: &quot;branin&quot;, &quot;arm_name&quot;: trial.arm.name, &quot;mean&quot;: branin_data.get(&quot;branin&quot;), # Can be set to 0.0 if function is known to be noiseless # or to an actual value when SEM is known. Setting SEM to # `None` results in Ax assuming unknown noise and inferring # noise level from data. &quot;sem&quot;: None, } return Ok(value=Data(df=pd.DataFrame.from_records([df_dict]))) except Exception as e: return Err( MetricFetchE(message=f&quot;Failed to fetch {self.name}&quot;, exception=e) )   Now we can set up the experiment using the runner and metric we defined. This experiment will have a single-objective optimization config, minimizing the Branin function, and the search space that corresponds to that function.  from ax import * def make_branin_experiment_with_runner_and_metric() -&gt; Experiment: parameters = [ RangeParameter( name=&quot;x1&quot;, parameter_type=ParameterType.FLOAT, lower=-5, upper=10, ), RangeParameter( name=&quot;x2&quot;, parameter_type=ParameterType.FLOAT, lower=0, upper=15, ), ] objective = Objective(metric=BraninForMockJobMetric(name=&quot;branin&quot;), minimize=True) return Experiment( name=&quot;branin_test_experiment&quot;, search_space=SearchSpace(parameters=parameters), optimization_config=OptimizationConfig(objective=objective), runner=MockJobRunner(), is_test=True, # Marking this experiment as a test experiment. ) experiment = make_branin_experiment_with_runner_and_metric()   Out: [INFO 09-29 17:02:22] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  ","version":"Next","tagName":"h2"},{"title":"4. Setting up a Scheduler​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#4-setting-up-a-scheduler","content":" ","version":"Next","tagName":"h2"},{"title":"4A. Auto-selecting a generation strategy​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#4a-auto-selecting-a-generation-strategy","content":" A Scheduler requires an Ax GenerationStrategy specifying the algorithm to use for the optimization. Here we use the choose_generation_strategy utility that auto-picks a generation strategy based on the search space properties. To construct a custom generation strategy instead, refer to the&quot;Generation Strategy&quot; tutorial.  Importantly, a generation strategy in Ax limits allowed parallelism levels for each generation step it contains. If you would like the Scheduler to ensure parallelism limitations, set max_examples on each generation step in your generation strategy.  from ax.modelbridge.dispatch_utils import choose_generation_strategy generation_strategy = choose_generation_strategy( search_space=experiment.search_space, max_parallelism_cap=3, )   Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:02:22] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  Now we have all the components needed to start the scheduler:  from ax.service.scheduler import Scheduler, SchedulerOptions scheduler = Scheduler( experiment=experiment, generation_strategy=generation_strategy, options=SchedulerOptions(), )   Out: [INFO 09-29 17:02:22] Scheduler: Scheduler requires experiment to have immutable search space and optimization config. Setting property immutable_search_space_and_opt_config to True on experiment.  ","version":"Next","tagName":"h3"},{"title":"4B. Optional: Defining a plotting function​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#4b-optional-defining-a-plotting-function","content":"   import numpy as np from ax.plot.trace import optimization_trace_single_method from ax.utils.notebook.plotting import render, init_notebook_plotting init_notebook_plotting() def get_plot(): best_objectives = np.array( [[trial.objective_mean for trial in scheduler.experiment.trials.values()]] ) best_objective_plot = optimization_trace_single_method( y=np.minimum.accumulate(best_objectives, axis=1), title=&quot;Model performance vs. # of iterations&quot;, ylabel=&quot;Y&quot;, ) return best_objective_plot   Out: [INFO 09-29 17:02:22] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.  Out: [INFO 09-29 17:02:22] ax.utils.notebook.plotting: Please see (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering) if visualizations are not rendering.  ","version":"Next","tagName":"h3"},{"title":"5. Running the optimization​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#5-running-the-optimization","content":" Once the Scheduler instance is set up, user can execute run_n_trials as many times as needed, and each execution will add up to the specified max_trials trials to the experiment. The number of trials actually run might be less than max_trials if the optimization was concluded (e.g. there are no more points in the search space).  scheduler.run_n_trials(max_trials=3)   Out: [INFO 09-29 17:02:22] Scheduler: Fetching data for newly completed trials: [].  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:22] Scheduler: Running trials [0]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:23] Scheduler: Running trials [1]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:23] Scheduler: Running trials [2]...  Out: [INFO 09-29 17:02:24] Scheduler: Fetching data for newly completed trials: 1 - 2.  Out: [INFO 09-29 17:02:24] Scheduler: Retrieved COMPLETED trials: 1 - 2.  Out: [INFO 09-29 17:02:24] Scheduler: Done submitting trials, waiting for remaining 1 running trials...  Out: [INFO 09-29 17:02:24] Scheduler: Fetching data for newly completed trials: [0].  Out: [INFO 09-29 17:02:24] Scheduler: Retrieved COMPLETED trials: [0].  Out: OptimizationResult()  best_objective_plot = get_plot() render(best_objective_plot)   loading...  We can examine experiment to see that it now has three trials:  from ax.service.utils.report_utils import exp_to_df exp_to_df(experiment)   Out: [WARNING 09-29 17:02:25] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tbranin\tx1\tx20\t0\t0_0\tCOMPLETED\tSobol\t31.5105\t-1.27445\t12.5633 1\t1\t1_0\tCOMPLETED\tSobol\t58.2046\t5.82909\t7.4031 2\t2\t2_0\tCOMPLETED\tSobol\t33.8565\t9.23264\t8.08675  Now we can run run_n_trials again to add three more trials to the experiment (this time, without plotting).  scheduler.run_n_trials(max_trials=3)   Out: [INFO 09-29 17:02:25] Scheduler: Fetching data for newly completed trials: [].  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:25] Scheduler: Running trials [3]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:26] Scheduler: Running trials [4]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.  Out: [INFO 09-29 17:02:28] Scheduler: Running trials [5]...  Out: [INFO 09-29 17:02:29] Scheduler: Fetching data for newly completed trials: 4 - 5.  Out: [INFO 09-29 17:02:29] Scheduler: Retrieved COMPLETED trials: 4 - 5.  Out: [INFO 09-29 17:02:29] Scheduler: Done submitting trials, waiting for remaining 1 running trials...  Out: [INFO 09-29 17:02:29] Scheduler: Fetching data for newly completed trials: [3].  Out: [INFO 09-29 17:02:29] Scheduler: Retrieved COMPLETED trials: [3].  Out: OptimizationResult()  best_objective_plot = get_plot() render(best_objective_plot)   loading...  Examiniming the experiment, we now see 6 trials, one of which is produced by Bayesian optimization (GPEI):  exp_to_df(experiment)   Out: [WARNING 09-29 17:02:29] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tbranin\tx1\tx20\t0\t0_0\tCOMPLETED\tSobol\t31.5105\t-1.27445\t12.5633 1\t1\t1_0\tCOMPLETED\tSobol\t58.2046\t5.82909\t7.4031 2\t2\t2_0\tCOMPLETED\tSobol\t33.8565\t9.23264\t8.08675 3\t3\t3_0\tCOMPLETED\tSobol\t9.05267\t1.89288\t2.00383 4\t4\t4_0\tCOMPLETED\tSobol\t16.6389\t-1.1721\t9.74915 5\t5\t5_0\tCOMPLETED\tBoTorch\t12.479\t-2.8492\t15  For each call to run_n_trials, one can specify a timeout; if run_n_trials has been running for too long without finishing its max_trials, the operation will exit gracefully:  scheduler.run_n_trials(max_trials=3, timeout_hours=0.00001)   Out: [INFO 09-29 17:02:30] Scheduler: Fetching data for newly completed trials: [].  Out: [INFO 09-29 17:02:31] Scheduler: Running trials [6]...  Out: [ERROR 09-29 17:02:31] Scheduler: Optimization timed out (timeout hours: 1e-05)!  Out: [INFO 09-29 17:02:31] Scheduler: should_abort_optimization is True, not running more trials.  Out: [INFO 09-29 17:02:31] Scheduler: Fetching data for newly completed trials: [6].  Out: [INFO 09-29 17:02:31] Scheduler: Retrieved COMPLETED trials: [6].  Out: [ERROR 09-29 17:02:31] Scheduler: Optimization timed out (timeout hours: 1e-05)!  Out: OptimizationResult()  best_objective_plot = get_plot() render(best_objective_plot)   loading...  ","version":"Next","tagName":"h2"},{"title":"6. Leveraging SQL storage and experiment resumption​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#6-leveraging-sql-storage-and-experiment-resumption","content":" When a scheduler is SQL-enabled, it will automatically save all updates it makes to the experiment in the course of the optimization. The experiment can then be resumed in the event of a crash or after a pause. The scheduler should be stateless and therefore, the scheduler itself is not saved in the database.  To store state of optimization to an SQL backend, first followsetup instructions on Ax website. Having set up the SQL backend, pass DBSettings to the Scheduler on instantiation (note that SQLAlchemy dependency will have to be installed – for installation, refer tooptional dependencies on Ax website):  from ax.storage.registry_bundle import RegistryBundle from ax.storage.sqa_store.db import ( create_all_tables, get_engine, init_engine_and_session_factory, ) from ax.storage.sqa_store.decoder import Decoder from ax.storage.sqa_store.encoder import Encoder from ax.storage.sqa_store.sqa_config import SQAConfig from ax.storage.sqa_store.structs import DBSettings bundle = RegistryBundle( metric_clss={BraninForMockJobMetric: None}, runner_clss={MockJobRunner: None} ) # URL is of the form &quot;dialect+driver://username:password@host:port/database&quot;. # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary. db_settings = DBSettings( url=&quot;sqlite:///foo.db&quot;, encoder=bundle.encoder, decoder=bundle.decoder, ) # The following lines are only necessary because it is the first time we are using this database # in practice, you will not need to run these lines every time you initialize your scheduler init_engine_and_session_factory(url=db_settings.url) engine = get_engine() create_all_tables(engine) stored_experiment = make_branin_experiment_with_runner_and_metric() generation_strategy = choose_generation_strategy(search_space=experiment.search_space) scheduler_with_storage = Scheduler( experiment=stored_experiment, generation_strategy=generation_strategy, options=SchedulerOptions(), db_settings=db_settings, )   Out: [INFO 09-29 17:02:31] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:02:31] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  Out: [INFO 09-29 17:02:31] Scheduler: Scheduler requires experiment to have immutable search space and optimization config. Setting property immutable_search_space_and_opt_config to True on experiment.  Out: [INFO 09-29 17:02:31] ax.service.utils.with_db_settings_base: Experiment branin_test_experiment is not yet in DB, storing it.  Out: [INFO 09-29 17:02:31] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:31] ax.service.utils.with_db_settings_base: Generation strategy Sobol+BoTorch is not yet in DB, storing it.  To resume a stored experiment:  reloaded_experiment_scheduler = Scheduler.from_stored_experiment( experiment_name=&quot;branin_test_experiment&quot;, options=SchedulerOptions(), # `DBSettings` are also required here so scheduler has access to the # database, from which it needs to load the experiment. db_settings=db_settings, )   Out: [INFO 09-29 17:02:31] ax.service.utils.with_db_settings_base: Loading experiment and generation strategy (with reduced state: True)...  Out: [INFO 09-29 17:02:31] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:31] ax.service.utils.with_db_settings_base: Loaded experiment branin_test_experiment &amp; 0 trials in 0.01 seconds.  Out: [INFO 09-29 17:02:31] ax.service.utils.with_db_settings_base: Loaded generation strategy for experiment branin_test_experiment in 0.09 seconds.  With the newly reloaded experiment, the Scheduler can continue the optimization:  reloaded_experiment_scheduler.run_n_trials(max_trials=3)   Out: [INFO 09-29 17:02:32] Scheduler: Fetching data for newly completed trials: [].  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:32] Scheduler: Running trials [0]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:32] Scheduler: Running trials [1]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:32] Scheduler: Running trials [2]...  Out: [INFO 09-29 17:02:32] Scheduler: Fetching data for newly completed trials: 1 - 2.  Out: [INFO 09-29 17:02:32] Scheduler: Retrieved COMPLETED trials: 1 - 2.  Out: [INFO 09-29 17:02:32] Scheduler: Done submitting trials, waiting for remaining 1 running trials...  Out: [INFO 09-29 17:02:32] Scheduler: Fetching data for newly completed trials: [0].  Out: [INFO 09-29 17:02:32] Scheduler: Retrieved COMPLETED trials: [0].  Out: OptimizationResult()  ","version":"Next","tagName":"h2"},{"title":"7. Configuring the scheduler with SchedulerOptions, like early stopping​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#7-configuring-the-scheduler-with-scheduleroptions-like-early-stopping","content":" Scheduler exposes many options to configure the exact settings of the closed-loop optimization to perform. A few notable ones are:  trial_type –– currently only Trial and not BatchTrial is supported, but support for BatchTrial-s will follow,tolerated_trial_failure_rate and min_failed_trials_for_failure_rate_check –– together these two settings control how the scheduler monitors the failure rate among trial runs it deploys. Once min_failed_trials_for_failure_rate_check is deployed, the scheduler will start checking whether the ratio of failed to total trials is greater than tolerated_trial_failure_rate, and if it is, scheduler will exit the optimization with a FailureRateExceededError,ttl_seconds_for_trials –– sometimes a failure in a trial run means that it will be difficult to query its status (e.g. due to a crash). If this setting is specified, the Ax Experiment will automatically mark trials that have been running for too long (more than their 'time-to-live' (TTL) seconds) as failed,run_trials_in_batches –– if True, the scheduler will attempt to run trials not by calling Scheduler.run_trial in a loop, but by calling Scheduler.run_trials on all ready-to-deploy trials at once. This could allow for saving compute in cases where the deployment operation has large overhead and deploying many trials at once saves compute. Note that using this option successfully will require your scheduler subclass to implement MySchedulerSubclass.run_trials andMySchedulerSubclass.poll_available_capacity.early_stopping_strategy -- determines whether a trial should be stopped given the current state of the experiment, so that less promising trials can be terminated quickly. For more on this, see the Trial-Level Early Stopping tutorial:https://ax.dev/tutorials/early_stopping/early_stopping.htmlglobal_stopping_strategy -- determines whether the full optimization should be stopped or not, so that the run terminates when little progress is being made. Aglobal_stopping_strategy instance can be passed to SchedulerOptions just as it is passed to AxClient, as illustrated in the tutorial on Global Stopping Strategy with AxClient: https://ax.dev/tutorials/gss.html  The rest of the options are described in the docstring below:  print(SchedulerOptions.__doc__)   Out: Settings for a scheduler instance. Attributes: max_pending_trials: Maximum number of pending trials the scheduler can have STAGED or RUNNING at once, required. If looking to use Runner.poll_available_capacity as a primary guide for how many trials should be pending at a given time, set this limit to a high number, as an upper bound on number of trials that should not be exceeded. trial_type: Type of trials (1-arm Trial or multi-arm Batch Trial) that will be deployed using the scheduler. Defaults to 1-arm Trial. NOTE: use BatchTrial only if need to evaluate multiple arms *together*, e.g. in an A/B-test influenced by data nonstationarity. For cases where just deploying multiple arms at once is beneficial but the trials are evaluated *independently*, implement run_trials method in scheduler subclass, to deploy multiple 1-arm trials at the same time. batch_size: If using BatchTrial the number of arms to be generated and deployed per trial. total_trials: Limit on number of trials a given Scheduler should run. If no stopping criteria are implemented on a given scheduler, exhaustion of this number of trials will be used as default stopping criterion in Scheduler.run_all_trials. Required to be non-null if using Scheduler.run_all_trials (not required for Scheduler.run_n_trials). tolerated_trial_failure_rate: Fraction of trials in this optimization that are allowed to fail without the whole optimization ending. Expects value between 0 and 1. NOTE: Failure rate checks begin once min_failed_trials_for_failure_rate_check trials have failed; after that point if the ratio of failed trials to total trials ran so far exceeds the failure rate, the optimization will halt. min_failed_trials_for_failure_rate_check: The minimum number of trials that must fail in Scheduler in order to start checking failure rate. log_filepath: File, to which to write optimization logs. logging_level: Minimum level of logging statements to log, defaults to logging.INFO. ttl_seconds_for_trials: Optional TTL for all trials created within this Scheduler, in seconds. Trials that remain RUNNING for more than their TTL seconds will be marked FAILED once the TTL elapses and may be re-suggested by the Ax optimization models. init_seconds_between_polls: Initial wait between rounds of polling, in seconds. Relevant if using the default wait- for-completed-runs functionality of the base Scheduler (if wait_for_completed_trials_and_report_results is not overridden). With the default waiting, every time a poll returns that no trial evaluations completed, wait time will increase; once some completed trial evaluations are found, it will reset back to this value. Specify 0 to not introduce any wait between polls. min_seconds_before_poll: Minimum number of seconds between beginning to run a trial and the first poll to check trial status. timeout_hours: Number of hours after which the optimization will abort. seconds_between_polls_backoff_factor: The rate at which the poll interval increases. run_trials_in_batches: If True and poll_available_capacity is implemented to return non-null results, trials will be dispatched in groups via run_trials instead of one-by-one via run_trial. This allows to save time, IO calls or computation in cases where dispatching trials in groups is more efficient then sequential deployment. The size of the groups will be determined as the minimum of self.poll_available_capacity() and the number of generator runs that the generation strategy is able to produce without more data or reaching its allowed max paralellism limit. debug_log_run_metadata: Whether to log run_metadata for debugging purposes. early_stopping_strategy: A BaseEarlyStoppingStrategy that determines whether a trial should be stopped given the current state of the experiment. Used in should_stop_trials_early. global_stopping_strategy: A BaseGlobalStoppingStrategy that determines whether the full optimization should be stopped or not. suppress_storage_errors_after_retries: Whether to fully suppress SQL storage-related errors if encountered, after retrying the call multiple times. Only use if SQL storage is not important for the given use case, since this will only log, but not raise, an exception if it's encountered while saving to DB or loading from it. wait_for_running_trials: Whether the scheduler should wait for running trials or exit. fetch_kwargs: Kwargs to be used when fetching data. validate_metrics: Whether to raise an error if there is a problem with the metrics attached to the experiment. status_quo_weight: The weight of the status quo arm. This is only used if the scheduler is using a BatchTrial. This requires that the status_quo be set on the experiment. enforce_immutable_search_space_and_opt_config: Whether to enforce that the search space and optimization config are immutable. If true, will add &quot;immutable_search_space_and_opt_config&quot;: True to experiment properties mt_experiment_trial_type: Type of trial to run for MultiTypeExperiments. This is currently required for MultiTypeExperiments. This is ignored for &quot;regular&quot; or single type experiments. If you don't know what a single type experiment is, you don't need this.   ","version":"Next","tagName":"h2"},{"title":"8. Advanced functionality​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#8-advanced-functionality","content":" ","version":"Next","tagName":"h2"},{"title":"8a. Reporting results to an external system​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#8a-reporting-results-to-an-external-system","content":" The Scheduler can report the optimization result to an external system each time there are new completed trials if the user-implemented subclass implementsMySchedulerSubclass.report_results to do so. For example, the folliwing method:  class MySchedulerSubclass(Scheduler): ... def report_results(self, force_refit: bool = False): write_to_external_database(len(self.experiment.trials)) return (True, \\{\\}) # Returns optimization success status and optional dict of outputs.   could be used to record number of trials in experiment so far in an external database.  Since report_results is an instance method, it has access to self.experiment andself.generation_strategy, which contain all the information about the state of the optimization thus far.  ","version":"Next","tagName":"h3"},{"title":"8b. Using run_trials_and_yield_results generator method​","type":1,"pageTitle":"Configurable closed-loop optimization with Ax Scheduler","url":"/Ax/docs/tutorials/scheduler/#8b-using-run_trials_and_yield_results-generator-method","content":" In some systems it's beneficial to have greater control over Scheduler.run_n_trialsinstead of just starting it and needing to wait for it to run all the way to completion before having access to its output. For this purpose, the Scheduler implements a generator method run_trials_and_yield_results, which yields the output ofScheduler.report_results each time there are new completed trials and can be used like so:  class ResultReportingScheduler(Scheduler): def report_results(self, force_refit: bool = False): return True, { &quot;trials so far&quot;: len(self.experiment.trials), &quot;currently producing trials from generation step&quot;: self.generation_strategy._curr.model_name, &quot;running trials&quot;: [t.index for t in self.running_trials], }   experiment = make_branin_experiment_with_runner_and_metric() scheduler = ResultReportingScheduler( experiment=experiment, generation_strategy=choose_generation_strategy( search_space=experiment.search_space, max_parallelism_cap=3, ), options=SchedulerOptions(), ) for reported_result in scheduler.run_trials_and_yield_results(max_trials=6): print(&quot;Reported result: &quot;, reported_result)   Out: [INFO 09-29 17:02:32] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:02:32] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  Out: [INFO 09-29 17:02:32] ResultReportingScheduler: Scheduler requires experiment to have immutable search space and optimization config. Setting property immutable_search_space_and_opt_config to True on experiment.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:32] ResultReportingScheduler: Running trials [0]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:32] ResultReportingScheduler: Running trials [1]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:33] ResultReportingScheduler: Running trials [2]...  Out: [INFO 09-29 17:02:34] ResultReportingScheduler: Generated all trials that can be generated currently. Max parallelism currently reached.  Out: [INFO 09-29 17:02:34] ResultReportingScheduler: Fetching data for newly completed trials: 0 - 2.  Out: [INFO 09-29 17:02:34] ResultReportingScheduler: Retrieved COMPLETED trials: 0 - 2.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:34] ResultReportingScheduler: Running trials [3]...  Out: Reported result: (True, {'trials so far': 3, 'currently producing trials from generation step': 'Sobol', 'running trials': []})  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:02:35] ResultReportingScheduler: Running trials [4]...  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.  Out: [INFO 09-29 17:02:37] ResultReportingScheduler: Running trials [5]...  Out: [INFO 09-29 17:02:38] ResultReportingScheduler: Fetching data for newly completed trials: 4 - 5.  Out: [INFO 09-29 17:02:38] ResultReportingScheduler: Retrieved COMPLETED trials: 4 - 5.  Out: [INFO 09-29 17:02:38] ResultReportingScheduler: Done submitting trials, waiting for remaining 1 running trials...  Out: [INFO 09-29 17:02:38] ResultReportingScheduler: Fetching data for newly completed trials: [3].  Out: [INFO 09-29 17:02:38] ResultReportingScheduler: Retrieved COMPLETED trials: [3].  Out: Reported result: (True, {'trials so far': 6, 'currently producing trials from generation step': 'BoTorch', 'running trials': [3]}) Reported result: (True, {'trials so far': 6, 'currently producing trials from generation step': 'BoTorch', 'running trials': []}) Reported result: (True, {'trials so far': 6, 'currently producing trials from generation step': 'BoTorch', 'running trials': []})  # Clean up to enable running the tutorial repeatedly with # the same results. You wouldn't do this if you wanted to # keep adding data to the same experiment. from ax.storage.sqa_store.delete import delete_experiment delete_experiment(&quot;branin_test_experiment&quot;)   Out: [INFO 09-29 17:02:38] ax.storage.sqa_store.delete: You are deleting branin_test_experiment and all its associated data from the database.   ","version":"Next","tagName":"h3"},{"title":"Multi-Objective Optimization Ax API","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/multiobjective_optimization/","content":"","keywords":"","version":"Next"},{"title":"Using the Service API​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#using-the-service-api","content":" For Multi-objective optimization (MOO) in the AxClient, objectives are specified through the ObjectiveProperties dataclass. An ObjectiveProperties requires a booleanminimize, and also accepts an optional floating point threshold. If a threshold is not specified, Ax will infer it through the use of heuristics. If the user knows the region of interest (because they have specs or prior knowledge), then specifying the thresholds is preferable to inferring it. But if the user would need to guess, inferring is preferable.  To learn more about how to choose a threshold, seeSet Objective Thresholds to focus candidate generation in a region of interest. See the Service API Tutorial for more infomation on running experiments with the Service API.  import torch from ax.plot.pareto_frontier import plot_pareto_frontier from ax.plot.pareto_utils import compute_posterior_pareto_frontier from ax.service.ax_client import AxClient from ax.service.utils.instantiation import ObjectiveProperties # Plotting imports and initialization from ax.utils.notebook.plotting import init_notebook_plotting, render from botorch.test_functions.multi_objective import BraninCurrin init_notebook_plotting()   Out: [INFO 09-29 17:12:37] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.  Out: [INFO 09-29 17:12:37] ax.utils.notebook.plotting: Please see (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering) if visualizations are not rendering.  # Load our sample 2-objective problem branin_currin = BraninCurrin(negate=True).to( dtype=torch.double, device=torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;), )   ax_client = AxClient() ax_client.create_experiment( name=&quot;moo_experiment&quot;, parameters=[ { &quot;name&quot;: f&quot;x{i+1}&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], } for i in range(2) ], objectives={ # `threshold` arguments are optional &quot;a&quot;: ObjectiveProperties(minimize=False, threshold=branin_currin.ref_point[0]), &quot;b&quot;: ObjectiveProperties(minimize=False, threshold=branin_currin.ref_point[1]), }, overwrite_existing_experiment=True, is_test=True, )   Out: [INFO 09-29 17:12:38] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 17:12:38] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x1. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:12:38] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:12:38] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).  Out: [INFO 09-29 17:12:38] ax.core.experiment: The is_test flag has been set to True. This flag is meant purely for development and integration testing purposes. If you are running a live experiment, please set this flag to False  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:12:38] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  ","version":"Next","tagName":"h3"},{"title":"Create an Evaluation Function​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#create-an-evaluation-function","content":" In the case of MOO experiments, evaluation functions can be any arbitrary function that takes in a dict of parameter names mapped to values and returns a dict of objective names mapped to a tuple of mean and SEM values.  def evaluate(parameters): evaluation = branin_currin( torch.tensor([parameters.get(&quot;x1&quot;), parameters.get(&quot;x2&quot;)]) ) # In our case, standard error is 0, since we are computing a synthetic function. # Set standard error to None if the noise level is unknown. return {&quot;a&quot;: (evaluation[0].item(), 0.0), &quot;b&quot;: (evaluation[1].item(), 0.0)}   ","version":"Next","tagName":"h3"},{"title":"Run Optimization​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#run-optimization","content":" for i in range(25): parameters, trial_index = ax_client.get_next_trial() # Local evaluation here can be replaced with deployment to external system. ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 0 with parameters {'x1': 0.2837, 'x2': 0.014464} using model Sobol.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 0 with data: {'a': (-66.617004, 0.0), 'b': (-13.492417, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 1 with parameters {'x1': 0.963623, 'x2': 0.889814} using model Sobol.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 1 with data: {'a': (-118.062721, 0.0), 'b': (-4.38965, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 2 with parameters {'x1': 0.729568, 'x2': 0.27858} using model Sobol.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 2 with data: {'a': (-28.506718, 0.0), 'b': (-8.877208, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 3 with parameters {'x1': 0.018867, 'x2': 0.652958} using model Sobol.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 3 with data: {'a': (-53.437031, 0.0), 'b': (-2.64577, 0.0)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 4 with parameters {'x1': 0.192546, 'x2': 0.494199} using model Sobol.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 4 with data: {'a': (-11.426407, 0.0), 'b': (-8.740768, 0.0)}.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.0, 'x2': 0.42961} using model BoTorch.  Out: [INFO 09-29 17:12:38] ax.service.ax_client: Completed trial 5 with data: {'a': (-128.140274, 0.0), 'b': (-2.063152, 0.0)}.  Out: [INFO 09-29 17:12:39] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.0, 'x2': 0.011941} using model BoTorch.  Out: [INFO 09-29 17:12:39] ax.service.ax_client: Completed trial 6 with data: {'a': (-302.004089, 0.0), 'b': (-3.0, 0.0)}.  Out: [INFO 09-29 17:12:39] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.0, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:39] ax.service.ax_client: Completed trial 7 with data: {'a': (-17.508297, 0.0), 'b': (-1.180408, 0.0)}.  Out: [INFO 09-29 17:12:40] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.497303, 'x2': 0.448141} using model BoTorch.  Out: [INFO 09-29 17:12:40] ax.service.ax_client: Completed trial 8 with data: {'a': (-17.408012, 0.0), 'b': (-7.888223, 0.0)}.  Out: [INFO 09-29 17:12:41] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.03918, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:41] ax.service.ax_client: Completed trial 9 with data: {'a': (-7.450374, 0.0), 'b': (-2.72623, 0.0)}.  Out: [INFO 09-29 17:12:42] ax.service.ax_client: Generated new trial 10 with parameters {'x1': 0.111551, 'x2': 0.954213} using model BoTorch.  Out: [INFO 09-29 17:12:42] ax.service.ax_client: Completed trial 10 with data: {'a': (-3.086375, 0.0), 'b': (-4.871634, 0.0)}.  Out: [INFO 09-29 17:12:43] ax.service.ax_client: Generated new trial 11 with parameters {'x1': 1.0, 'x2': 0.0} using model BoTorch.  Out: [INFO 09-29 17:12:43] ax.service.ax_client: Completed trial 11 with data: {'a': (-10.960894, 0.0), 'b': (-10.179487, 0.0)}.  Out: [INFO 09-29 17:12:44] ax.service.ax_client: Generated new trial 12 with parameters {'x1': 0.070888, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:44] ax.service.ax_client: Completed trial 12 with data: {'a': (-3.813025, 0.0), 'b': (-3.768338, 0.0)}.  Out: [INFO 09-29 17:12:44] ax.service.ax_client: Generated new trial 13 with parameters {'x1': 0.082606, 'x2': 0.911558} using model BoTorch.  Out: [INFO 09-29 17:12:44] ax.service.ax_client: Completed trial 13 with data: {'a': (-2.200706, 0.0), 'b': (-4.383107, 0.0)}.  Out: [INFO 09-29 17:12:45] ax.service.ax_client: Generated new trial 14 with parameters {'x1': 0.01807, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:45] ax.service.ax_client: Completed trial 14 with data: {'a': (-12.162113, 0.0), 'b': (-1.913986, 0.0)}.  Out: [INFO 09-29 17:12:46] ax.service.ax_client: Generated new trial 15 with parameters {'x1': 0.568928, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:46] ax.service.ax_client: Completed trial 15 with data: {'a': (-170.415359, 0.0), 'b': (-4.446478, 0.0)}.  Out: [INFO 09-29 17:12:48] ax.service.ax_client: Generated new trial 16 with parameters {'x1': 0.052821, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:48] ax.service.ax_client: Completed trial 16 with data: {'a': (-5.357145, 0.0), 'b': (-3.20551, 0.0)}.  Out: [INFO 09-29 17:12:49] ax.service.ax_client: Generated new trial 17 with parameters {'x1': 0.107565, 'x2': 0.864706} using model BoTorch.  Out: [INFO 09-29 17:12:49] ax.service.ax_client: Completed trial 17 with data: {'a': (-0.694307, 0.0), 'b': (-5.16587, 0.0)}.  Out: [INFO 09-29 17:12:50] ax.service.ax_client: Generated new trial 18 with parameters {'x1': 0.008764, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:50] ax.service.ax_client: Completed trial 18 with data: {'a': (-14.771387, 0.0), 'b': (-1.538348, 0.0)}.  Out: [INFO 09-29 17:12:52] ax.service.ax_client: Generated new trial 19 with parameters {'x1': 0.02806, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:12:52] ax.service.ax_client: Completed trial 19 with data: {'a': (-9.717279, 0.0), 'b': (-2.307071, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [INFO 09-29 17:12:54] ax.service.ax_client: Generated new trial 20 with parameters {'x1': 0.094862, 'x2': 0.901159} using model BoTorch.  Out: [INFO 09-29 17:12:54] ax.service.ax_client: Completed trial 20 with data: {'a': (-1.32342, 0.0), 'b': (-4.733772, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. [INFO 09-29 17:13:00] ax.service.ax_client: Generated new trial 21 with parameters {'x1': 0.074997, 'x2': 0.954797} using model BoTorch.  Out: [INFO 09-29 17:13:00] ax.service.ax_client: Completed trial 21 with data: {'a': (-2.913071, 0.0), 'b': (-4.023861, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-05 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-04 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. [INFO 09-29 17:13:06] ax.service.ax_client: Generated new trial 22 with parameters {'x1': 0.045635, 'x2': 1.0} using model BoTorch.  Out: [INFO 09-29 17:13:06] ax.service.ax_client: Completed trial 22 with data: {'a': (-6.363218, 0.0), 'b': (-2.958321, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: [INFO 09-29 17:13:08] ax.service.ax_client: Generated new trial 23 with parameters {'x1': 0.589936, 'x2': 0.0} using model BoTorch.  Out: [INFO 09-29 17:13:08] ax.service.ax_client: Completed trial 23 with data: {'a': (-5.898928, 0.0), 'b': (-11.192137, 0.0)}.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [INFO 09-29 17:13:15] ax.service.ax_client: Generated new trial 24 with parameters {'x1': 0.060571, 'x2': 0.993217} using model BoTorch.  Out: [INFO 09-29 17:13:15] ax.service.ax_client: Completed trial 24 with data: {'a': (-4.463593, 0.0), 'b': (-3.475808, 0.0)}.  ","version":"Next","tagName":"h3"},{"title":"Plot Pareto Frontier​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#plot-pareto-frontier","content":" objectives = ax_client.experiment.optimization_config.objective.objectives frontier = compute_posterior_pareto_frontier( experiment=ax_client.experiment, data=ax_client.experiment.fetch_data(), primary_objective=objectives[1].metric, secondary_objective=objectives[0].metric, absolute_metrics=[&quot;a&quot;, &quot;b&quot;], num_points=20, ) render(plot_pareto_frontier(frontier, CI_level=0.90))   loading...  Deep Dive  In the rest of this tutorial, we will show two algorithms available in Ax for multi-objective optimization and visualize how they compare to eachother and to quasirandom search.  MOO covers the case where we care about multiple outcomes in our experiment but we do not know before hand a specific weighting of those objectives (covered byScalarizedObjective) or a specific constraint on one objective (covered byOutcomeConstraints) that will produce the best result.  The solution in this case is to find a whole Pareto frontier, a surface in outcome-space containing points that can't be improved on in every outcome. This shows us the tradeoffs between objectives that we can choose to make.  ","version":"Next","tagName":"h3"},{"title":"Problem Statement​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#problem-statement","content":" Optimize a list of M objective functions $ \\bigl(f^{(1)}( x),..., f^{(M)}( x) \\bigr)$ over a bounded search space $\\mathcal X \\subset \\mathbb R^d$.  We assume $f^{(i)}$ are expensive-to-evaluate black-box functions with no known analytical expression, and no observed gradients. For instance, a machine learning model where we're interested in maximizing accuracy and minimizing inference time, with $\\mathcal X$ the set of possible configuration spaces  ","version":"Next","tagName":"h3"},{"title":"Pareto Optimality​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#pareto-optimality","content":" In a multi-objective optimization problem, there typically is no single best solution. Rather, the goal is to identify the set of Pareto optimal solutions such that any improvement in one objective means deteriorating another. Provided with the Pareto set, decision-makers can select an objective trade-off according to their preferences. In the plot below, the red dots are the Pareto optimal solutions (assuming both objectives are to be minimized).   ","version":"Next","tagName":"h3"},{"title":"Evaluating the Quality of a Pareto Front (Hypervolume)​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#evaluating-the-quality-of-a-pareto-front-hypervolume","content":" Given a reference point $ r \\in \\mathbb R^M$, which we represent as a list of MObjectiveThresholds, one for each coordinate, the hypervolume (HV) of a Pareto set $\\mathcal P = { f(x_i)}_{i=1}^{|\\mathcal P|}$ is the volume of the space dominated (superior in every one of our M objectives) by $\\mathcal P$ and bounded from above by a point $ r$. The reference point should be set to be slightly worse (10% is reasonable) than the worst value of each objective that a decision maker would tolerate. In the figure below, the grey area is the hypervolume in this 2-objective problem.  ","version":"Next","tagName":"h3"},{"title":"Set Objective Thresholds to focus candidate generation in a region of interest​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#set-objective-thresholds-to-focus-candidate-generation-in-a-region-of-interest","content":" The below plots show three different sets of points generated by the qNEHVI [1] algorithm with different objective thresholds (aka reference points). Note that here we use absolute thresholds, but thresholds can also be relative to a status_quo arm.  The first plot shows the points without the ObjectiveThresholds visible (they're set far below the origin of the graph).  The second shows the points generated with (-18, -6) as thresholds. The regions violating the thresholds are greyed out. Only the white region in the upper right exceeds both threshold, points in this region dominate the intersection of these thresholds (this intersection is the reference point). Only points in this region contribute to the hypervolume objective. A few exploration points are not in the valid region, but almost all the rest of the points are.  The third shows points generated with a very strict pair of thresholds, (-18, -2). Only the white region in the upper right exceeds both thresholds. Many points do not lie in the dominating region, but there are still more focused there than in the second examples.  ","version":"Next","tagName":"h3"},{"title":"Further Information​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#further-information","content":" A deeper explanation of our the qNEHVI [1] and qNParEGO [2] algorithms this notebook explores can be found at  [1]S. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems 34, 2021.  [2]S. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.  In addition, the underlying BoTorch implementation has a researcher-oriented tutorial athttps://botorch.org/tutorials/multi_objective_bo.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#setup","content":" import numpy as np import pandas as pd from ax.core.data import Data from ax.core.experiment import Experiment from ax.core.metric import Metric from ax.core.objective import MultiObjective, Objective from ax.core.optimization_config import ( MultiObjectiveOptimizationConfig, ObjectiveThreshold, ) from ax.core.parameter import ParameterType, RangeParameter from ax.core.search_space import SearchSpace from ax.metrics.noisy_function import NoisyFunctionMetric # Analysis utilities, including a method to evaluate hypervolumes from ax.modelbridge.modelbridge_utils import observed_hypervolume from ax.modelbridge.registry import Models from ax.runners.synthetic import SyntheticRunner from ax.service.utils.report_utils import exp_to_df # BoTorch acquisition class for ParEGO from botorch.acquisition.multi_objective.parego import qLogNParEGO   ","version":"Next","tagName":"h2"},{"title":"Define experiment configurations​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#define-experiment-configurations","content":" ","version":"Next","tagName":"h2"},{"title":"Search Space​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#search-space","content":" x1 = RangeParameter(name=&quot;x1&quot;, lower=0, upper=1, parameter_type=ParameterType.FLOAT) x2 = RangeParameter(name=&quot;x2&quot;, lower=0, upper=1, parameter_type=ParameterType.FLOAT) search_space = SearchSpace(parameters=[x1, x2])   ","version":"Next","tagName":"h3"},{"title":"MultiObjectiveOptimizationConfig​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#multiobjectiveoptimizationconfig","content":" To optimize multiple objective we must create a MultiObjective containing the metrics we'll optimize and MultiObjectiveOptimizationConfig (which containsObjectiveThresholds) instead of our more typical Objective and OptimizationConfig  We define NoisyFunctionMetrics to wrap our synthetic Branin-Currin problem's outputs. Add noise to see how robust our different optimization algorithms are.  class MetricA(NoisyFunctionMetric): def f(self, x: np.ndarray) -&gt; float: return float(branin_currin(torch.tensor(x))[0]) class MetricB(NoisyFunctionMetric): def f(self, x: np.ndarray) -&gt; float: return float(branin_currin(torch.tensor(x))[1]) metric_a = MetricA(&quot;a&quot;, [&quot;x1&quot;, &quot;x2&quot;], noise_sd=0.0, lower_is_better=False) metric_b = MetricB(&quot;b&quot;, [&quot;x1&quot;, &quot;x2&quot;], noise_sd=0.0, lower_is_better=False)   mo = MultiObjective( objectives=[Objective(metric=metric_a), Objective(metric=metric_b)], )   objective_thresholds = [ ObjectiveThreshold(metric=metric, bound=val, relative=False) for metric, val in zip(mo.metrics, branin_currin.ref_point) ]   optimization_config = MultiObjectiveOptimizationConfig( objective=mo, objective_thresholds=objective_thresholds, )   ","version":"Next","tagName":"h3"},{"title":"Define experiment creation utilities​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#define-experiment-creation-utilities","content":" These construct our experiment, then initialize with Sobol points before we fit a Gaussian Process model to those initial points.  # Reasonable defaults for number of quasi-random initialization points and for subsequent model-generated trials. N_INIT = 6 N_BATCH = 25   def build_experiment(): experiment = Experiment( name=&quot;pareto_experiment&quot;, search_space=search_space, optimization_config=optimization_config, runner=SyntheticRunner(), ) return experiment   ## Initialize with Sobol samples def initialize_experiment(experiment): sobol = Models.SOBOL(search_space=experiment.search_space, seed=1234) for _ in range(N_INIT): experiment.new_trial(sobol.gen(1)).run() return experiment.fetch_data()   Sobol  We use quasirandom points as a fast baseline for evaluating the quality of our multi-objective optimization algorithms.  sobol_experiment = build_experiment() sobol_data = initialize_experiment(sobol_experiment)   sobol_model = Models.SOBOL( experiment=sobol_experiment, data=sobol_data, ) sobol_hv_list = [] for i in range(N_BATCH): generator_run = sobol_model.gen(1) trial = sobol_experiment.new_trial(generator_run=generator_run) trial.run() exp_df = exp_to_df(sobol_experiment) outcomes = np.array(exp_df[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double) # Fit a GP-based model in order to calculate hypervolume. # We will not use this model to generate new points. dummy_model = Models.BOTORCH_MODULAR( experiment=sobol_experiment, data=sobol_experiment.fetch_data(), ) try: hv = observed_hypervolume(modelbridge=dummy_model) except: hv = 0 print(&quot;Failed to compute hv&quot;) sobol_hv_list.append(hv) print(f&quot;Iteration: {i}, HV: {hv}&quot;) sobol_outcomes = np.array(exp_to_df(sobol_experiment)[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:20] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 0, HV: 0.0 Iteration: 1, HV: 0.0 Iteration: 2, HV: 20.776142749102483 Iteration: 3, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 4, HV: 20.776142749102483 Iteration: 5, HV: 20.776142749102483 Iteration: 6, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 7, HV: 20.776142749102483 Iteration: 8, HV: 20.776142749102483 Iteration: 9, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 10, HV: 20.776142749102483 Iteration: 11, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 12, HV: 20.776142749102483 Iteration: 13, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 14, HV: 20.776142749102483 Iteration: 15, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 16, HV: 20.776142749102483 Iteration: 17, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:22] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 18, HV: 20.776142749102483 Iteration: 19, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:23] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:23] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 20, HV: 20.776142749102483 Iteration: 21, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:23] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 22, HV: 20.776142749102483  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:13:23] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:13:23] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 23, HV: 20.776142749102483 Iteration: 24, HV: 20.776142749102483  ","version":"Next","tagName":"h2"},{"title":"qNEHVI​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#qnehvi","content":" Noisy Expected Hypervolume Improvement. This is our current recommended algorithm for multi-objective optimization.  ehvi_experiment = build_experiment() ehvi_data = initialize_experiment(ehvi_experiment)   ehvi_hv_list = [] ehvi_model = None for i in range(N_BATCH): ehvi_model = Models.BOTORCH_MODULAR( experiment=ehvi_experiment, data=ehvi_data, ) generator_run = ehvi_model.gen(1) trial = ehvi_experiment.new_trial(generator_run=generator_run) trial.run() ehvi_data = Data.from_multiple_data([ehvi_data, trial.fetch_data()]) exp_df = exp_to_df(ehvi_experiment) outcomes = np.array(exp_df[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double) try: hv = observed_hypervolume(modelbridge=ehvi_model) except: hv = 0 print(&quot;Failed to compute hv&quot;) ehvi_hv_list.append(hv) print(f&quot;Iteration: {i}, HV: {hv}&quot;) ehvi_outcomes = np.array(exp_to_df(ehvi_experiment)[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double)   Out: [WARNING 09-29 17:13:24] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:13:24] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 0, HV: 0.0 Iteration: 1, HV: 0.0  Out: [WARNING 09-29 17:13:24] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 2, HV: 0.0  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:13:25] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 3, HV: 2.369795709893773  Out: [WARNING 09-29 17:13:25] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 4, HV: 2.369795709893773  Out: [WARNING 09-29 17:13:27] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 5, HV: 32.66997383973003  Out: [WARNING 09-29 17:13:28] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 6, HV: 44.191362840505406  Out: [WARNING 09-29 17:13:29] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 7, HV: 45.847374286546724  Out: [WARNING 09-29 17:13:32] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 8, HV: 48.96813602640998  Out: [WARNING 09-29 17:13:36] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 9, HV: 51.0273224707675  Out: [WARNING 09-29 17:13:38] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 10, HV: 51.0273224707675  Out: [WARNING 09-29 17:13:40] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 11, HV: 52.80718891665181  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. [WARNING 09-29 17:13:47] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 12, HV: 53.55746352269732  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-05 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:13:52] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 13, HV: 54.29834391815279  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:13:58] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 14, HV: 54.81671485873035  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:14:00] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 15, HV: 55.288895171155545  Out: [WARNING 09-29 17:14:02] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 16, HV: 55.72685022234546  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:14:09] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 17, HV: 55.94220517211903  Out: [WARNING 09-29 17:14:11] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 18, HV: 56.253704754935214  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed in gen_candidates_scipy with the following warning(s): [NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), OptimizationWarning('Optimization failed within scipy.optimize.minimize with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')] Trying again with a new set of initial conditions.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:568: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions. /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:14:21] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 19, HV: 56.46019349521489  Out: [WARNING 09-29 17:14:26] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 20, HV: 56.642856148191086  Out: [WARNING 09-29 17:14:29] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 21, HV: 56.75684161462832  Out: [WARNING 09-29 17:14:33] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 22, HV: 56.93947668815698  Out: [WARNING 09-29 17:14:37] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 23, HV: 57.048548539601455  Out: [WARNING 09-29 17:14:40] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:14:40] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 24, HV: 57.23010200743587  ","version":"Next","tagName":"h2"},{"title":"Plot qNEHVI Pareto Frontier based on model posterior​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#plot-qnehvi-pareto-frontier-based-on-model-posterior","content":" The plotted points are samples from the fitted model's posterior, not observed samples.  frontier = compute_posterior_pareto_frontier( experiment=ehvi_experiment, data=ehvi_experiment.fetch_data(), primary_objective=metric_b, secondary_objective=metric_a, absolute_metrics=[&quot;a&quot;, &quot;b&quot;], num_points=20, ) render(plot_pareto_frontier(frontier, CI_level=0.90))   loading...  ","version":"Next","tagName":"h2"},{"title":"qNParEGO​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#qnparego","content":" This is a good alternative algorithm for multi-objective optimization when qNEHVI runs too slowly. We use qLogNParEGO acquisition function with Modular BoTorch Model.  parego_experiment = build_experiment() parego_data = initialize_experiment(parego_experiment)   parego_hv_list = [] parego_model = None for i in range(N_BATCH): parego_model = Models.BOTORCH_MODULAR( experiment=parego_experiment, data=parego_data, botorch_acqf_class=qLogNParEGO, ) generator_run = parego_model.gen(1) trial = parego_experiment.new_trial(generator_run=generator_run) trial.run() parego_data = Data.from_multiple_data([parego_data, trial.fetch_data()]) exp_df = exp_to_df(parego_experiment) outcomes = np.array(exp_df[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double) try: hv = observed_hypervolume(modelbridge=parego_model) except: hv = 0 print(&quot;Failed to compute hv&quot;) parego_hv_list.append(hv) print(f&quot;Iteration: {i}, HV: {hv}&quot;) parego_outcomes = np.array(exp_to_df(parego_experiment)[[&quot;a&quot;, &quot;b&quot;]], dtype=np.double)   Out: [WARNING 09-29 17:14:45] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 0, HV: 0.0  Out: [WARNING 09-29 17:14:46] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 1, HV: 0.0  Out: [WARNING 09-29 17:14:47] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 2, HV: 0.0  Out: [WARNING 09-29 17:14:47] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:14:48] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 3, HV: 0.0 Iteration: 4, HV: 0.0  Out: [WARNING 09-29 17:14:48] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 5, HV: 0.0  Out: [WARNING 09-29 17:14:50] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 6, HV: 0.0  Out: [WARNING 09-29 17:14:50] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 7, HV: 2.369795709893773  Out: [WARNING 09-29 17:14:51] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 8, HV: 5.117636754883208  Out: [WARNING 09-29 17:14:51] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 9, HV: 5.117636754883208  Out: [WARNING 09-29 17:14:52] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 10, HV: 10.971755974955732  Out: [WARNING 09-29 17:14:52] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 11, HV: 28.684916683639273  Out: [WARNING 09-29 17:14:53] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:14:53] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 12, HV: 28.698664839626133 Iteration: 13, HV: 29.40908850350919  Out: [WARNING 09-29 17:14:54] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 14, HV: 29.40908850350919  Out: [WARNING 09-29 17:14:55] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 15, HV: 29.42945796196096  Out: [WARNING 09-29 17:14:56] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal [WARNING 09-29 17:14:56] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 16, HV: 31.971488957077298 Iteration: 17, HV: 46.903563553420206  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: [WARNING 09-29 17:14:57] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 18, HV: 46.903563553420206  Out: [WARNING 09-29 17:14:58] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 19, HV: 46.933240054891314  Out: [WARNING 09-29 17:14:59] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 20, HV: 47.05783987440818  Out: [WARNING 09-29 17:15:01] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 21, HV: 47.28153096130532  Out: [WARNING 09-29 17:15:02] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 22, HV: 47.50811056143222  Out: [WARNING 09-29 17:15:03] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: /Users/cristianlara/Projects/botorch/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal  Out: Iteration: 23, HV: 47.58253530930928  Out: [WARNING 09-29 17:15:03] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [WARNING 09-29 17:15:03] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: Iteration: 24, HV: 47.67812108951956  ","version":"Next","tagName":"h2"},{"title":"Plot qNParEGO Pareto Frontier based on model posterior​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#plot-qnparego-pareto-frontier-based-on-model-posterior","content":" The plotted points are samples from the fitted model's posterior, not observed samples.  frontier = compute_posterior_pareto_frontier( experiment=parego_experiment, data=parego_experiment.fetch_data(), primary_objective=metric_b, secondary_objective=metric_a, absolute_metrics=[&quot;a&quot;, &quot;b&quot;], num_points=20, ) render(plot_pareto_frontier(frontier, CI_level=0.90))   loading...  ","version":"Next","tagName":"h2"},{"title":"Plot empirical data​","type":1,"pageTitle":"Multi-Objective Optimization Ax API","url":"/Ax/docs/tutorials/multiobjective_optimization/#plot-empirical-data","content":" Plot observed hypervolume, with color representing the iteration that a point was generated on.​  To examine optimization process from another perspective, we plot the collected observations under each algorithm where the color corresponds to the BO iteration at which the point was collected. The plot on the right for $q$NEHVI shows that the $q$NEHVI quickly identifies the Pareto frontier and most of its evaluations are very close to the Pareto frontier. $q$NParEGO also identifies has many observations close to the Pareto frontier, but relies on optimizing random scalarizations, which is a less principled way of optimizing the Pareto front compared to $q$NEHVI, which explicitly attempts focuses on improving the Pareto front. Sobol generates random points and has few points close to the Pareto front.  import matplotlib import numpy as np from matplotlib import pyplot as plt from matplotlib.cm import ScalarMappable %matplotlib inline fig, axes = plt.subplots(1, 3, figsize=(20, 6)) algos = [&quot;Sobol&quot;, &quot;qNParEGO&quot;, &quot;qNEHVI&quot;] outcomes_list = [sobol_outcomes, parego_outcomes, ehvi_outcomes] cm = matplotlib.colormaps[&quot;viridis&quot;] BATCH_SIZE = 1 n_results = N_BATCH * BATCH_SIZE + N_INIT batch_number = torch.cat( [ torch.zeros(N_INIT), torch.arange(1, N_BATCH + 1).repeat(BATCH_SIZE, 1).t().reshape(-1), ] ).numpy() for i, train_obj in enumerate(outcomes_list): x = i sc = axes[x].scatter( train_obj[:n_results, 0], train_obj[:n_results, 1], c=batch_number[:n_results], alpha=0.8, ) axes[x].set_title(algos[i]) axes[x].set_xlabel(&quot;Objective 1&quot;) axes[x].set_xlim(-150, 5) axes[x].set_ylim(-15, 0) axes[0].set_ylabel(&quot;Objective 2&quot;) norm = plt.Normalize(batch_number.min(), batch_number.max()) sm = ScalarMappable(norm=norm, cmap=cm) sm.set_array([]) fig.subplots_adjust(right=0.9) cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7]) cbar = fig.colorbar(sm, cax=cbar_ax) cbar.ax.set_title(&quot;Iteration&quot;)   Out: Matplotlib is building the font cache; this may take a moment.  Out: Text(0.5, 1.0, 'Iteration')    Hypervolume statistics  The hypervolume of the space dominated by points that dominate the reference point.  Plot the results​  The plot below shows a common metric of multi-objective optimization performance when the true Pareto frontier is known: the log difference between the hypervolume of the true Pareto front and the hypervolume of the approximate Pareto front identified by each algorithm. The log hypervolume difference is plotted at each step of the optimization for each of the algorithms.  The plot show that $q$NEHVI vastly outperforms $q$NParEGO which outperforms the Sobol baseline.  iters = np.arange(1, N_BATCH + 1) log_hv_difference_sobol = np.log10(branin_currin.max_hv - np.asarray(sobol_hv_list))[ : N_BATCH + 1 ] log_hv_difference_parego = np.log10(branin_currin.max_hv - np.asarray(parego_hv_list))[ : N_BATCH + 1 ] log_hv_difference_ehvi = np.log10(branin_currin.max_hv - np.asarray(ehvi_hv_list))[ : N_BATCH + 1 ] fig, ax = plt.subplots(1, 1, figsize=(8, 6)) ax.plot(iters, log_hv_difference_sobol, label=&quot;Sobol&quot;, linewidth=1.5) ax.plot(iters, log_hv_difference_parego, label=&quot;qNParEGO&quot;, linewidth=1.5) ax.plot(iters, log_hv_difference_ehvi, label=&quot;qNEHVI&quot;, linewidth=1.5) ax.set( xlabel=&quot;number of observations (beyond initial points)&quot;, ylabel=&quot;Log Hypervolume Difference&quot;, ) ax.legend(loc=&quot;lower right&quot;)   Out: &lt;matplotlib.legend.Legend at 0x31ac1d070&gt;   ","version":"Next","tagName":"h2"},{"title":"Hyperparameter Optimization on Slurm via SubmitIt","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/submitit/","content":"","keywords":"","version":"Next"},{"title":"Importing Necessary Libraries​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#importing-necessary-libraries","content":" Let's start by importing the necessary libraries.  import time from ax.service.ax_client import AxClient, ObjectiveProperties from ax.utils.notebook.plotting import render from ax.service.utils.report_utils import exp_to_df from submitit import AutoExecutor, LocalJob, DebugJob   ","version":"Next","tagName":"h2"},{"title":"Defining the Function to Optimize​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#defining-the-function-to-optimize","content":" We'll define a simple function to optimize. This function takes two parameters, and returns a single metric.  def evaluate(parameters): x = parameters[&quot;x&quot;] y = parameters[&quot;y&quot;] return {&quot;result&quot;: (x - 3)**2 + (y - 4)**2}   Note: SubmitIt'sCommandFunctionallows you to define commands to run on the node and then redirects the standard output.  ","version":"Next","tagName":"h2"},{"title":"Setting up Ax​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#setting-up-ax","content":" We'll use Ax's Service API for this example. We start by initializing an AxClient and creating an experiment.  ax_client = AxClient() ax_client.create_experiment( name=&quot;my_experiment&quot;, parameters=[ {&quot;name&quot;: &quot;x&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [-10.0, 10.0]}, {&quot;name&quot;: &quot;y&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [-10.0, 10.0]}, ], objectives={&quot;result&quot;: ObjectiveProperties(minimize=True)}, parameter_constraints=[&quot;x + y &lt;= 2.0&quot;], # Optional. )   Out: [INFO 09-29 17:05:24] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 17:05:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:05:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter y. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:05:24] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x', parameter_type=FLOAT, range=[-10.0, 10.0]), RangeParameter(name='y', parameter_type=FLOAT, range=[-10.0, 10.0])], parameter_constraints=[ParameterConstraint(1.0*x + 1.0*y &lt;= 2.0)]).  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:05:24] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  Other commonly used parameters typesinclude choice parameters and fixed parameters.  Tip 1: you can specify additional information for parameters such as log_scale, if a parameter operates at a log-scale and is_ordered for choice parameters that have a meaningful ordering.  Tip 2: Ax is an excellent choice for multi-objective optimization problems when there are multiple competing objectives and the goal is to find all Pareto-optimal solutions.  Tip 3: One can define constraints on both the parameters and the outcome.  ","version":"Next","tagName":"h2"},{"title":"Setting up SubmitIt​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#setting-up-submitit","content":" We'll use SubmitIt's AutoExecutor for this example. We start by initializing anAutoExecutor, and setting a few commonly used parameters.  # Log folder and cluster. Specify cluster='local' or cluster='debug' to run the jobs locally during development. # When we're are ready for deployment, switch to cluster='slurm' executor = AutoExecutor(folder=&quot;/tmp/submitit_runs&quot;, cluster='debug') executor.update_parameters(timeout_min=60) # Timeout of the slurm job. Not including slurm scheduling delay. executor.update_parameters(cpus_per_task=2)   Other commonly used Slurm parameters include partition, ntasks_per_node,cpus_per_task, cpus_per_gpu, gpus_per_node, gpus_per_task, qos, mem,mem_per_gpu, mem_per_cpu, account.  ","version":"Next","tagName":"h2"},{"title":"Running the Optimization Loop​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#running-the-optimization-loop","content":" Now, we're ready to run the optimization loop. We'll use an ask-tell loop, where we ask Ax for a suggestion, evaluate it using our function, and then tell Ax the result.  The example loop schedules new jobs whenever there is availability. For tasks that take a similar amount of time regardless of the parameters, it may make more sense to wait for the whole batch to finish before scheduling the next (so ax can make better informed parameter choices).  Note that get_next_trials may not use all available num_parallel_jobs if it doesn't have good parameter candidates to run.  total_budget = 10 num_parallel_jobs = 3 jobs = [] submitted_jobs = 0 # Run until all the jobs have finished and our budget is used up. while submitted_jobs &lt; total_budget or jobs: for job, trial_index in jobs[:]: # Poll if any jobs completed # Local and debug jobs don't run until .result() is called. if job.done() or type(job) in [LocalJob, DebugJob]: result = job.result() ax_client.complete_trial(trial_index=trial_index, raw_data=result) jobs.remove((job, trial_index)) # Schedule new jobs if there is availablity trial_index_to_param, _ = ax_client.get_next_trials( max_trials=min(num_parallel_jobs - len(jobs), total_budget - submitted_jobs)) for trial_index, parameters in trial_index_to_param.items(): job = executor.submit(evaluate, parameters) submitted_jobs += 1 jobs.append((job, trial_index)) time.sleep(1) # Display the current trials. display(exp_to_df(ax_client.experiment)) # Sleep for a bit before checking the jobs again to avoid overloading the cluster. # If you have a large number of jobs, consider adding a sleep statement in the job polling loop aswell. time.sleep(30)   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:05:24] ax.service.ax_client: Generated new trial 0 with parameters {'x': -3.232278, 'y': -0.207863} using model Sobol.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:05:24] ax.service.ax_client: Generated new trial 1 with parameters {'x': -6.682506, 'y': 1.394327} using model Sobol.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.  Out: [INFO 09-29 17:05:24] ax.service.ax_client: Generated new trial 2 with parameters {'x': -7.990737, 'y': -8.715674} using model Sobol.  Out: [WARNING 09-29 17:05:27] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: [INFO 09-29 17:05:27] ax.service.utils.report_utils: No results present for the specified metrics [Metric('result')]. Returning arm parameters and metadata only.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tx\ty0\t0\t0_0\tRUNNING\tSobol\t-3.23228\t-0.207863 1\t1\t1_0\tRUNNING\tSobol\t-6.68251\t1.39433 2\t2\t2_0\tRUNNING\tSobol\t-7.99074\t-8.71567  Out: [INFO 09-29 17:05:57] ax.service.ax_client: Completed trial 0 with data: {'result': (56.547402, None)}.  Out: [INFO 09-29 17:05:57] ax.service.ax_client: Completed trial 1 with data: {'result': (100.540459, None)}.  Out: [INFO 09-29 17:05:57] ax.service.ax_client: Completed trial 2 with data: {'result': (282.484659, None)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:05:57] ax.service.ax_client: Generated new trial 3 with parameters {'x': -0.814856, 'y': -6.817524} using model Sobol.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:05:57] ax.service.ax_client: Generated new trial 4 with parameters {'x': -9.2751, 'y': 5.327883} using model Sobol.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:05:59] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tresult\tx\ty0\t0\t0_0\tCOMPLETED\tSobol\t56.5474\t-3.23228\t-0.207863 1\t1\t1_0\tCOMPLETED\tSobol\t100.54\t-6.68251\t1.39433 2\t2\t2_0\tCOMPLETED\tSobol\t282.485\t-7.99074\t-8.71567 3\t3\t3_0\tRUNNING\tSobol\tnan\t-0.814856\t-6.81752 4\t4\t4_0\tRUNNING\tSobol\tnan\t-9.2751\t5.32788  Out: [INFO 09-29 17:06:29] ax.service.ax_client: Completed trial 3 with data: {'result': (131.57195, None)}.  Out: [INFO 09-29 17:06:29] ax.service.ax_client: Completed trial 4 with data: {'result': (152.441357, None)}.  Out: [INFO 09-29 17:06:32] ax.service.ax_client: Generated new trial 5 with parameters {'x': 0.163909, 'y': 0.68992} using model BoTorch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.  Out: [INFO 09-29 17:06:34] ax.service.ax_client: Generated new trial 6 with parameters {'x': -1.18158, 'y': 3.18158} using model BoTorch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.  Out: [INFO 09-29 17:06:37] ax.service.ax_client: Generated new trial 7 with parameters {'x': 2.187537, 'y': -1.305886} using model BoTorch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:06:40] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tresult\tx\ty0\t0\t0_0\tCOMPLETED\tSobol\t56.5474\t-3.23228\t-0.207863 1\t1\t1_0\tCOMPLETED\tSobol\t100.54\t-6.68251\t1.39433 2\t2\t2_0\tCOMPLETED\tSobol\t282.485\t-7.99074\t-8.71567 3\t3\t3_0\tCOMPLETED\tSobol\t131.572\t-0.814856\t-6.81752 4\t4\t4_0\tCOMPLETED\tSobol\t152.441\t-9.2751\t5.32788 5\t5\t5_0\tRUNNING\tBoTorch\tnan\t0.163909\t0.68992 6\t6\t6_0\tRUNNING\tBoTorch\tnan\t-1.18158\t3.18158 7\t7\t7_0\tRUNNING\tBoTorch\tnan\t2.18754\t-1.30589  Out: [INFO 09-29 17:07:10] ax.service.ax_client: Completed trial 5 with data: {'result': (19.000041, None)}.  Out: [INFO 09-29 17:07:10] ax.service.ax_client: Completed trial 6 with data: {'result': (18.155422, None)}.  Out: [INFO 09-29 17:07:10] ax.service.ax_client: Completed trial 7 with data: {'result': (28.812522, None)}.  Out: [INFO 09-29 17:07:12] ax.service.ax_client: Generated new trial 8 with parameters {'x': 10.0, 'y': -8.481359} using model BoTorch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.  Out: [INFO 09-29 17:07:14] ax.service.ax_client: Generated new trial 9 with parameters {'x': 0.418894, 'y': 1.581106} using model BoTorch.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/core/data.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation. [WARNING 09-29 17:07:17] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tresult\tx\ty0\t0\t0_0\tCOMPLETED\tSobol\t56.5474\t-3.23228\t-0.207863 1\t1\t1_0\tCOMPLETED\tSobol\t100.54\t-6.68251\t1.39433 2\t2\t2_0\tCOMPLETED\tSobol\t282.485\t-7.99074\t-8.71567 3\t3\t3_0\tCOMPLETED\tSobol\t131.572\t-0.814856\t-6.81752 4\t4\t4_0\tCOMPLETED\tSobol\t152.441\t-9.2751\t5.32788 5\t5\t5_0\tCOMPLETED\tBoTorch\t19\t0.163909\t0.68992 6\t6\t6_0\tCOMPLETED\tBoTorch\t18.1554\t-1.18158\t3.18158 7\t7\t7_0\tCOMPLETED\tBoTorch\t28.8125\t2.18754\t-1.30589 8\t8\t8_0\tRUNNING\tBoTorch\tnan\t10\t-8.48136 9\t9\t9_0\tRUNNING\tBoTorch\tnan\t0.418894\t1.58111  Out: [INFO 09-29 17:07:47] ax.service.ax_client: Completed trial 8 with data: {'result': (204.784334, None)}.  Out: [INFO 09-29 17:07:47] ax.service.ax_client: Completed trial 9 with data: {'result': (12.513156, None)}.  Out: [WARNING 09-29 17:07:47] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\tresult\tx\ty0\t0\t0_0\tCOMPLETED\tSobol\t56.5474\t-3.23228\t-0.207863 1\t1\t1_0\tCOMPLETED\tSobol\t100.54\t-6.68251\t1.39433 2\t2\t2_0\tCOMPLETED\tSobol\t282.485\t-7.99074\t-8.71567 3\t3\t3_0\tCOMPLETED\tSobol\t131.572\t-0.814856\t-6.81752 4\t4\t4_0\tCOMPLETED\tSobol\t152.441\t-9.2751\t5.32788 5\t5\t5_0\tCOMPLETED\tBoTorch\t19\t0.163909\t0.68992 6\t6\t6_0\tCOMPLETED\tBoTorch\t18.1554\t-1.18158\t3.18158 7\t7\t7_0\tCOMPLETED\tBoTorch\t28.8125\t2.18754\t-1.30589 8\t8\t8_0\tCOMPLETED\tBoTorch\t204.784\t10\t-8.48136 9\t9\t9_0\tCOMPLETED\tBoTorch\t12.5132\t0.418894\t1.58111  ","version":"Next","tagName":"h2"},{"title":"Finally​","type":1,"pageTitle":"Hyperparameter Optimization on Slurm via SubmitIt","url":"/Ax/docs/tutorials/submitit/#finally","content":" We can retrieve the best parameters and render the response surface.  best_parameters, (means, covariances) = ax_client.get_best_parameters() print(f'Best set of parameters: {best_parameters}') print(f'Mean objective value: {means}') # The covariance is only meaningful when multiple objectives are present. render(ax_client.get_contour_plot())   Out: [INFO 09-29 17:08:17] ax.service.ax_client: Retrieving contour plot with parameter 'x' on X-axis and 'y' on Y-axis, for metric 'result'. Remaining parameters are affixed to the middle of their range.  Out: Best set of parameters: {'x': 0.4188942902669357, 'y': 1.5811057097266623} Mean objective value: {'result': 12.395281019540292}  loading... ","version":"Next","tagName":"h2"},{"title":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/sebo/","content":"","keywords":"","version":"Next"},{"title":"Problem Setup​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#problem-setup","content":" In this simple experiment we use the Branin function embedded in a 10-dimensional space. Additional resources:  To set up a custom metric for your problem, refer to the dedicated section of the Developer API tutorial:https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics.To avoid needing to setup up custom metrics by Ax Service API:https://ax.dev/tutorials/gpei_hartmann_service.html.  aug_dim = 8 # evaluation function def branin_augment(x_vec, augment_dim): assert len(x_vec) == augment_dim x1, x2 = ( 15 * x_vec[0] - 5, 15 * x_vec[1], ) # Only dimensions 0 and augment_dim-1 affect the value of the function t1 = x2 - 5.1 / (4 * math.pi**2) * x1**2 + 5 / math.pi * x1 - 6 t2 = 10 * (1 - 1 / (8 * math.pi)) * np.cos(x1) return t1**2 + t2 + 10   class AugBraninMetric(NoisyFunctionMetric): def f(self, x: np.ndarray) -&gt; float: return checked_cast(float, branin_augment(x_vec=x, augment_dim=aug_dim)) # Create search space in Ax search_space = SearchSpace( parameters=[ RangeParameter( name=f&quot;x{i}&quot;, parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0 ) for i in range(aug_dim) ] )   # Create optimization goals optimization_config = OptimizationConfig( objective=Objective( metric=AugBraninMetric( name=&quot;objective&quot;, param_names=[f&quot;x{i}&quot; for i in range(aug_dim)], noise_sd=None, # Set noise_sd=None if you want to learn the noise, otherwise it defaults to 1e-6 ), minimize=True, ) ) # Experiment experiment = Experiment( name=&quot;sebo_experiment&quot;, search_space=search_space, optimization_config=optimization_config, runner=SyntheticRunner(), ) # target sparse point to regularize towards to. Here we set target sparse value being zero for all the parameters. target_point = torch.tensor([0 for _ in range(aug_dim)], **tkwargs)   ","version":"Next","tagName":"h2"},{"title":"Run optimization loop​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#run-optimization-loop","content":" N_INIT = 10 if SMOKE_TEST: N_BATCHES = 1 BATCH_SIZE = 1 SURROGATE_CLASS = None # Auto-pick SingleTaskGP else: N_BATCHES = 4 BATCH_SIZE = 5 SURROGATE_CLASS = SaasFullyBayesianSingleTaskGP print(f&quot;Doing {N_INIT + N_BATCHES * BATCH_SIZE} evaluations&quot;)   Out: Doing 30 evaluations  # Initial Sobol points sobol = Models.SOBOL(search_space=experiment.search_space) for _ in range(N_INIT): experiment.new_trial(sobol.gen(1)).run()   data = experiment.fetch_data() for i in range(N_BATCHES): model = Models.BOTORCH_MODULAR( experiment=experiment, data=data, surrogate=Surrogate(botorch_model_class=SURROGATE_CLASS), # can use SAASGP (i.e. SaasFullyBayesianSingleTaskGP) for high-dim cases search_space=experiment.search_space, botorch_acqf_class=qNoisyExpectedHypervolumeImprovement, acquisition_class=SEBOAcquisition, acquisition_options={ &quot;penalty&quot;: &quot;L0_norm&quot;, # it can be L0_norm or L1_norm. &quot;target_point&quot;: target_point, &quot;sparsity_threshold&quot;: aug_dim, }, torch_device=tkwargs['device'], ) generator_run = model.gen(BATCH_SIZE) trial = experiment.new_batch_trial(generator_run=generator_run) trial.run() new_data = trial.fetch_data(metrics=list(experiment.metrics.values())) data = Data.from_multiple_data([data, new_data]) print(f&quot;Iteration: {i}, Best so far: {data.df['mean'].min():.3f}&quot;)   Out: Iteration: 0, Best so far: 2.494  Out: Iteration: 1, Best so far: 2.494  Out: Iteration: 2, Best so far: 2.054  Out: Iteration: 3, Best so far: 2.054  ","version":"Next","tagName":"h2"},{"title":"Plot sparisty vs objective​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#plot-sparisty-vs-objective","content":" Visualize the objective and sparsity trade-offs using SEBO. Each point represent designs along the Pareto frontier found by SEBO. The x-axis corresponds to the number of active parameters used, i.e. non-sparse parameters, and the y-axis corresponds the best identified objective values. Based on this, decision-makers balance both simplicity/interpretability of generated policies and optimization performance when deciding which configuration to use.  def nnz_exact(x, sparse_point): return len(x) - (np.array(x) == np.array(sparse_point)).sum() df = data.df df['L0_norm'] = df['arm_name'].apply(lambda d: nnz_exact(list(experiment.arms_by_name[d].parameters.values()), [0 for _ in range(aug_dim)]) )   result_by_sparsity = {l: df[df.L0_norm &lt;= l]['mean'].min() for l in range(1, aug_dim+1)} result_by_sparsity   Out: {1: 6.117271199892615, 2: 3.0181950045127772, 3: 3.0181950045127772, 4: 2.0536196356479337, 5: 2.0536196356479337, 6: 2.0536196356479337, 7: 2.0536196356479337, 8: 2.0536196356479337}  fig, ax = plt.subplots(figsize=(8, 6)) ax.plot(list(result_by_sparsity.keys()), list(result_by_sparsity.values()), '.b-', label=&quot;sebo&quot;, markersize=10) ax.grid(True) ax.set_title(f&quot;Branin, D={aug_dim}&quot;, fontsize=20) ax.set_xlabel(&quot;Number of active parameters&quot;, fontsize=20) ax.set_ylabel(&quot;Best value found&quot;, fontsize=20) # ax.legend(fontsize=18) plt.show()     Demo of Using GenerationStrategy and Service API  Please check Service API tutorialfor more detailed information.  ","version":"Next","tagName":"h2"},{"title":"Create GenerationStrategy​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#create-generationstrategy","content":" gs = GenerationStrategy( name=&quot;SEBO_L0&quot;, steps=[ GenerationStep( # Initialization step model=Models.SOBOL, num_trials=N_INIT, ), GenerationStep( # BayesOpt step model=Models.BOTORCH_MODULAR, # No limit on how many generator runs will be produced num_trials=-1, model_kwargs={ # Kwargs to pass to `BoTorchModel.__init__` &quot;surrogate&quot;: Surrogate(botorch_model_class=SURROGATE_CLASS), &quot;acquisition_class&quot;: SEBOAcquisition, &quot;botorch_acqf_class&quot;: qNoisyExpectedHypervolumeImprovement, &quot;acquisition_options&quot;: { &quot;penalty&quot;: &quot;L0_norm&quot;, # it can be L0_norm or L1_norm. &quot;target_point&quot;: target_point, &quot;sparsity_threshold&quot;: aug_dim, }, }, ) ] )   ","version":"Next","tagName":"h2"},{"title":"Initialize client and set up experiment​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#initialize-client-and-set-up-experiment","content":" ax_client = AxClient(generation_strategy=gs) experiment_parameters = [ { &quot;name&quot;: f&quot;x{i}&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0, 1], &quot;value_type&quot;: &quot;float&quot;, &quot;log_scale&quot;: False, } for i in range(aug_dim) ] objective_metrics = { &quot;objective&quot;: ObjectiveProperties(minimize=False, threshold=-10), } ax_client.create_experiment( name=&quot;branin_augment_sebo_experiment&quot;, parameters=experiment_parameters, objectives=objective_metrics, )   Out: [INFO 09-29 18:10:47] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 18:10:47] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x0', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x7', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).  ","version":"Next","tagName":"h2"},{"title":"Define evaluation function​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#define-evaluation-function","content":" def evaluation(parameters): # put parameters into 1-D array x = [parameters.get(param[&quot;name&quot;]) for param in experiment_parameters] res = branin_augment(x_vec=x, augment_dim=aug_dim) eval_res = { # flip the sign to maximize &quot;objective&quot;: (res * -1, 0.0), } return eval_res   ","version":"Next","tagName":"h2"},{"title":"Run optimization loop​","type":1,"pageTitle":"Sparsity Exploration Bayesian Optimization (SEBO) Ax API","url":"/Ax/docs/tutorials/sebo/#run-optimization-loop-1","content":" Running only 1 BO trial for demonstration.  for _ in range(N_INIT + 1): parameters, trial_index = ax_client.get_next_trial() res = evaluation(parameters) ax_client.complete_trial(trial_index=trial_index, raw_data=res)   Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 0 with parameters {'x0': 0.737621, 'x1': 0.954931, 'x2': 0.942383, 'x3': 0.351454, 'x4': 0.118169, 'x5': 0.912978, 'x6': 0.695522, 'x7': 0.614025} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 0 with data: {'objective': (-194.266845, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 1 with parameters {'x0': 0.123834, 'x1': 0.158728, 'x2': 0.076607, 'x3': 0.971911, 'x4': 0.59112, 'x5': 0.289113, 'x6': 0.230608, 'x7': 0.404815} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 1 with data: {'objective': (-98.333617, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 2 with parameters {'x0': 0.354525, 'x1': 0.528493, 'x2': 0.546903, 'x3': 0.166528, 'x4': 0.926084, 'x5': 0.088412, 'x6': 0.964837, 'x7': 0.230184} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 2 with data: {'objective': (-24.978708, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 3 with parameters {'x0': 0.975175, 'x1': 0.357901, 'x2': 0.433065, 'x3': 0.545263, 'x4': 0.390628, 'x5': 0.707421, 'x6': 0.49993, 'x7': 0.751894} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 3 with data: {'objective': (-7.977363, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 4 with parameters {'x0': 0.750888, 'x1': 0.655229, 'x2': 0.170585, 'x3': 0.731026, 'x4': 0.373581, 'x5': 0.86337, 'x6': 0.751092, 'x7': 0.106566} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 4 with data: {'objective': (-95.795886, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 5 with parameters {'x0': 0.388138, 'x1': 0.482074, 'x2': 0.786987, 'x3': 0.101856, 'x4': 0.838639, 'x5': 0.495093, 'x6': 0.286185, 'x7': 0.882183} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 5 with data: {'objective': (-22.549336, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 6 with parameters {'x0': 0.149119, 'x1': 0.83008, 'x2': 0.339518, 'x3': 0.783172, 'x4': 0.675559, 'x5': 0.133286, 'x6': 0.5184, 'x7': 0.736726} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 6 with data: {'objective': (-2.21565, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 7 with parameters {'x0': 0.520685, 'x1': 0.032534, 'x2': 0.703951, 'x3': 0.411289, 'x4': 0.147987, 'x5': 0.506419, 'x6': 0.053486, 'x7': 0.27361} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 7 with data: {'objective': (-5.16189, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 8 with parameters {'x0': 0.622784, 'x1': 0.605093, 'x2': 0.266506, 'x3': 0.060451, 'x4': 0.501436, 'x5': 0.642803, 'x6': 0.832134, 'x7': 0.02973} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 8 with data: {'objective': (-63.543825, 0.0)}.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Generated new trial 9 with parameters {'x0': 0.234521, 'x1': 0.274221, 'x2': 0.651935, 'x3': 0.680884, 'x4': 0.036345, 'x5': 0.028154, 'x6': 0.359411, 'x7': 0.988481} using model Sobol.  Out: [INFO 09-29 18:10:47] ax.service.ax_client: Completed trial 9 with data: {'objective': (-31.365708, 0.0)}.  Out: [INFO 09-29 18:11:11] ax.service.ax_client: Generated new trial 10 with parameters {'x0': 1.0, 'x1': 0.0, 'x2': 0.0, 'x3': 0.0, 'x4': 0.0, 'x5': 0.0, 'x6': 0.0, 'x7': 0.0} using model BoTorch.  Out: [INFO 09-29 18:11:11] ax.service.ax_client: Completed trial 10 with data: {'objective': (-10.960889, 0.0)}. ","version":"Next","tagName":"h2"},{"title":"Why Ax?","type":0,"sectionRef":"#","url":"/Ax/docs/why-ax","content":"Why Ax? Developers and researchers alike face problems which confront them with a large space of possible ways to configure something –– whether those are &quot;magic numbers&quot; used for infrastructure or compiler flags, learning rates or other hyperparameters in machine learning, or images and calls-to-action used in marketing promotions. Selecting and tuning these configurations can often take time, resources, and can affect the quality of user experiences. Ax is a machine learning system to help automate this process, so that researchers and developers can determine how to get the most out of their software in an optimally efficient way. Ax is a platform for optimizing any kind of experiment, including machine learning experiments, A/B tests, and simulations. Ax can optimize discrete configurations (e.g., variants of an A/B test) using multi-armed bandit optimization, and continuous (e.g., integer or floating point)-valued configurations using Bayesian optimization. This makes it suitable for a wide range of applications. Ax has been successfully applied to a variety of product, infrastructure, ML, and research applications at Facebook. Unique capabilities Support for noisy functions. Results of A/B tests and simulations with reinforcement learning agents often exhibit high amounts of noise. Ax supports state-of-the-art algorithms which work better than traditional Bayesian optimization in high-noise settings.Customization. Ax's developer API makes it easy to integrate custom data modeling and decision algorithms. This allows developers to build their own custom optimization services with minimal overhead.Multi-modal experimentation. Ax has first-class support for running and combining data from different types of experiments, such as &quot;offline&quot; simulation data and &quot;online&quot; data from real-world experiments.Multi-objective optimization. Ax supports multi-objective and constrained optimization which are common to real-world problems, like improving load time without increasing data use.","keywords":"","version":"Next"},{"title":"Tune a CNN on MNIST","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/tune_cnn_service/","content":"","keywords":"","version":"Next"},{"title":"1. Load MNIST data​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#1-load-mnist-data","content":" First, we need to load the MNIST data and partition it into training, validation, and test sets.  Note: this will download the dataset if necessary.  BATCH_SIZE = 512 train_loader, valid_loader, test_loader = load_mnist(batch_size=BATCH_SIZE)   Out: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz  Out: Failed to download (trying next): HTTP Error 403: Forbidden Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz  Out: 0%| | 0/9912422 [00:00&lt;?, ?it/s]  Out: 10%|███▎ | 983040/9912422 [00:00&lt;00:00, 9815665.87it/s]  Out: 23%|███████▍ | 2293760/9912422 [00:00&lt;00:00, 11596202.59it/s]  Out: 35%|███████████▏ | 3473408/9912422 [00:00&lt;00:00, 11286185.72it/s]  Out: 47%|██████████████▉ | 4620288/9912422 [00:00&lt;00:00, 10497514.38it/s]  Out: 58%|██████████████████▍ | 5701632/9912422 [00:00&lt;00:00, 10535690.23it/s]  Out: 68%|██████████████████████▌ | 6782976/9912422 [00:00&lt;00:00, 9754330.44it/s]  Out: 79%|█████████████████████████▉ | 7798784/9912422 [00:00&lt;00:00, 9225555.09it/s]  Out: 88%|█████████████████████████████▏ | 8749056/9912422 [00:00&lt;00:00, 9139495.92it/s]  Out: 98%|████████████████████████████████▎| 9699328/9912422 [00:01&lt;00:00, 8784117.93it/s]  Out: 100%|█████████████████████████████████| 9912422/9912422 [00:01&lt;00:00, 9512240.61it/s]  Out: Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 403: Forbidden Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz  Out: Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz  Out: 0%| | 0/28881 [00:00&lt;?, ?it/s]  Out: 100%|█████████████████████████████████████| 28881/28881 [00:00&lt;00:00, 2988201.04it/s]  Out: Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Failed to download (trying next): HTTP Error 403: Forbidden Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz  Out: Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz  Out: 0%| | 0/1648877 [00:00&lt;?, ?it/s]  Out: 60%|████████████████████▎ | 983040/1648877 [00:00&lt;00:00, 9783129.96it/s]  Out: 100%|█████████████████████████████████| 1648877/1648877 [00:00&lt;00:00, 4786597.45it/s]  Out: Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 403: Forbidden Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz  Out: 0%| | 0/4542 [00:00&lt;?, ?it/s]  Out: 100%|███████████████████████████████████████| 4542/4542 [00:00&lt;00:00, 2998194.64it/s]  Out: Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw  ","version":"Next","tagName":"h2"},{"title":"2. Initialize Client​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#2-initialize-client","content":" Create a client object to interface with Ax APIs. By default this runs locally without storage.  ax_client = AxClient()   Out: [INFO 09-29 17:03:05] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  ","version":"Next","tagName":"h2"},{"title":"3. Set up experiment​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#3-set-up-experiment","content":" An experiment consists of a search space (parameters and parameter constraints) andoptimization configuration (objective name, minimization setting, and outcome constraints).  # Create an experiment with required arguments: name, parameters, and objective_name. ax_client.create_experiment( name=&quot;tune_cnn_on_mnist&quot;, # The name of the experiment. parameters=[ { &quot;name&quot;: &quot;lr&quot;, # The name of the parameter. &quot;type&quot;: &quot;range&quot;, # The type of the parameter (&quot;range&quot;, &quot;choice&quot; or &quot;fixed&quot;). &quot;bounds&quot;: [1e-6, 0.4], # The bounds for range parameters. # &quot;values&quot; The possible values for choice parameters . # &quot;value&quot; The fixed value for fixed parameters. &quot;value_type&quot;: &quot;float&quot;, # Optional, the value type (&quot;int&quot;, &quot;float&quot;, &quot;bool&quot; or &quot;str&quot;). Defaults to inference from type of &quot;bounds&quot;. &quot;log_scale&quot;: True, # Optional, whether to use a log scale for range parameters. Defaults to False. # &quot;is_ordered&quot; Optional, a flag for choice parameters. }, { &quot;name&quot;: &quot;momentum&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], }, ], objectives={&quot;accuracy&quot;: ObjectiveProperties(minimize=False)}, # The objective name and minimization setting. # parameter_constraints: Optional, a list of strings of form &quot;p1 &gt;= p2&quot; or &quot;p1 + p2 &lt;= some_bound&quot;. # outcome_constraints: Optional, a list of strings of form &quot;constrained_metric &lt;= some_bound&quot;. )   Out: [INFO 09-29 17:03:05] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter momentum. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:03:05] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='lr', parameter_type=FLOAT, range=[1e-06, 0.4], log_scale=True), RangeParameter(name='momentum', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:03:05] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  ","version":"Next","tagName":"h2"},{"title":"4. Define how to evaluate trials​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#4-define-how-to-evaluate-trials","content":" First we define a simple CNN class to classify the MNIST images  class CNN(nn.Module): def __init__(self) -&gt; None: super().__init__() self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1) self.fc1 = nn.Linear(8 * 8 * 20, 64) self.fc2 = nn.Linear(64, 10) def forward(self, x: Tensor) -&gt; Tensor: x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 3, 3) x = x.view(-1, 8 * 8 * 20) x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=-1)   In this tutorial, we want to optimize classification accuracy on the validation set as a function of the learning rate and momentum. The train_evaluate function takes in a parameterization (set of parameter values), computes the classification accuracy, and returns that metric.  def train_evaluate(parameterization): &quot;&quot;&quot; Train the model and then compute an evaluation metric. In this tutorial, the CNN utils package is doing a lot of work under the hood: - `train` initializes the network, defines the loss function and optimizer, performs the training loop, and returns the trained model. - `evaluate` computes the accuracy of the model on the evaluation dataset and returns the metric. For your use case, you can define training and evaluation functions of your choosing. &quot;&quot;&quot; net = CNN() net = train( net=net, train_loader=train_loader, parameters=parameterization, dtype=dtype, device=device, ) return evaluate( net=net, data_loader=valid_loader, dtype=dtype, device=device, )   ","version":"Next","tagName":"h2"},{"title":"5. Run optimization loop​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#5-run-optimization-loop","content":" First we use attach_trial to attach a custom trial with manually-chosen parameters. This step is optional, but we include it here to demonstrate adding manual trials and to serve as a baseline model with decent performance.  # Attach the trial ax_client.attach_trial( parameters={&quot;lr&quot;: 0.000026, &quot;momentum&quot;: 0.58} ) # Get the parameters and run the trial baseline_parameters = ax_client.get_trial_parameters(trial_index=0) ax_client.complete_trial(trial_index=0, raw_data=train_evaluate(baseline_parameters))   Out: [INFO 09-29 17:03:06] ax.core.experiment: Attached custom parameterizations [{'lr': 2.6e-05, 'momentum': 0.58}] as trial 0.  Out: [INFO 09-29 17:03:09] ax.service.ax_client: Completed trial 0 with data: {'accuracy': (0.841833, None)}.  Now we start the optimization loop.  At each step, the user queries the client for a new trial then submits the evaluation of that trial back to the client.  Note that Ax auto-selects an appropriate optimization algorithm based on the search space. For more advanced use cases that require a specific optimization algorithm, pass a generation_strategy argument into the AxClient constructor. Note that when Bayesian Optimization is used, generating new trials may take a few minutes.  for i in range(25): parameters, trial_index = ax_client.get_next_trial() # Local evaluation here can be replaced with deployment to external system. ax_client.complete_trial(trial_index=trial_index, raw_data=train_evaluate(parameters))   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:03:10] ax.service.ax_client: Generated new trial 1 with parameters {'lr': 0.009955, 'momentum': 0.633423} using model Sobol.  Out: [INFO 09-29 17:03:14] ax.service.ax_client: Completed trial 1 with data: {'accuracy': (0.100333, None)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:03:14] ax.service.ax_client: Generated new trial 2 with parameters {'lr': 9e-05, 'momentum': 0.335441} using model Sobol.  Out: [INFO 09-29 17:03:18] ax.service.ax_client: Completed trial 2 with data: {'accuracy': (0.884167, None)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:03:18] ax.service.ax_client: Generated new trial 3 with parameters {'lr': 2e-06, 'momentum': 0.902833} using model Sobol.  Out: [INFO 09-29 17:03:23] ax.service.ax_client: Completed trial 3 with data: {'accuracy': (0.678667, None)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:03:23] ax.service.ax_client: Generated new trial 4 with parameters {'lr': 0.10174, 'momentum': 0.065787} using model Sobol.  Out: [INFO 09-29 17:03:25] ax.service.ax_client: Completed trial 4 with data: {'accuracy': (0.100333, None)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:03:25] ax.service.ax_client: Generated new trial 5 with parameters {'lr': 0.017004, 'momentum': 0.802601} using model Sobol.  Out: [INFO 09-29 17:03:31] ax.service.ax_client: Completed trial 5 with data: {'accuracy': (0.088167, None)}.  Out: [INFO 09-29 17:03:31] ax.service.ax_client: Generated new trial 6 with parameters {'lr': 3.5e-05, 'momentum': 0.0} using model BoTorch.  Out: [INFO 09-29 17:03:36] ax.service.ax_client: Completed trial 6 with data: {'accuracy': (0.756167, None)}.  Out: [INFO 09-29 17:03:36] ax.service.ax_client: Generated new trial 7 with parameters {'lr': 8.5e-05, 'momentum': 1.0} using model BoTorch.  Out: [INFO 09-29 17:03:41] ax.service.ax_client: Completed trial 7 with data: {'accuracy': (0.908, None)}.  Out: [INFO 09-29 17:03:41] ax.service.ax_client: Generated new trial 8 with parameters {'lr': 4.7e-05, 'momentum': 1.0} using model BoTorch.  Out: [INFO 09-29 17:03:46] ax.service.ax_client: Completed trial 8 with data: {'accuracy': (0.7935, None)}.  Out: [INFO 09-29 17:03:46] ax.service.ax_client: Generated new trial 9 with parameters {'lr': 0.000186, 'momentum': 0.914201} using model BoTorch.  Out: [INFO 09-29 17:03:51] ax.service.ax_client: Completed trial 9 with data: {'accuracy': (0.821333, None)}.  Out: [INFO 09-29 17:03:51] ax.service.ax_client: Generated new trial 10 with parameters {'lr': 0.000155, 'momentum': 0.0} using model BoTorch.  Out: [INFO 09-29 17:03:56] ax.service.ax_client: Completed trial 10 with data: {'accuracy': (0.855333, None)}.  Out: [INFO 09-29 17:03:57] ax.service.ax_client: Generated new trial 11 with parameters {'lr': 9.7e-05, 'momentum': 0.0} using model BoTorch.  Out: [INFO 09-29 17:04:00] ax.service.ax_client: Completed trial 11 with data: {'accuracy': (0.847833, None)}.  Out: [INFO 09-29 17:04:00] ax.service.ax_client: Generated new trial 12 with parameters {'lr': 0.000121, 'momentum': 1.0} using model BoTorch.  Out: [INFO 09-29 17:04:04] ax.service.ax_client: Completed trial 12 with data: {'accuracy': (0.850333, None)}.  Out: [INFO 09-29 17:04:04] ax.service.ax_client: Generated new trial 13 with parameters {'lr': 0.000116, 'momentum': 0.52957} using model BoTorch.  Out: [INFO 09-29 17:04:07] ax.service.ax_client: Completed trial 13 with data: {'accuracy': (0.917833, None)}.  Out: [INFO 09-29 17:04:08] ax.service.ax_client: Generated new trial 14 with parameters {'lr': 0.000174, 'momentum': 0.407224} using model BoTorch.  Out: [INFO 09-29 17:04:11] ax.service.ax_client: Completed trial 14 with data: {'accuracy': (0.907833, None)}.  Out: [INFO 09-29 17:04:11] ax.service.ax_client: Generated new trial 15 with parameters {'lr': 1e-06, 'momentum': 0.0} using model BoTorch.  Out: [INFO 09-29 17:04:15] ax.service.ax_client: Completed trial 15 with data: {'accuracy': (0.167833, None)}.  Out: [INFO 09-29 17:04:16] ax.service.ax_client: Generated new trial 16 with parameters {'lr': 8.9e-05, 'momentum': 0.520364} using model BoTorch.  Out: [INFO 09-29 17:04:20] ax.service.ax_client: Completed trial 16 with data: {'accuracy': (0.9085, None)}.  Out: [INFO 09-29 17:04:20] ax.service.ax_client: Generated new trial 17 with parameters {'lr': 0.000121, 'momentum': 0.420034} using model BoTorch.  Out: [INFO 09-29 17:04:24] ax.service.ax_client: Completed trial 17 with data: {'accuracy': (0.908833, None)}.  Out: [INFO 09-29 17:04:25] ax.service.ax_client: Generated new trial 18 with parameters {'lr': 9.3e-05, 'momentum': 0.602704} using model BoTorch.  Out: [INFO 09-29 17:04:29] ax.service.ax_client: Completed trial 18 with data: {'accuracy': (0.904333, None)}.  Out: [INFO 09-29 17:04:30] ax.modelbridge.base: Untransformed parameter 0.40000000000000013 greater than upper bound 0.4, clamping  Out: [INFO 09-29 17:04:30] ax.service.ax_client: Generated new trial 19 with parameters {'lr': 0.4, 'momentum': 1.0} using model BoTorch.  Out: [INFO 09-29 17:04:33] ax.service.ax_client: Completed trial 19 with data: {'accuracy': (0.0955, None)}.  Out: [INFO 09-29 17:04:34] ax.service.ax_client: Generated new trial 20 with parameters {'lr': 0.000101, 'momentum': 0.460781} using model BoTorch.  Out: [INFO 09-29 17:04:37] ax.service.ax_client: Completed trial 20 with data: {'accuracy': (0.906, None)}.  Out: [INFO 09-29 17:04:37] ax.service.ax_client: Generated new trial 21 with parameters {'lr': 0.000142, 'momentum': 0.507193} using model BoTorch.  Out: [INFO 09-29 17:04:40] ax.service.ax_client: Completed trial 21 with data: {'accuracy': (0.922167, None)}.  Out: [INFO 09-29 17:04:41] ax.service.ax_client: Generated new trial 22 with parameters {'lr': 0.000274, 'momentum': 0.0} using model BoTorch.  Out: [INFO 09-29 17:04:44] ax.service.ax_client: Completed trial 22 with data: {'accuracy': (0.863333, None)}.  Out: [INFO 09-29 17:04:45] ax.service.ax_client: Generated new trial 23 with parameters {'lr': 0.000139, 'momentum': 0.414705} using model BoTorch.  Out: [INFO 09-29 17:04:50] ax.service.ax_client: Completed trial 23 with data: {'accuracy': (0.920667, None)}.  Out: [INFO 09-29 17:04:51] ax.service.ax_client: Generated new trial 24 with parameters {'lr': 0.00011, 'momentum': 0.494636} using model BoTorch.  Out: [INFO 09-29 17:04:54] ax.service.ax_client: Completed trial 24 with data: {'accuracy': (0.911833, None)}.  Out: [INFO 09-29 17:04:55] ax.service.ax_client: Generated new trial 25 with parameters {'lr': 0.00018, 'momentum': 0.489022} using model BoTorch.  Out: [INFO 09-29 17:04:58] ax.service.ax_client: Completed trial 25 with data: {'accuracy': (0.902833, None)}.  ","version":"Next","tagName":"h2"},{"title":"How many trials can run in parallel?​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#how-many-trials-can-run-in-parallel","content":" By default, Ax restricts number of trials that can run in parallel for some optimization stages, in order to improve the optimization performance and reduce the number of trials that the optimization will require. To check the maximum parallelism for each optimization stage:  ax_client.get_max_parallelism()   Out: [(5, 5), (-1, 3)]  The output of this function is a list of tuples of form (number of trials, max parallelism), so the example above means &quot;the max parallelism is 5 for the first 5 trials and 3 for all subsequent trials.&quot; This is because the first 5 trials are produced quasi-randomly and can all be evaluated at once, and subsequent trials are produced via Bayesian optimization, which converges on optimal point in fewer trials when parallelism is limited. MaxParallelismReachedException indicates that the parallelism limit has been reached –– refer to the 'Service API Exceptions Meaning and Handling' section at the end of the tutorial for handling.  ","version":"Next","tagName":"h3"},{"title":"How to view all existing trials during optimization?​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#how-to-view-all-existing-trials-during-optimization","content":" ax_client.get_trials_data_frame()   Out: [WARNING 09-29 17:04:59] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  \ttrial_index\tarm_name\ttrial_status\tgeneration_method\taccuracy\tlr\tmomentum0\t0\t0_0\tCOMPLETED\tManual\t0.841833\t2.6e-05\t0.58 1\t1\t1_0\tCOMPLETED\tSobol\t0.100333\t0.009955\t0.633423 2\t2\t2_0\tCOMPLETED\tSobol\t0.884167\t9e-05\t0.335441 3\t3\t3_0\tCOMPLETED\tSobol\t0.678667\t2e-06\t0.902833 4\t4\t4_0\tCOMPLETED\tSobol\t0.100333\t0.10174\t0.065787 5\t5\t5_0\tCOMPLETED\tSobol\t0.088167\t0.017004\t0.802601 6\t6\t6_0\tCOMPLETED\tBoTorch\t0.756167\t3.5e-05\t0 7\t7\t7_0\tCOMPLETED\tBoTorch\t0.908\t8.5e-05\t1 8\t8\t8_0\tCOMPLETED\tBoTorch\t0.7935\t4.7e-05\t1 9\t9\t9_0\tCOMPLETED\tBoTorch\t0.821333\t0.000186\t0.914201 10\t10\t10_0\tCOMPLETED\tBoTorch\t0.855333\t0.000155\t0 11\t11\t11_0\tCOMPLETED\tBoTorch\t0.847833\t9.7e-05\t0 12\t12\t12_0\tCOMPLETED\tBoTorch\t0.850333\t0.000121\t1 13\t13\t13_0\tCOMPLETED\tBoTorch\t0.917833\t0.000116\t0.52957 14\t14\t14_0\tCOMPLETED\tBoTorch\t0.907833\t0.000174\t0.407224 15\t15\t15_0\tCOMPLETED\tBoTorch\t0.167833\t1e-06\t0 16\t16\t16_0\tCOMPLETED\tBoTorch\t0.9085\t8.9e-05\t0.520364 17\t17\t17_0\tCOMPLETED\tBoTorch\t0.908833\t0.000121\t0.420034 18\t18\t18_0\tCOMPLETED\tBoTorch\t0.904333\t9.3e-05\t0.602704 19\t19\t19_0\tCOMPLETED\tBoTorch\t0.0955\t0.4\t1 20\t20\t20_0\tCOMPLETED\tBoTorch\t0.906\t0.000101\t0.460781 21\t21\t21_0\tCOMPLETED\tBoTorch\t0.922167\t0.000142\t0.507193 22\t22\t22_0\tCOMPLETED\tBoTorch\t0.863333\t0.000274\t0 23\t23\t23_0\tCOMPLETED\tBoTorch\t0.920667\t0.000139\t0.414705 24\t24\t24_0\tCOMPLETED\tBoTorch\t0.911833\t0.00011\t0.494636 25\t25\t25_0\tCOMPLETED\tBoTorch\t0.902833\t0.00018\t0.489022  ","version":"Next","tagName":"h3"},{"title":"6. Retrieve best parameters​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#6-retrieve-best-parameters","content":" Once it's complete, we can access the best parameters found, as well as the corresponding metric values. Note that these parameters may not necessarily be the set that yielded the highest observed accuracy because Ax uses the highest modelpredicted accuracy to choose the best parameters (seehere for more details). Due to randomness in the data or the algorithm itself, using observed accuracy may result in choosing an outlier for the best set of parameters. Using the model predicted best will use the model to regularize the observations and reduce the likelihood of picking some outlier in the data.  best_parameters, values = ax_client.get_best_parameters() best_parameters   Out: {'lr': 0.00014152067288925645, 'momentum': 0.5071930879797697}  mean, covariance = values mean   Out: {'accuracy': 0.910329840245651}  ","version":"Next","tagName":"h2"},{"title":"7. Plot the response surface and optimization trace​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#7-plot-the-response-surface-and-optimization-trace","content":" Contour plot showing classification accuracy as a function of the two hyperparameters.  The black squares show points that we have actually run; notice how they are clustered in the optimal region.  render(ax_client.get_contour_plot(param_x=&quot;lr&quot;, param_y=&quot;momentum&quot;, metric_name=&quot;accuracy&quot;))   Out: [INFO 09-29 17:04:59] ax.service.ax_client: Retrieving contour plot with parameter 'lr' on X-axis and 'momentum' on Y-axis, for metric 'accuracy'. Remaining parameters are affixed to the middle of their range.  loading...  Here we plot the optimization trace, showing the progression of finding the point with the optimal objective:  render( ax_client.get_optimization_trace() )   loading...  ","version":"Next","tagName":"h2"},{"title":"8. Train CNN with best hyperparameters and evaluate on test set​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#8-train-cnn-with-best-hyperparameters-and-evaluate-on-test-set","content":" Note that the resulting accuracy on the test set generally won't be the same as the maximum accuracy achieved on the evaluation set throughout optimization.  df = ax_client.get_trials_data_frame() best_arm_idx = df.trial_index[df[&quot;accuracy&quot;] == df[&quot;accuracy&quot;].max()].values[0] best_arm = ax_client.get_trial_parameters(best_arm_idx) best_arm   Out: [WARNING 09-29 17:05:00] ax.service.utils.report_utils: Column reason missing for all trials. Not appending column.  Out: {'lr': 0.00014152067288925645, 'momentum': 0.5071930879797697}  combined_train_valid_set = torch.utils.data.ConcatDataset( [ train_loader.dataset.dataset, valid_loader.dataset.dataset, ] ) combined_train_valid_loader = torch.utils.data.DataLoader( combined_train_valid_set, batch_size=BATCH_SIZE, shuffle=True, )   net = train( net=CNN(), train_loader=combined_train_valid_loader, parameters=best_arm, dtype=dtype, device=device, ) test_accuracy = evaluate( net=net, data_loader=test_loader, dtype=dtype, device=device, )   print(f&quot;Classification Accuracy (test set): {round(test_accuracy*100, 2)}%&quot;)   Out: Classification Accuracy (test set): 96.98%  ","version":"Next","tagName":"h2"},{"title":"9. Save / reload optimization to JSON / SQL​","type":1,"pageTitle":"Tune a CNN on MNIST","url":"/Ax/docs/tutorials/tune_cnn_service/#9-save--reload-optimization-to-json--sql","content":" We can serialize the state of optimization to JSON and save it to a .json file or save it to the SQL backend. For the former:  ax_client.save_to_json_file() # For custom filepath, pass `filepath` argument.   Out: [INFO 09-29 17:05:13] ax.service.ax_client: Saved JSON-serialized state of optimization to ax_client_snapshot.json.  restored_ax_client = ( AxClient.load_from_json_file() ) # For custom filepath, pass `filepath` argument.   Out: [INFO 09-29 17:05:14] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  To store state of optimization to an SQL backend, first followsetup instructions on Ax website.  Having set up the SQL backend, pass DBSettings to AxClient on instantiation (note that SQLAlchemy dependency will have to be installed – for installation, refer tooptional dependencies on Ax website):  from ax.storage.sqa_store.structs import DBSettings # URL is of the form &quot;dialect+driver://username:password@host:port/database&quot;. db_settings = DBSettings(url=&quot;sqlite:///foo.db&quot;) # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary. new_ax = AxClient(db_settings=db_settings)   Out: [INFO 09-29 17:05:14] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  When valid DBSettings are passed into AxClient, a unique experiment name is a required argument (name) to ax_client.create_experiment. The state of the optimization is auto-saved any time it changes (i.e. a new trial is added or completed, etc).  To reload an optimization state later, instantiate AxClient with the same DBSettingsand use ax_client.load_experiment_from_database(experiment_name=&quot;my_experiment&quot;).  Special Cases  Evaluation failure: should any optimization iterations fail during evaluation,log_trial_failure will ensure that the same trial is not proposed again.  _, trial_index = ax_client.get_next_trial() ax_client.log_trial_failure(trial_index=trial_index)   Out: [INFO 09-29 17:05:15] ax.service.ax_client: Generated new trial 26 with parameters {'lr': 0.000286, 'momentum': 0.42988} using model BoTorch.  Out: [INFO 09-29 17:05:15] ax.service.ax_client: Registered failure of trial 26.  Need to run many trials in parallel: for optimal results and optimization efficiency, we strongly recommend sequential optimization (generating a few trials, then waiting for them to be completed with evaluation data). However, if your use case needs to dispatch many trials in parallel before they are updated with data and you are running into the &quot;All trials for current model have been generated, but not enough data has been observed to fit next model&quot; error, instantiate AxClient asAxClient(enforce_sequential_optimization=False).  Service API Exceptions Meaning and Handling  DataRequiredError: Ax generation strategy needs to be updated with more data to proceed to the next optimization model. When the optimization moves from initialization stage to the Bayesian optimization stage, the underlying BayesOpt model needs sufficient data to train. For optimal results and optimization efficiency (finding the optimal point in the least number of trials), we recommend sequential optimization (generating a few trials, then waiting for them to be completed with evaluation data). Therefore, the correct way to handle this exception is to wait until more trial evaluations complete and log their data via ax_client.complete_trial(...).  However, if there is strong need to generate more trials before more data is available, instantiate AxClient as AxClient(enforce_sequential_optimization=False). With this setting, as many trials will be generated from the initialization stage as requested, and the optimization will move to the BayesOpt stage whenever enough trials are completed.  MaxParallelismReachedException: generation strategy restricts the number of trials that can be run simultaneously (to encourage sequential optimization), and the parallelism limit has been reached. The correct way to handle this exception is the same as DataRequiredError – to wait until more trial evluations complete and log their data via ax_client.complete_trial(...).  In some cases higher parallelism is important, soenforce_sequential_optimization=False kwarg to AxClient allows the user to suppress limiting of parallelism. It's also possible to override the default parallelism setting for all stages of the optimization by passing choose_generation_strategy_kwargs toax_client.create_experiment:  ax_client = AxClient() ax_client.create_experiment( parameters=[ {&quot;name&quot;: &quot;x&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [-5.0, 10.0]}, {&quot;name&quot;: &quot;y&quot;, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 15.0]}, ], # Sets max parallelism to 10 for all steps of the generation strategy. choose_generation_strategy_kwargs={&quot;max_parallelism_override&quot;: 10}, )   Out: [INFO 09-29 17:05:15] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 17:05:15] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:05:15] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter y. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:05:15] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x', parameter_type=FLOAT, range=[-5.0, 10.0]), RangeParameter(name='y', parameter_type=FLOAT, range=[0.0, 15.0])], parameter_constraints=[]).  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=2 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=5  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=5  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:05:15] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 5 trials, BoTorch for subsequent trials]). Iterations after 5 will take longer to generate due to model-fitting.  ax_client.get_max_parallelism() # Max parallelism is now 10 for all stages of the optimization.   Out: [(5, 10), (-1, 10)] ","version":"Next","tagName":"h2"},{"title":"Visualizations","type":0,"sectionRef":"#","url":"/Ax/docs/tutorials/visualizations/","content":"","keywords":"","version":"Next"},{"title":"1. Create experiment and run optimization​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#1-create-experiment-and-run-optimization","content":" The vizualizations require an experiment object and a model fit on the evaluated data. The routine below is a copy of the Service API tutorial, so the explanation here is omitted. Retrieving the experiment and model objects for each API paradigm is shown in the respective tutorials  1a. Define search space and evaluation function​  noise_sd = 0.1 param_names = [f&quot;x{i+1}&quot; for i in range(6)] # x1, x2, ..., x6 def noisy_hartmann_evaluation_function(parameterization): x = np.array([parameterization.get(p_name) for p_name in param_names]) noise1, noise2 = np.random.normal(0, noise_sd, 2) return { &quot;hartmann6&quot;: (hartmann6(x) + noise1, noise_sd), &quot;l2norm&quot;: (np.sqrt((x**2).sum()) + noise2, noise_sd), }   1b. Create Experiment​  ax_client = AxClient() ax_client.create_experiment( name=&quot;test_visualizations&quot;, parameters=[ { &quot;name&quot;: p_name, &quot;type&quot;: &quot;range&quot;, &quot;bounds&quot;: [0.0, 1.0], } for p_name in param_names ], objectives={&quot;hartmann6&quot;: ObjectiveProperties(minimize=True)}, outcome_constraints=[&quot;l2norm &lt;= 1.25&quot;], )   Out: [INFO 09-29 17:01:24] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the verbose_logging argument to False. Note that float values in the logs are rounded to 6 decimal points.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x1. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x3. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x4. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x5. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x6. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.  Out: [INFO 09-29 17:01:24] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[]).  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.  Out: [INFO 09-29 17:01:24] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.  1c. Run the optimization and fit a GP on all data​  for i in range(20): parameters, trial_index = ax_client.get_next_trial() # Local evaluation here can be replaced with deployment to external system. ax_client.complete_trial( trial_index=trial_index, raw_data=noisy_hartmann_evaluation_function(parameters) )   Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 0 with parameters {'x1': 0.217147, 'x2': 0.24226, 'x3': 0.942423, 'x4': 0.551954, 'x5': 0.113248, 'x6': 0.778652} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 0 with data: {'hartmann6': (-0.627081, 0.1), 'l2norm': (1.394703, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 1 with parameters {'x1': 0.899619, 'x2': 0.544207, 'x3': 0.271069, 'x4': 0.214294, 'x5': 0.652881, 'x6': 0.085256} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 1 with data: {'hartmann6': (-0.232983, 0.1), 'l2norm': (1.338892, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 2 with parameters {'x1': 0.613586, 'x2': 0.449477, 'x3': 0.694775, 'x4': 0.86204, 'x5': 0.991306, 'x6': 0.332011} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 2 with data: {'hartmann6': (-0.18172, 0.1), 'l2norm': (1.709512, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 3 with parameters {'x1': 0.299934, 'x2': 0.774498, 'x3': 0.023356, 'x4': 0.403225, 'x5': 0.273816, 'x6': 0.525272} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 3 with data: {'hartmann6': (-0.490295, 0.1), 'l2norm': (1.388586, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 4 with parameters {'x1': 0.436694, 'x2': 0.32577, 'x3': 0.461523, 'x4': 0.352192, 'x5': 0.467985, 'x6': 0.969165} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 4 with data: {'hartmann6': (-0.773964, 0.1), 'l2norm': (1.466576, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 5 with parameters {'x1': 0.742443, 'x2': 0.899173, 'x3': 0.757816, 'x4': 0.881807, 'x5': 0.797168, 'x6': 0.166469} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 5 with data: {'hartmann6': (-0.156158, 0.1), 'l2norm': (2.0135, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 6 with parameters {'x1': 0.770724, 'x2': 0.118927, 'x3': 0.213748, 'x4': 0.046074, 'x5': 0.589694, 'x6': 0.420659} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 6 with data: {'hartmann6': (-0.020075, 0.1), 'l2norm': (1.084103, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 7 with parameters {'x1': 0.080408, 'x2': 0.668525, 'x3': 0.510229, 'x4': 0.68891, 'x5': 0.176405, 'x6': 0.723492} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 7 with data: {'hartmann6': (-0.211787, 0.1), 'l2norm': (1.222905, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 8 with parameters {'x1': 0.051931, 'x2': 0.417225, 'x3': 0.17882, 'x4': 0.955695, 'x5': 0.747325, 'x6': 0.583136} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 8 with data: {'hartmann6': (0.051102, 0.1), 'l2norm': (1.335655, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 9 with parameters {'x1': 0.861586, 'x2': 0.873454, 'x3': 0.601094, 'x4': 0.309562, 'x5': 0.017796, 'x6': 0.280501} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 9 with data: {'hartmann6': (-0.019018, 0.1), 'l2norm': (1.529207, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 10 with parameters {'x1': 0.655492, 'x2': 0.155692, 'x3': 0.427144, 'x4': 0.645663, 'x5': 0.368824, 'x6': 0.026902} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 10 with data: {'hartmann6': (0.044719, 0.1), 'l2norm': (0.976306, 0.1)}.  Out: /Users/cristianlara/Projects/Ax-1.0/ax/modelbridge/cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction. [INFO 09-29 17:01:24] ax.service.ax_client: Generated new trial 11 with parameters {'x1': 0.46127, 'x2': 0.572494, 'x3': 0.849596, 'x4': 0.120563, 'x5': 0.897303, 'x6': 0.829676} using model Sobol.  Out: [INFO 09-29 17:01:24] ax.service.ax_client: Completed trial 11 with data: {'hartmann6': (0.021645, 0.1), 'l2norm': (1.551668, 0.1)}.  Out: [INFO 09-29 17:01:29] ax.service.ax_client: Generated new trial 12 with parameters {'x1': 0.548399, 'x2': 0.039597, 'x3': 0.228325, 'x4': 0.414972, 'x5': 0.363342, 'x6': 0.590747} using model BoTorch.  Out: [INFO 09-29 17:01:29] ax.service.ax_client: Completed trial 12 with data: {'hartmann6': (-1.654472, 0.1), 'l2norm': (0.864892, 0.1)}.  Out: [INFO 09-29 17:01:32] ax.service.ax_client: Generated new trial 13 with parameters {'x1': 0.555949, 'x2': 0.0, 'x3': 0.0, 'x4': 0.431248, 'x5': 0.431477, 'x6': 0.667878} using model BoTorch.  Out: [INFO 09-29 17:01:32] ax.service.ax_client: Completed trial 13 with data: {'hartmann6': (-1.161214, 0.1), 'l2norm': (0.886572, 0.1)}.  Out: [INFO 09-29 17:01:33] ax.service.ax_client: Generated new trial 14 with parameters {'x1': 0.465761, 'x2': 0.0, 'x3': 0.386818, 'x4': 0.419128, 'x5': 0.271558, 'x6': 0.60762} using model BoTorch.  Out: [INFO 09-29 17:01:33] ax.service.ax_client: Completed trial 14 with data: {'hartmann6': (-1.941374, 0.1), 'l2norm': (1.262377, 0.1)}.  Out: [INFO 09-29 17:01:39] ax.service.ax_client: Generated new trial 15 with parameters {'x1': 0.584766, 'x2': 0.0, 'x3': 0.430003, 'x4': 0.417014, 'x5': 0.105126, 'x6': 0.614851} using model BoTorch.  Out: [INFO 09-29 17:01:39] ax.service.ax_client: Completed trial 15 with data: {'hartmann6': (-0.834074, 0.1), 'l2norm': (1.23201, 0.1)}.  Out: [INFO 09-29 17:01:41] ax.service.ax_client: Generated new trial 16 with parameters {'x1': 0.545339, 'x2': 0.0, 'x3': 0.272192, 'x4': 0.317869, 'x5': 0.291945, 'x6': 0.694177} using model BoTorch.  Out: [INFO 09-29 17:01:41] ax.service.ax_client: Completed trial 16 with data: {'hartmann6': (-1.900013, 0.1), 'l2norm': (1.096653, 0.1)}.  Out: [INFO 09-29 17:01:44] ax.service.ax_client: Generated new trial 17 with parameters {'x1': 0.50536, 'x2': 0.0, 'x3': 0.273584, 'x4': 0.939351, 'x5': 0.293315, 'x6': 0.697805} using model BoTorch.  Out: [INFO 09-29 17:01:44] ax.service.ax_client: Completed trial 17 with data: {'hartmann6': (-0.181284, 0.1), 'l2norm': (1.112907, 0.1)}.  Out: [INFO 09-29 17:01:47] ax.service.ax_client: Generated new trial 18 with parameters {'x1': 0.515606, 'x2': 0.0, 'x3': 0.303883, 'x4': 0.418851, 'x5': 0.290393, 'x6': 0.790276} using model BoTorch.  Out: [INFO 09-29 17:01:47] ax.service.ax_client: Completed trial 18 with data: {'hartmann6': (-1.475551, 0.1), 'l2norm': (1.05417, 0.1)}.  Out: [INFO 09-29 17:01:52] ax.service.ax_client: Generated new trial 19 with parameters {'x1': 0.475179, 'x2': 0.0, 'x3': 0.316528, 'x4': 0.261652, 'x5': 0.289608, 'x6': 0.572033} using model BoTorch.  Out: [INFO 09-29 17:01:52] ax.service.ax_client: Completed trial 19 with data: {'hartmann6': (-2.353964, 0.1), 'l2norm': (0.884022, 0.1)}.  ","version":"Next","tagName":"h2"},{"title":"2. Contour plots​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#2-contour-plots","content":" The plot below shows the response surface for hartmann6 metric as a function of thex1, x2 parameters.  The other parameters are fixed in the middle of their respective ranges, which in this example is 0.5 for all of them.  # this could alternately be done with `ax.plot.contour.plot_contour` render(ax_client.get_contour_plot(param_x=&quot;x1&quot;, param_y=&quot;x2&quot;, metric_name=&quot;hartmann6&quot;))   Out: [INFO 09-29 17:01:52] ax.service.ax_client: Retrieving contour plot with parameter 'x1' on X-axis and 'x2' on Y-axis, for metric 'hartmann6'. Remaining parameters are affixed to the middle of their range.  loading...  2a. Interactive contour plot​  The plot below allows toggling between different pairs of parameters to view the contours.  model = ax_client.generation_strategy.model render(interact_contour(model=model, metric_name=&quot;hartmann6&quot;))   loading...  ","version":"Next","tagName":"h2"},{"title":"3. Tradeoff plots​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#3-tradeoff-plots","content":" This plot illustrates the tradeoffs achievable for 2 different metrics. The plot takes the x-axis metric as input (usually the objective) and allows toggling among all other metrics for the y-axis.  This is useful to get a sense of the pareto frontier (i.e. what is the best objective value achievable for different bounds on the constraint)  render(plot_objective_vs_constraints(model, &quot;hartmann6&quot;, rel=False))   loading...  ","version":"Next","tagName":"h2"},{"title":"4. Cross-validation plots​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#4-cross-validation-plots","content":" CV plots are useful to check how well the model predictions calibrate against the actual measurements. If all points are close to the dashed line, then the model is a good predictor of the real data.  cv_results = cross_validate(model) render(interact_cross_validation(cv_results))   loading...  ","version":"Next","tagName":"h2"},{"title":"5. Slice plots​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#5-slice-plots","content":" Slice plots show the metric outcome as a function of one parameter while fixing the others. They serve a similar function as contour plots.  render(plot_slice(model, &quot;x2&quot;, &quot;hartmann6&quot;))   loading...  ","version":"Next","tagName":"h2"},{"title":"6. Tile plots​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#6-tile-plots","content":" Tile plots are useful for viewing the effect of each arm.  render(interact_fitted(model, rel=False))   loading...  ","version":"Next","tagName":"h2"},{"title":"Fix for plots that are not rendering​","type":1,"pageTitle":"Visualizations","url":"/Ax/docs/tutorials/visualizations/#fix-for-plots-that-are-not-rendering","content":" In certain environments like Google Colab or remote setups, plots may not render. If this is the case, we recommend using the below workaround which overrides the default renderer in plotly. The below cell changes the renderer to &quot;jupyterlab&quot; for this tutorial, but you can find the right renderer for your use case by callingpio.renderers  import plotly.io as pio pio.renderers.default = &quot;jupyterlab&quot; render(ax_client.get_contour_plot(param_x=&quot;x1&quot;, param_y=&quot;x2&quot;, metric_name=&quot;hartmann6&quot;))   Out: [INFO 09-29 17:02:06] ax.service.ax_client: Retrieving contour plot with parameter 'x1' on X-axis and 'x2' on Y-axis, for metric 'hartmann6'. Remaining parameters are affixed to the middle of their range.  loading... ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}